{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e682692",
   "metadata": {},
   "source": [
    "# Atlantic\n",
    "The Content Testing System in SMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a07f454c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gsheets import Sheets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "import datetime as dt\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "import warnings\n",
    "import smartsheet\n",
    "import infrastructure\n",
    "import pygsheets\n",
    "from scipy.stats import zscore\n",
    "import filepaths\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import os\n",
    "import filepath\n",
    "import infrastructure\n",
    "\n",
    "bold = \"\\033[1m\"\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3bc1ee",
   "metadata": {},
   "source": [
    "### Read in Files\n",
    "\n",
    "<strong>Requirements</strong>\n",
    "- Lexi\n",
    "- EIMT\n",
    "- Offer_WK\n",
    "- Cobra (mamba)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6f40544",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### - Getting Lexi - #########################\n",
    "def reading_lexi():\n",
    "    global lexi\n",
    "    print(\"Reading in SMS Lexi...\")\n",
    "    sms_lexi = pd.read_csv(filepath.output_folder + 'SS_LC_merged_data.csv')\n",
    "    lexi = infrastructure.transform_sms_df(sms_lexi)\n",
    "    lexi['Hitpath Offer ID'] = lexi['Hitpath Offer ID'].astype(str).str.replace('.0', '')\n",
    "    df = lexi\n",
    "    return print(\"SMS Lexi is loaded.\")\n",
    "\n",
    "######################### - Getting Smartsheet - #########################\n",
    "def reading_smartsheet():\n",
    "    print(\"Gathering EIMT and Offer Workbook\")\n",
    "    global EMIT\n",
    "    global offer_wk\n",
    "    # use smartsheet api to get offer workbook and EIMT\n",
    "    EMIT = infrastructure.get_publisher()\n",
    "    offer_wk = infrastructure.get_smartsheet('offers_sms')\n",
    "    offer_wk['Hitpath Offer ID'] = offer_wk['Hitpath Offer ID'].astype(str).str.replace('.0', '')\n",
    "    return print(\"Smartsheets are loaded.\")\n",
    "def reading_cobra():\n",
    "    global cobra\n",
    "    cobra = infrastructure.get_mamba_full_slot()\n",
    "    cobra['Hitpath Offer ID'] = cobra['Hitpath Offer ID'].astype(str).str.replace('.0', '')\n",
    "    return print(\"Mamba are loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ebdbcdc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in SMS Lexi...\n",
      "SMS Lexi is loaded.\n",
      "Gathering EIMT and Offer Workbook\n",
      "Smartsheets are loaded.\n",
      "Mamba are loaded.\n"
     ]
    }
   ],
   "source": [
    "reading_lexi()\n",
    "reading_smartsheet()\n",
    "reading_cobra()\n",
    "lexi = lexi.merge(offer_wk[['Hitpath Offer ID','Vertical']], left_on = 'Hitpath Offer ID',right_on = 'Hitpath Offer ID', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ceddc73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MBC_NPD.RTO', 'FLC_NPD.RTO', 'MBC_I.RC', 'FLC_I.RC', 'HZB_CM.OSR', 'MBC_CM.OSR', 'DSS_DMS.SJ', 'HZB_DMS.SJ', 'SVT_AMD.PL', 'HZB_AMD.PL', 'SVT_AL.PL.3', 'HZB_AL.PL.3', 'A4F_I.A4F', 'DSS_TLG.PL', 'HZB_TLG.PL', 'HZB_PN.SWP', 'MBC_PN.SWP', 'FLC_PN.SWP', 'UAA_SPK.SWP2', 'UAATF_SPK.SWP2', 'MBC_PA.PS', 'HZB_PA.PS', 'UAA_FSM.YS', 'HZB_FSM.YS', 'FLC_EDM.247L', 'MBC_EDM.247L', 'DSS_ZM.PL.2', 'HZB_ZM.PL.2', 'MFA_I.MFA', 'SVT_B2.F', 'HZB_B2.F', 'MBC_WWM.YFA.2', 'FLC_WWM.YFA.2', 'FLC_I.CC', 'MBC_I.CC', 'HZB_RHD.CC', 'DSS_RHD.CC', 'UAA_SPK.CR2', 'FLC_SPK.CR2', 'MBC_PN.FC', 'FLC_PN.FC', 'SVT_AL.PL', 'HZB_AL.PL', 'SVT_AL.PL.2', 'HZB_AL.PL.2', 'DSS_JET.ZTA', 'FLC_JET.ZTA']\n"
     ]
    }
   ],
   "source": [
    "mamba_dic_df = infrastructure.get_mamba_directory()\n",
    "mamba_dic_df['Text ID'] = mamba_dic_df['Account'].str.split('_', expand=True)[2]\n",
    "current_active_segment = mamba_dic_df['Segment'].str.split('\\n').tolist()\n",
    "current_active_segment = [item for sublist in current_active_segment for item in sublist if item]\n",
    "current_active_sc_sv = mamba_dic_df['Account'].str[:-7].unique().tolist()\n",
    "#EMIT[\"Dataset\"] = EMIT[\"Text ID\"].str.cat(EMIT[\"Text DP\"], sep = \"_\")\n",
    "#DPPub_to_Dataset = dict(zip(EMIT['shortcode_DP.SV'], EMIT['Dataset']))\n",
    "#Dataset_to_DPPub = dict(zip(EMIT['Dataset'], EMIT['shortcode_DP.SV']))\n",
    "print(current_active_sc_sv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a900a19",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14d80f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_CT_eligiblity(default_max_drops, offer, lexi, cobra):\n",
    "    \"\"\" \n",
    "    #sheets = Sheets.from_files(filepaths.gsheets)\n",
    "    #viper_settings = sheets.get(CT_settings_url)\n",
    "\n",
    "    #CT_settings = viper_settings.find('Testing Settings').to_frame()\n",
    "    #CT_settings = CT_settings.fillna(default_max_drops) # MAX NUMBER OF P DROPS IS DEFAULTED TO 2\n",
    "    #CT_settings_dict = dict(zip(CT_settings['shortcode_DP.SV'],CT_settings['Max Number of CT']))\n",
    "    \"\"\"\n",
    "    CT_settings_dict = {}\n",
    "    for i in current_active_sc_sv: \n",
    "        CT_settings_dict[i] = default_max_drops\n",
    "    # Dates and Parameter Stuff\n",
    "    ## Calculate the starting date of the current week\n",
    "    current_date = datetime.today()\n",
    "    start_of_week = datetime.today() - timedelta(days=current_date.weekday() + 1)\n",
    "    start_date = datetime.today()\n",
    "    end_date = start_date + timedelta(14)\n",
    "    \n",
    "    # Manipulating Cobra\n",
    "    #cobra['shortcode_DP.SV'] = cobra['Dataset'].map(dataset_convertor)\n",
    "    cobra_temp = cobra.groupby([pd.Grouper(key='Date', freq='W'), 'shortcode_DP.SV'])['Send Strategy'].count().reset_index()\n",
    "    cobra_temp = cobra_temp[['shortcode_DP.SV', 'Date']]\n",
    "    Account_CT_Checklist = cobra_temp[cobra_temp['Date'] >= (start_of_week - timedelta(1))].sort_values(by = ['shortcode_DP.SV', 'Date'], ascending = True)\n",
    "    #Account_CT_Checklist = pd.DataFrame({'shortcode_DP.SV': cobra_temp[cobra_temp['Date'] >  dt.date.today()]['shortcode_DP.SV'].unique()})\n",
    "    cobra_CT = cobra[cobra['Send Strategy'] == 'CT']\n",
    "\n",
    "         # use pd.Grouper to get current and future week\n",
    "    cobra_weekly_CT = cobra_CT.groupby([pd.Grouper(key='Date', freq='W'), 'shortcode_DP.SV'])['Send Strategy'].count().reset_index()\n",
    "    cobra_weekly_CT.rename(columns={'Send Strategy': 'CT Count'}, inplace=True)\n",
    "    cobra_weekly_CT = cobra_weekly_CT[cobra_weekly_CT['Date'] > (start_of_week - timedelta(1))]\n",
    "    cobra_weekly_CT['Max CT'] = cobra_weekly_CT['shortcode_DP.SV'].map(CT_settings_dict)\n",
    "    cobra_weekly_CT['CT available'] = cobra_weekly_CT['Max CT'] - cobra_weekly_CT['CT Count']\n",
    "    Account_CT_Checklist = pd.merge(Account_CT_Checklist, cobra_weekly_CT, on=['shortcode_DP.SV','Date'], how='left')\n",
    "    # earliest possible date\n",
    "    default_earliest_date = dt.date.today() + timedelta(2)\n",
    "    \n",
    "# See if account has a P drop to swap and is under its Max CT, if so, swap the P drop to a CT\n",
    "    cobra_P = cobra[cobra['Send Strategy'] == 'P']\n",
    "    cobra_P = cobra_P[cobra_P['Hitpath Offer ID'] == offer]\n",
    "    #cobra_P = cobra_P[cobra_P['Date'].dt.date >= default_earliest_date]\n",
    "    cobra_weekly_P = cobra_P.groupby([pd.Grouper(key='Date', freq='W'), 'shortcode_DP.SV'])['Send Strategy'].count().reset_index()\n",
    "    cobra_weekly_P.rename(columns={'Send Strategy': 'P Available to Swap to CT'}, inplace=True)\n",
    "    cobra_weekly_P = cobra_weekly_P[cobra_weekly_P['Date'] > (start_of_week - timedelta(1))]\n",
    "    Account_CT_Checklist = pd.merge(Account_CT_Checklist, cobra_weekly_P, on=['shortcode_DP.SV','Date'], how='left')\n",
    "\n",
    "# If not, but is under Max CT and has sent the offer more than 1 P drop in last 30 days, send in P slot\n",
    "## Can send in normal P slot?\n",
    "    date_threshhold = max(lexi['Date']) - timedelta(30)\n",
    "    lexi_30 = lexi[lexi['Date'] > date_threshhold]\n",
    "    lexi_30 = lexi_30[lexi_30['Send Strategy'] == 'P']\n",
    "    lexi_30 = lexi_30[lexi_30['Hitpath Offer ID'] == offer]\n",
    "    lexi_30 = lexi_30.groupby(['shortcode_DP.SV'])['Send Strategy'].count().reset_index()\n",
    "    lexi_30.rename(columns={'Send Strategy': 'Sent in Last 30 Days'}, inplace=True)\n",
    "    lexi_30['Can it Send in P Slot?'] = lexi_30['Sent in Last 30 Days'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    Account_CT_Checklist = pd.merge(Account_CT_Checklist, lexi_30[['shortcode_DP.SV', 'Can it Send in P Slot?']], on=['shortcode_DP.SV'], how='left')\n",
    "\n",
    "# If not, but is under max CT and has not send the offer more than once in last 30 days, send outside P slot\n",
    "## Needs to send outside normal P slots?\n",
    "\n",
    "    \n",
    "# If account has not sent the offer at all\n",
    "\n",
    "\n",
    "# Cleaning up NaNs\n",
    "    Account_CT_Checklist['CT Count'].fillna(0, inplace=True)\n",
    "    Account_CT_Checklist['P Available to Swap to CT'].fillna(0, inplace=True)\n",
    "    Account_CT_Checklist['Can it Send in P Slot?'].fillna(0, inplace=True)\n",
    "    Account_CT_Checklist['Max CT'] = Account_CT_Checklist['shortcode_DP.SV'].map(CT_settings_dict)\n",
    "    Account_CT_Checklist['CT available'] = Account_CT_Checklist['Max CT'] - Account_CT_Checklist['CT Count']\n",
    "     \n",
    "    '''\n",
    "    print(\"The start date:\", start_date.strftime(\"%Y-%m-%d\"))\n",
    "    print(\"The end date:\", end_date.strftime(\"%Y-%m-%d\"))\n",
    "    print(\"The start date of the current week is:\", start_of_week.strftime(\"%Y-%m-%d\"))\n",
    "    '''\n",
    "    \n",
    "    return Account_CT_Checklist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e09a9700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_offer_vertical_gaps(df):\n",
    "    df = lexi.copy()\n",
    "    df.sort_values(by = ['shortcode_DP.SV','Hitpath Offer ID','Send Strategy','Date'], ascending = True, inplace = True)\n",
    "    df['Offer Gap'] = df.groupby(['shortcode_DP.SV','Send Strategy','Hitpath Offer ID'])['Date'].diff()\n",
    "    df['Offer Gap'] = df['Offer Gap'].dt.days\n",
    "    df.loc[df['Send Strategy'] != 'P', 'Offer Gap'] = np.nan\n",
    "    # filter for last 60 days\n",
    "    date_threshhold = max(df['Date']) - timedelta(60)\n",
    "    df = df[df['Date'] > date_threshhold]\n",
    "    \n",
    "    #df = df.rename(columns={\"Vertical\": \"Offer Vertical\"})\n",
    "    df_gap = df[(df['Offer Gap'] > 0) & (df['Offer Gap'] < 30)]\n",
    "    df_gap = df_gap.groupby(['Data Vertical','Offer Vertical'])['Offer Gap'].mean().reset_index()\n",
    "    return df_gap\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d34013d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_account_rankings(df, offer,cobra):\n",
    "    \n",
    "        # offer info\n",
    "    \n",
    "    #offer_vertical = offer_wk[offer_wk['Hitpath Offer ID'] == offer]['Vertical'].iloc[0]\n",
    "    \n",
    "    filtered_df = offer_wk[offer_wk['Hitpath Offer ID'] == offer]\n",
    "\n",
    "    # check if the filtered DataFrame is not empty\n",
    "    if not filtered_df.empty:\n",
    "        offer_vertical = filtered_df['Vertical'].iloc[0]\n",
    "    else:\n",
    "        offer_vertical = None\n",
    "    \n",
    "    print(offer, offer_vertical)\n",
    "        \n",
    "        # filter for last 60 days\n",
    "    date_threshhold = max(df['Date']) - timedelta(60)\n",
    "    df = df[df['Date'] > date_threshhold]\n",
    "    \n",
    "        # get account ranking for offer\n",
    "    df_offer = df[df['Hitpath Offer ID'] == offer]\n",
    "    df_offer = df_offer.groupby('SC_DP&Pub')[['Revenue', 'Delivered', 'Clicks', 'Jump Page Clicks', 'Opportunity Cost']].sum().reset_index()\n",
    "    df_offer['eCPM'] = 1000 * (df_offer['Revenue']/df_offer['Delivered'])\n",
    "    df_offer['OCRR'] = (df_offer['Opportunity Cost']/df_offer['Revenue'])\n",
    "        # Create rank columns for each column\n",
    "    rank_columns = df_offer[['Revenue', 'Delivered','eCPM', 'Opportunity Cost','OCRR']].apply(lambda x: x.rank(ascending=False))\n",
    "        # Add rank columns to the original DataFrame\n",
    "    df_offer = pd.concat([df_offer, rank_columns.add_suffix('_rank')], axis=1)\n",
    "    df_offer['Average Rank'] = df_offer.iloc[:,8:].mean(axis=1)\n",
    "    df_offer = df_offer.sort_values(by = 'Average Rank', ascending = True)\n",
    "    df_offer['Overall Rank'] = df_offer['Average Rank'].rank()\n",
    "    \n",
    "        # offer vertical ranking\n",
    "    df_vertical = df[df['Vertical'] == offer_vertical]\n",
    "    df_vertical = df_vertical.groupby('SC_DP&Pub')[['Revenue', 'Delivered', 'Clicks','Jump Page Clicks', 'Opportunity Cost']].sum().reset_index()\n",
    "    df_vertical['eCPM (OV)'] = 1000 * (df_vertical['Revenue']/df_vertical['Delivered'])\n",
    "    df_vertical['OCRR (OV)'] = (df_vertical['Opportunity Cost']/df_vertical['Revenue'])\n",
    "        # Create rank columns for each column\n",
    "    rank_columns = df_vertical[['Revenue', 'Delivered','eCPM (OV)', 'Opportunity Cost','OCRR (OV)']].apply(lambda x: x.rank(ascending=False))\n",
    "        # Add rank columns to the original DataFrame\n",
    "    df_vertical = pd.concat([df_vertical, rank_columns.add_suffix('_rank (OV)')], axis=1)\n",
    "    df_vertical['Average Rank (OV)'] = df_offer.iloc[:,8:].mean(axis=1)\n",
    "    df_vertical = df_vertical.sort_values(by = 'Average Rank (OV)', ascending = True)\n",
    "    df_vertical['Overall Rank'] = df_vertical['Average Rank (OV)'].rank()\n",
    "    df_vertical['Overall Rank'] = df_vertical['Overall Rank'].apply(lambda x: x + 5 if not pd.isnull(x) else x)\n",
    "    df_vertical = df_vertical[['SC_DP&Pub','Overall Rank']]\n",
    "    df_vertical['Rank Origin'] = 'Vertical'\n",
    "    \n",
    "        # Create column that checks eligibility\n",
    "    #static_eligbility = check_ESP_eligibility(df=EMIT)\n",
    "    #df_offer['CT Eligibility'] = df_offer['shortcode_DP.SV'].isin(static_eligbility).astype(int)\n",
    "    #df_offer_eligble = df_offer[df_offer['CT Eligibility'] == 1]\n",
    "    df_offer_eligble = df_offer.copy()\n",
    "    \n",
    "        # calculate Tiers\n",
    "        # Replace -inf with NaN\n",
    "    df_offer_eligble['OCRR'] = df_offer_eligble['OCRR'].replace(float('-inf'), np.nan)\n",
    "        # Calculate the z-scores\n",
    "    df_offer_eligble['OCRR Z-Scores'] = zscore(df_offer_eligble['OCRR'], nan_policy='omit')\n",
    "    \n",
    "    Tier_1 = .5\n",
    "    Tier_2 = .25\n",
    "    Tier_3 = 0\n",
    "    Tier_4 = -.25\n",
    "    \n",
    "    conditions = [\n",
    "        (df_offer_eligble['OCRR Z-Scores'] >= Tier_1),\n",
    "        (df_offer_eligble['OCRR Z-Scores'] < Tier_1) &  (df_offer_eligble['OCRR Z-Scores'] >= Tier_2),\n",
    "        (df_offer_eligble['OCRR Z-Scores'] < Tier_2) &  (df_offer_eligble['OCRR Z-Scores'] >= Tier_3),\n",
    "        (df_offer_eligble['OCRR Z-Scores'] < Tier_3) &  (df_offer_eligble['OCRR Z-Scores'] >= Tier_4),\n",
    "        (df_offer_eligble['OCRR Z-Scores'] < Tier_4)\n",
    "    ]\n",
    "    \n",
    "    message = ['Tier 1', 'Tier 2', 'Tier 3',\n",
    "               'Tier 4','Tier 5']\n",
    "    \n",
    "    df_offer_eligble['Account Tier'] = np.select(conditions, message)\n",
    "    df_tier_summary = pd.DataFrame(df_offer_eligble['Account Tier'].value_counts())\n",
    "    df_tier_summary['Tier'] = df_tier_summary.index\n",
    "    df_tier_summary.rename(columns={'Account Tier': 'Accounts per Tier'}, inplace=True)\n",
    "    df_tier_summary = df_tier_summary[['Tier', 'Accounts per Tier']].reset_index(drop = True)\n",
    "    df_tier_summary = df_tier_summary.sort_values(by = 'Tier', ascending = True)\n",
    "    df_tier_summary = tabulate(df_tier_summary, headers='keys', tablefmt='grid')\n",
    "    \n",
    "    #print(\"For Hitpath Offer ID\", offer, \"the account Tier distribution is: \\n\", df_tier_summary )\n",
    "    \n",
    "    accounts_ranked = df_offer_eligble[['SC_DP&Pub','Overall Rank']].sort_values(by = ['Overall Rank'], ascending = True)\n",
    "    accounts_ranked['Rank Origin'] = 'Account'\n",
    "    \n",
    "    accounts_ranked = pd.concat([accounts_ranked, df_vertical], ignore_index=True)\n",
    "    accounts_ranked = accounts_ranked[~((accounts_ranked['Rank Origin'] == 'Vertical') & accounts_ranked.duplicated('SC_DP&Pub'))]\n",
    "    accounts_ranked['PubID'] = accounts_ranked['SC_DP&Pub'].str[-6:]\n",
    "    accounts_ranked['shortcode_DP.SV'] = accounts_ranked['SC_DP&Pub'].str[:-7]\n",
    "    \n",
    "    # filter out accounts that are paused to the offer\n",
    "    paused_pubs = offer_wk[offer_wk['Hitpath Offer ID'] == offer]['Paused Pubs'].str.split('\\n|\\t|,|\"| ')\n",
    "    accounts_ranked = accounts_ranked[~accounts_ranked['PubID'].isin(paused_pubs)]\n",
    "\n",
    "    # give account has a CT slot available some extra credit \n",
    "    # earliest possible date\n",
    "    default_earliest_date = dt.date.today() + timedelta(2)\n",
    "    cobra_P = cobra[cobra['Send Strategy'] == 'P']\n",
    "    cobra_P = cobra_P[cobra_P['Hitpath Offer ID'] == offer]\n",
    "    cobra_weekly_P_pubid = cobra_P[cobra_P['Date'].dt.date >= default_earliest_date]['shortcode_DP.SV'].unique().tolist()\n",
    "    accounts_ranked['CT Slot Available'] = accounts_ranked['shortcode_DP.SV'].isin(cobra_weekly_P_pubid).astype(int)\n",
    "    accounts_ranked['CT Slot Rank'] = 1- accounts_ranked['CT Slot Available']\n",
    "    accounts_ranked['Overall Rank'] = accounts_ranked['Overall Rank'] + accounts_ranked['CT Slot Rank']\n",
    "    accounts_ranked = accounts_ranked.sort_values(by = ['Overall Rank','CT Slot Rank'], ascending = True)\n",
    "    return accounts_ranked\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15daeec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_date(date, offer):\n",
    "    \n",
    "    days_restrictions = offer_wk[offer_wk['Hitpath Offer ID'] == offer]['Day Restrictions'].iloc[0]\n",
    "    days_allowed = str(days_restrictions)\n",
    "    \n",
    "    # Parse the input date\n",
    "    #input_date = pd.Timestamp(date)\n",
    "    input_date = date\n",
    "    \n",
    "    # If days_allowed is None or 'None', return the input date as it is\n",
    "    if days_allowed is None or days_allowed.lower() == 'none' or days_allowed.lower() == 'nan':\n",
    "        return pd.Timestamp(input_date)\n",
    "\n",
    "    # Create a dictionary to map weekdays to numbers (Monday = 0, Tuesday = 1, etc.)\n",
    "    weekdays = {'Monday': 0, 'Tuesday': 1, 'Wednesday': 2, 'Thursday': 3, 'Friday': 4, 'Saturday': 5, 'Sunday': 6}\n",
    "\n",
    "    # Check if the days_allowed contains a single value or a range\n",
    "    if ' - ' in days_allowed:\n",
    "        lower_bound, upper_bound = days_allowed.split(' - ')\n",
    "    else:\n",
    "        lower_bound = upper_bound = days_allowed\n",
    "\n",
    "    # Calculate the target weekday numbers for the lower and upper bounds\n",
    "    target_lower = weekdays[lower_bound]\n",
    "    target_upper = weekdays[upper_bound]\n",
    "\n",
    "    # Calculate the current weekday number of the input date\n",
    "    current_weekday = input_date.weekday()\n",
    "\n",
    "    # Calculate the difference between the current weekday and the target lower bound\n",
    "    weekday_difference = (target_lower - current_weekday) % 7\n",
    "\n",
    "    # Adjust the input date by adding the weekday difference\n",
    "    adjusted_date = input_date + timedelta(days=weekday_difference)\n",
    "\n",
    "    # If the adjusted date is still outside the days allowed, add 1 day until it falls within the allowed range\n",
    "    while adjusted_date.weekday() < target_lower or adjusted_date.weekday() > target_upper:\n",
    "        adjusted_date += timedelta(days=1)\n",
    "\n",
    "    # Return the adjusted date as a string in the format 'MM/DD/YYYY'\n",
    "    return pd.Timestamp(adjusted_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f1458bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_drop_date(offer_gaps_df,CT_eligibility_df,account, offer, cobra, end_date):\n",
    "    print(\"#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#\")\n",
    "    print(\"Searching for available date for\", account)\n",
    "    \n",
    "    cobra = cobra[cobra['Send Strategy'] == 'P']\n",
    "    \n",
    "    all_drop_dates = cobra[['shortcode_DP.SV','Hitpath Offer ID', 'Date']]\n",
    "    all_drop_dates = all_drop_dates[all_drop_dates['Hitpath Offer ID'] == offer]\n",
    "    all_drop_dates = all_drop_dates[all_drop_dates['shortcode_DP.SV'] == account]\n",
    "    all_drop_dates['Date'] = pd.to_datetime(all_drop_dates['Date'])\n",
    "    \n",
    "    last_drop_dates = cobra.groupby(['shortcode_DP.SV','Hitpath Offer ID'])['Date'].max().reset_index()\n",
    "    last_drop_dates = last_drop_dates[last_drop_dates['Hitpath Offer ID'] == offer]\n",
    "    last_drop_dates = last_drop_dates[last_drop_dates['shortcode_DP.SV'] == account]\n",
    "    \n",
    "    account_dv = lexi[lexi['shortcode_DP.SV'] == account]['Data Vertical'].iloc[0]\n",
    "    offer_vertical = offer_wk[offer_wk['Hitpath Offer ID'] == offer]['Vertical'].iloc[0]\n",
    "    offer_gap = offer_gaps_df[offer_gaps_df['Data Vertical'] == account_dv]\n",
    "    offer_gap_df = offer_gap[offer_gap['Offer Vertical'] == offer_vertical]\n",
    "    \n",
    "    if len(offer_gap_df) == 0:\n",
    "        offer_gap = 10\n",
    "    elif len(offer_gap_df) != 0:\n",
    "        offer_gap = offer_gap_df['Offer Gap'].iloc[0].round(0)\n",
    "    else:\n",
    "        offer_gap = 10\n",
    "\n",
    "    # earliest possible date\n",
    "    default_earliest_date = dt.date.today() + timedelta(3)\n",
    "    \n",
    "    # next upcoming drop based on offer gap\n",
    "    if len(last_drop_dates) == 0:\n",
    "        next_drop_date = default_earliest_date\n",
    "    if len(last_drop_dates) != 0:\n",
    "        next_drop_date = last_drop_dates['Date'].iloc[0] +  timedelta(offer_gap)\n",
    "    \n",
    "    # first week it's avaible to drop\n",
    "    # check the eligibility of accounts\n",
    "    CT_eligibility_Pub = CT_eligibility_df[CT_eligibility_df['shortcode_DP.SV'] == account]\n",
    "    CT_eligibility_Pub = CT_eligibility_Pub[CT_eligibility_Pub['CT available'] > 0 ]\n",
    "    \n",
    "    if CT_eligibility_Pub['P Available to Swap to CT'].iloc[0] == 1:\n",
    "        \n",
    "        earliest_week = CT_eligibility_Pub['Date'].iloc[0]\n",
    "        earliest_week = max(pd.to_datetime(default_earliest_date), \n",
    "                   pd.to_datetime(earliest_week)\n",
    "                  )\n",
    "        input_date = pd.to_datetime(earliest_week)\n",
    "        if len(all_drop_dates[all_drop_dates['Date'] > input_date]['Date']) > 0:\n",
    "            print(\"     There is an existing P drop, will swap.\")\n",
    "            next_drop_date = pd.to_datetime(all_drop_dates[all_drop_dates['Date'] > input_date]['Date'].min())\n",
    "            print(\"     Swapping drop on\", next_drop_date)\n",
    "        else: \n",
    "            print(\"     No existing P drop, we will add in a drop.\")\n",
    "            print(\"     The earliest available week to test with proper gapping is\", next_drop_date)\n",
    "\n",
    "    \n",
    "    if (CT_eligibility_Pub['P Available to Swap to CT'].iloc[0] != 1):\n",
    "        print(\"     No existing P drop, we will add in a drop.\")\n",
    "        earliest_week = CT_eligibility_Pub['Date'].iloc[0]\n",
    "        print(\"     The earliest available week to test with proper gapping is\", next_drop_date)\n",
    "\n",
    "    the_date = max(pd.to_datetime(default_earliest_date), \n",
    "                   pd.to_datetime(next_drop_date), \n",
    "                   pd.to_datetime(earliest_week)\n",
    "                  )\n",
    "    \n",
    "    # check to make sure it's on a weekday and not on a restricted day\n",
    "    the_date = adjust_date(the_date, offer)\n",
    "\n",
    "    print(\"     Earliest available drop is\", the_date)\n",
    "    \n",
    "    if the_date.date() >= dt.date.today()+ timedelta(end_date):\n",
    "        print(\"     This account is NOT eligible to CT this offer.\")\n",
    "        print(\" \")\n",
    "        result = 0\n",
    "    elif the_date.date() < dt.date.today() + timedelta(end_date):\n",
    "        print(\"     This account is eligible to CT this offer.\")\n",
    "        print(\" \")\n",
    "        result = 1       \n",
    "    return the_date, result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a9d474fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# at the moment, we don't have content feedback report, I need to import Lanina and find out the content that is waiting for test. \n",
    "def read_content_feedback_report(directory_path):\n",
    "\n",
    "    # List all files and directories in the specified directory\n",
    "    contents = os.listdir(directory_path)\n",
    "    \n",
    "    today = date.today().strftime(\"%m_%d_%Y\")\n",
    "    filename_date = f'Content_Feedback_Loop_Report_{today}.xlsx' \n",
    "\n",
    "    df = pd.DataFrame(contents, columns=['File Name'])\n",
    "    filtered_df = df[df['File Name'].str.contains(filename_date)]\n",
    "    filtered_df['Date'] = filtered_df['File Name'].str.extract(r'(\\d{2}_\\d{2}_\\d{4})')\n",
    "    filtered_df['Date'] = pd.to_datetime(filtered_df['Date'], format='%m_%d_%Y')\n",
    "    filtered_df = filtered_df.sort_values(by = 'Date', ascending = False)\n",
    "    file_name = str(filtered_df['File Name'].iloc[0])\n",
    "    \n",
    "    content_feedback_report = pd.read_excel(directory_path+file_name, sheet_name = 'Awaiting Testing -  Live')\n",
    "\n",
    "    return content_feedback_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f812d71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_P_drops(df):\n",
    "\n",
    "    # Get the current date\n",
    "    current_date = datetime.today()\n",
    "\n",
    "    # Calculate the date 1 week prior\n",
    "    one_week_prior = current_date - timedelta(days=7)\n",
    "\n",
    "    # Filter the dataframe to include only rows within the date range and where the 'Send Strategy' column contains 'P'\n",
    "    filtered_df = df[(df['Date'] >= one_week_prior) & (df['Date'] <= current_date) & (df['Send Strategy'] == 'P')]\n",
    "\n",
    "    # Group the filtered dataframe by 'Date' and count the occurrences of 'P' per day\n",
    "    p_counts = filtered_df.groupby(['shortcode_DP.SV','Date'])['Send Strategy'].count().reset_index()\n",
    "    average_frequency = p_counts.groupby(['shortcode_DP.SV'])['Send Strategy'].mean().reset_index()\n",
    "\n",
    "    # Calculate the average frequency of 'P' per day\n",
    "    average_frequency['Send Strategy'] = np.floor(average_frequency['Send Strategy'])\n",
    "    \n",
    "    Account_P_Max = dict(zip(average_frequency['shortcode_DP.SV'], average_frequency['Send Strategy']))\n",
    "\n",
    "    return Account_P_Max\n",
    "def choose_drop_slot(cobra, account, date, offer, P_drop_dict, eligibility_df):\n",
    "\n",
    "    # get schedule that day in that account\n",
    "    cobra_pub = cobra[cobra['shortcode_DP.SV'] == account]\n",
    "    cobra_pub = cobra_pub[cobra_pub['Date'] == date]\n",
    "    \n",
    "    # get max P drop for the account\n",
    "    Min_P = 1\n",
    "    Max_P = int(P_drop_dict.get(account, 3))\n",
    "    all_P_slots = list(range(2, 10+1))\n",
    "    restricted_p_slots = list(range(2, Max_P+2))\n",
    "\n",
    "    # if drop exists on that day, overwrite\n",
    "    cobra_pub_offer = cobra_pub[cobra_pub['Hitpath Offer ID'] == offer]\n",
    "    if len(cobra_pub_offer) > 0:\n",
    "        drop_slot = cobra_pub_offer['Drop']\n",
    "\n",
    "    elif len(cobra_pub_offer) < 1:\n",
    "\n",
    "        \"\"\" \n",
    "        # if P eligbility is 1, then it can test within normal P slots\n",
    "        # In email: if the offer we sent in the last 30 days, we think the CT drops can directly be sent in the P drops, otherwise, we will send it extra drop outside the maximum P drops\n",
    "        # In sms: If the offer we didn't schedule but need CT, we consider everything as normal P drops (Offset and limit a test drop in drop 2)\n",
    "        if eligibility_df['Can it Send in P Slot?'].iloc[0] == 1:\n",
    "            cobra_pub_2 = cobra_pub[cobra_pub['Hitpath Offer ID']=='nan']\n",
    "            cobra_pub_2 = cobra_pub_2.sort_values(by = 'Drop', ascending = True)\n",
    "            open_drops = list(cobra_pub_2['Drop'])\n",
    "            open_drops = list(map(int, open_drops)) \n",
    "            drop_slot = min([x for x in open_drops if x >= Min_P])\n",
    "        \n",
    "        # if P eligibility is 0, then it will test outside normal P slots\n",
    "        if eligibility_df['Can it Send in P Slot?'].iloc[0] == 0:\n",
    "            cobra_pub_2 = cobra_pub[cobra_pub['Hitpath Offer ID']=='nan']\n",
    "            cobra_pub_2 = cobra_pub_2.sort_values(by = 'Drop', ascending = True)\n",
    "            open_drops = list(cobra_pub_2['Drop'])\n",
    "            open_drops = list(map(int, open_drops))\n",
    "            drop_slot = min([x for x in open_drops if x > Max_P])\n",
    "        \"\"\"\n",
    "        cobra_pub_2 = cobra_pub[((cobra_pub['Hitpath Offer ID']=='nan')| (cobra_pub['Hitpath Offer ID'] == '')) & (cobra_pub['Drop'] == 2)]\n",
    "        if len(cobra_pub_2) > 0:\n",
    "            drop_slot = 2\n",
    "        else: \n",
    "            print(\"The day is full with Test drop\")\n",
    "\n",
    "        # if neither 1 or 0 somehow, then test outside normal P slots\n",
    "\n",
    "    else:\n",
    "        print(\"help me pls\")\n",
    "\n",
    "\n",
    "    return int(drop_slot)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "890db6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cobra_slot(cobra, account, date, offer, Drop_Number, CCIDs):\n",
    "    global limit_size\n",
    "    result = 1\n",
    "    cobra_slot = cobra[(cobra['shortcode_DP.SV'] == account) & (cobra['Date'] == date) & (cobra['Drop'] == 1)]\n",
    "    sc = account.split('_')[0]\n",
    "    # Time - check orginal, if not, then use default\n",
    "    time = cobra_slot['Time'].values[0]\n",
    "    if time == '':\n",
    "        time = cobra[(cobra['shortcode_DP.SV'] == account) & (cobra['Time']!='') & (cobra['Drop'] == 1)].sort_values(by = 'Date', ascending = False)['Time'].values[0]\n",
    "    # Segment - unified \n",
    "    segment = cobra_slot['Segment '].values[0]\n",
    "    if segment == '':\n",
    "        segment = cobra[(cobra['shortcode_DP.SV'] == account) & (cobra['Segment ']!='') & (cobra['Drop'] == 1)].sort_values(by = 'Date', ascending = False)['Segment '].values[0]\n",
    "    # Offer - unified\n",
    "    ss_offer = infrastructure.get_ss_offer()\n",
    "    try: \n",
    "        campaign = ss_offer.loc[(ss_offer['SS Offers (updated)'].str.contains(offer, na = False)) & (ss_offer['SS Offers (updated)'].str[-4:].str.contains(sc, na = False)), 'SS Offers (updated)' ].values[0]\n",
    "    except: \n",
    "        print(\"The offer is not added in the SS.\")\n",
    "    # Job Name - unified \n",
    "    date1 = date.strftime(\"%d%b%y\")  \n",
    "    job_name = \"SS_\"+segment[:3] + \"_\"+segment[4:].replace(\".\",'-').replace(\"_\",'-')+\"_\"+ offer +\"_\"+ \"CT\" + \"_\" +   date1\n",
    "    # Offset - seperate, keep first one from original \n",
    "    # Creative - seperate, keep first one from original\n",
    "    if Drop_Number == 1:\n",
    "        limit = cobra_slot['Limit'].values[0]\n",
    "        offset  =  cobra_slot['Offset'].values[0]\n",
    "    else: \n",
    "        get_limit_size = limit_size(offer_id_test = offer ,df = lexi,account = account)\n",
    "        result = get_limit_size[1] \n",
    "        limit = get_limit_size[0]\n",
    "        offset = 0 \n",
    "    # Creative \n",
    "    creative = cobra_slot['Creative'].values[0]\n",
    "    if (creative !='') & (Drop_Number == 1):\n",
    "        creative = creative.split('\\n')[0] + \"\\n\" + CCIDs\n",
    "    else: \n",
    "        creative = CCIDs\n",
    "    \n",
    "    return time, segment, campaign, limit,offset, creative,job_name,result \n",
    "        \n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e27a476f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define limit and offset size if we want to add a new CT drop \n",
    "def limit_size(offer_id_test,df,account):\n",
    "    #input\n",
    "    test_offer_info = offer_wk.loc[offer_wk['Hitpath Offer ID'] == offer_id_test]\n",
    "    payout = float(test_offer_info['$ Payout'].replace(r'[^0-9\\.]','',regex=True))\n",
    "    vertical = offer_wk.loc[offer_wk['Hitpath Offer ID'] == offer_id_test, 'Vertical'].iloc[0]\n",
    "    if(payout  > 50.00):\n",
    "        payout = payout/2\n",
    "    # payout = 8.00\n",
    "    \n",
    "    expected_revenue = payout*1.6+20\n",
    "    #last 60 days of data\n",
    "    last60days= pd.Timestamp(dt.date.today()-dt.timedelta(days=60)).strftime('%Y-%m-%d')\n",
    "    df = df[df['Date']>=last60days]\n",
    "    df['Hitpath Offer ID'] = df['Hitpath Offer ID'].astype(str)\n",
    "    df['Hitpath Offer ID'] = df['Hitpath Offer ID'].replace(r'\\.0$', '', regex=True)\n",
    "    df = df[df['Send Strategy'] != 'AR']\n",
    "    #median delivered volume by shortcode_DP.SV\n",
    "    median_delivered = df.groupby(['Date','shortcode_DP.SV'])['Delivered'].sum().reset_index()\n",
    "    median_delivered = median_delivered.groupby('shortcode_DP.SV')['Delivered'].median().reset_index()\n",
    "    #eCPM by shortcode_DP.SV\n",
    "    eCPM_df = df.groupby('shortcode_DP.SV').agg({'Revenue':'sum','Delivered':'sum'})\n",
    "    eCPM_df['eCPM'] = eCPM_df['Revenue'] * 1000 / eCPM_df['Delivered']\n",
    "    #merge eCPM and median deliverd\n",
    "    merged_segment_size = median_delivered.merge(eCPM_df,how = 'right', on = 'shortcode_DP.SV')\n",
    "    merged_segment_size = merged_segment_size.rename(columns=\n",
    "                                                    {\"Delivered_x\": \"Median Delivered\", \n",
    "                                                    \"Delivered_y\": \"Total Delivered\"})\n",
    "    merged_segment_size['segment_size'] = expected_revenue * 1000 / merged_segment_size['eCPM']\n",
    "    merged_segment_size['segment_size'] -= merged_segment_size['segment_size'] % -100\n",
    "    merged_segment_size['% Testing Size'] = merged_segment_size['segment_size']/ merged_segment_size['Median Delivered']\n",
    "    merged_segment_size['% Testing Size'] = merged_segment_size['% Testing Size'].fillna(0)\n",
    "    proper_segment = merged_segment_size.loc[(merged_segment_size['% Testing Size'] < 0.5),]\n",
    "    if len(proper_segment) <2:\n",
    "        proper_segment['segment_size'] = 4000\n",
    "        proper_segment['% Testing Size'] = proper_segment['segment_size']/ proper_segment['Median Delivered']\n",
    "        proper_segment = proper_segment.loc[(proper_segment['% Testing Size'] < 0.5),]\n",
    "    limit = proper_segment.loc[(proper_segment['shortcode_DP.SV'] == account) &(proper_segment['% Testing Size'] < 0.5), 'segment_size'].values[0] \n",
    "    if limit is None :\n",
    "        print(\"     This account is NOT eligible to CT this offer because of segment size.\")\n",
    "        print(\" \")\n",
    "        result = 0\n",
    "    else:\n",
    "        result = 1       \n",
    "    return limit, result \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9ad0c8",
   "metadata": {},
   "source": [
    "## Driver Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fbd198c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mamba are loaded.\n",
      "Starting process to create CCID testing schedule.\n",
      "------------------------------------------------------------------------\n",
      "Generating Schedule for CCIDs using Hitpath Offer ID: 12089\n",
      "------------------------------------------------------------------------\n",
      "12089 None\n",
      "------------------------------------------------------------------------\n",
      "Generating Schedule for CCIDs using Hitpath Offer ID: 12089\n",
      "------------------------------------------------------------------------\n",
      "12089 None\n",
      "------------------------------------------------------------------------\n",
      "Generating Schedule for CCIDs using Hitpath Offer ID: 12113\n",
      "------------------------------------------------------------------------\n",
      "12113 Loan\n",
      "#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#\n",
      "Searching for available date for FLC_EDM.247L\n",
      "     No existing P drop, we will add in a drop.\n",
      "     The earliest available week to test with proper gapping is 2024-03-29 00:00:00\n",
      "     Earliest available drop is 2024-03-29 00:00:00\n",
      "     This account is eligible to CT this offer.\n",
      " \n",
      "#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#\n",
      "Searching for available date for FLC_NPD.RTO\n",
      "     No existing P drop, we will add in a drop.\n",
      "     The earliest available week to test with proper gapping is 2024-03-21\n",
      "     Earliest available drop is 2024-03-21 00:00:00\n",
      "     This account is eligible to CT this offer.\n",
      " \n",
      "* FAILED ON FLC_NPD.RTO *\n",
      "#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#\n",
      "Searching for available date for FLC_I.RC\n",
      "     No existing P drop, we will add in a drop.\n",
      "     The earliest available week to test with proper gapping is 2024-03-21\n",
      "     Earliest available drop is 2024-03-21 00:00:00\n",
      "     This account is eligible to CT this offer.\n",
      " \n",
      "* FAILED ON FLC_I.RC *\n",
      "#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#\n",
      "Searching for available date for FLC_JET.ZTA\n",
      "     No existing P drop, we will add in a drop.\n",
      "     The earliest available week to test with proper gapping is 2024-03-21\n",
      "     Earliest available drop is 2024-03-21 00:00:00\n",
      "     This account is eligible to CT this offer.\n",
      " \n",
      "* FAILED ON FLC_JET.ZTA *\n",
      "#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#\n",
      "Searching for available date for FLC_PN.FC\n",
      "     No existing P drop, we will add in a drop.\n",
      "     The earliest available week to test with proper gapping is 2023-09-30 00:00:00\n",
      "     Earliest available drop is 2024-03-21 00:00:00\n",
      "     This account is eligible to CT this offer.\n",
      " \n",
      "* FAILED ON FLC_PN.FC *\n",
      "#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#\n",
      "Searching for available date for FLC_PN.SWP\n",
      "     No existing P drop, we will add in a drop.\n",
      "     The earliest available week to test with proper gapping is 2024-03-21\n",
      "     Earliest available drop is 2024-03-21 00:00:00\n",
      "     This account is eligible to CT this offer.\n",
      " \n",
      "** We have found 2 CTs ending search for 12113 **\n",
      " \n",
      " \n",
      "------------------------------------------------------------------------\n",
      "Generating Schedule for CCIDs using Hitpath Offer ID: 12113\n",
      "------------------------------------------------------------------------\n",
      "12113 Loan\n",
      "#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#\n",
      "Searching for available date for SVT_AL.PL.2\n",
      "     No existing P drop, we will add in a drop.\n",
      "     The earliest available week to test with proper gapping is 2024-03-27 00:00:00\n",
      "     Earliest available drop is 2024-03-27 00:00:00\n",
      "     This account is eligible to CT this offer.\n",
      " \n",
      "* FAILED ON SVT_AL.PL.2 *\n",
      "#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#\n",
      "Searching for available date for SVT_AMD.PL\n",
      "     No existing P drop, we will add in a drop.\n",
      "     The earliest available week to test with proper gapping is 2024-03-25 00:00:00\n",
      "     Earliest available drop is 2024-03-25 00:00:00\n",
      "     This account is eligible to CT this offer.\n",
      " \n",
      "* FAILED ON SVT_AMD.PL *\n",
      "#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#\n",
      "Searching for available date for SVT_AL.PL.3\n",
      "     No existing P drop, we will add in a drop.\n",
      "     The earliest available week to test with proper gapping is 2024-03-25 00:00:00\n",
      "     Earliest available drop is 2024-03-25 00:00:00\n",
      "     This account is eligible to CT this offer.\n",
      " \n",
      "* FAILED ON SVT_AL.PL.3 *\n",
      "#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#\n",
      "Searching for available date for SVT_AL.PL\n",
      "     No existing P drop, we will add in a drop.\n",
      "     The earliest available week to test with proper gapping is 2024-03-26 00:00:00\n",
      "     Earliest available drop is 2024-03-26 00:00:00\n",
      "     This account is eligible to CT this offer.\n",
      " \n",
      "* FAILED ON SVT_AL.PL *\n",
      "#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#\n",
      "Searching for available date for SVT_B2.F\n",
      "     No existing P drop, we will add in a drop.\n",
      "     The earliest available week to test with proper gapping is 2024-01-15 00:00:00\n",
      "     Earliest available drop is 2024-03-21 00:00:00\n",
      "     This account is eligible to CT this offer.\n",
      " \n",
      "* FAILED ON SVT_B2.F *\n",
      "------------------------------------------------------------------------\n",
      "Generating Schedule for CCIDs using Hitpath Offer ID: 12710\n",
      "------------------------------------------------------------------------\n",
      "12710 None\n",
      "------------------------------------------------------------------------\n",
      "Generating Schedule for CCIDs using Hitpath Offer ID: 12762\n",
      "------------------------------------------------------------------------\n",
      "12762 Home Warranty\n",
      "#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#\n",
      "Searching for available date for MBC_I.RC\n",
      "     No existing P drop, we will add in a drop.\n",
      "     The earliest available week to test with proper gapping is 2024-03-21 00:00:00\n",
      "* FAILED ON MBC_I.RC *\n",
      "#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#\n",
      "Searching for available date for MBC_PN.FC\n",
      "     No existing P drop, we will add in a drop.\n",
      "     The earliest available week to test with proper gapping is 2024-03-08 00:00:00\n",
      "* FAILED ON MBC_PN.FC *\n",
      "#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#\n",
      "Searching for available date for MBC_NPD.RTO\n",
      "     No existing P drop, we will add in a drop.\n",
      "     The earliest available week to test with proper gapping is 2024-03-11 00:00:00\n",
      "* FAILED ON MBC_NPD.RTO *\n",
      "#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#\n",
      "Searching for available date for MBC_PA.PS\n",
      "     No existing P drop, we will add in a drop.\n",
      "     The earliest available week to test with proper gapping is 2024-03-20 00:00:00\n",
      "* FAILED ON MBC_PA.PS *\n",
      "#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#\n",
      "Searching for available date for MBC_EDM.247L\n",
      "     No existing P drop, we will add in a drop.\n",
      "     The earliest available week to test with proper gapping is 2024-02-10 00:00:00\n",
      "* FAILED ON MBC_EDM.247L *\n",
      "#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#\n",
      "Searching for available date for MBC_CM.OSR\n",
      "     No existing P drop, we will add in a drop.\n",
      "     The earliest available week to test with proper gapping is 2024-03-21\n",
      "* FAILED ON MBC_CM.OSR *\n",
      "#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#\n",
      "Searching for available date for MBC_PN.SWP\n",
      "     No existing P drop, we will add in a drop.\n",
      "     The earliest available week to test with proper gapping is 2024-03-10 00:00:00\n",
      "* FAILED ON MBC_PN.SWP *\n",
      "#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#\n",
      "Searching for available date for MBC_WWM.YFA.2\n",
      "     No existing P drop, we will add in a drop.\n",
      "     The earliest available week to test with proper gapping is 2024-03-06 00:00:00\n",
      "* FAILED ON MBC_WWM.YFA.2 *\n",
      "------------------------------------------------------------------------\n",
      "Generating Schedule for CCIDs using Hitpath Offer ID: 12762\n",
      "------------------------------------------------------------------------\n",
      "12762 Home Warranty\n",
      "#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#\n",
      "Searching for available date for HZB_CM.OSR\n",
      "     There is an existing P drop, will swap.\n",
      "     Swapping drop on 2024-03-24 00:00:00\n",
      "* FAILED ON HZB_CM.OSR *\n",
      "#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#\n",
      "Searching for available date for HZB_AMD.PL\n",
      "     No existing P drop, we will add in a drop.\n",
      "     The earliest available week to test with proper gapping is 2024-03-28 00:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* FAILED ON HZB_AMD.PL *\n",
      "#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#\n",
      "Searching for available date for HZB_FSM.YS\n",
      "     No existing P drop, we will add in a drop.\n",
      "     The earliest available week to test with proper gapping is 2024-03-21 00:00:00\n",
      "* FAILED ON HZB_FSM.YS *\n",
      "#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#\n",
      "Searching for available date for HZB_B2.F\n",
      "     No existing P drop, we will add in a drop.\n",
      "     The earliest available week to test with proper gapping is 2024-03-20 00:00:00\n",
      "* FAILED ON HZB_B2.F *\n",
      "#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#\n",
      "Searching for available date for HZB_PA.PS\n",
      "     No existing P drop, we will add in a drop.\n",
      "     The earliest available week to test with proper gapping is 2024-03-28 00:00:00\n",
      "* FAILED ON HZB_PA.PS *\n",
      "#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#\n",
      "Searching for available date for HZB_AL.PL.3\n",
      "     No existing P drop, we will add in a drop.\n",
      "     The earliest available week to test with proper gapping is 2024-03-27 00:00:00\n",
      "* FAILED ON HZB_AL.PL.3 *\n",
      "#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#\n",
      "Searching for available date for HZB_AL.PL\n",
      "     No existing P drop, we will add in a drop.\n",
      "     The earliest available week to test with proper gapping is 2024-03-10 00:00:00\n",
      "* FAILED ON HZB_AL.PL *\n",
      "#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#\n",
      "Searching for available date for HZB_RHD.CC\n",
      "     No existing P drop, we will add in a drop.\n",
      "     The earliest available week to test with proper gapping is 2024-03-08 00:00:00\n",
      "* FAILED ON HZB_RHD.CC *\n",
      "#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#\n",
      "Searching for available date for HZB_ZM.PL.2\n",
      "     No existing P drop, we will add in a drop.\n",
      "     The earliest available week to test with proper gapping is 2024-02-11 00:00:00\n",
      "* FAILED ON HZB_ZM.PL.2 *\n",
      "------------------------------------------------------------------------\n",
      "Generating Schedule for CCIDs using Hitpath Offer ID: 12762\n",
      "------------------------------------------------------------------------\n",
      "12762 Home Warranty\n",
      "#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#\n",
      "Searching for available date for UAA_SPK.SWP2\n",
      "     No existing P drop, we will add in a drop.\n",
      "     The earliest available week to test with proper gapping is 2024-03-20 00:00:00\n",
      "* FAILED ON UAA_SPK.SWP2 *\n",
      "#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#\n",
      "Searching for available date for UAA_SPK.CR2\n",
      "     No existing P drop, we will add in a drop.\n",
      "     The earliest available week to test with proper gapping is 2024-03-09 00:00:00\n",
      "* FAILED ON UAA_SPK.CR2 *\n",
      "#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#\n",
      "Searching for available date for UAA_FSM.YS\n",
      "     No existing P drop, we will add in a drop.\n",
      "     The earliest available week to test with proper gapping is 2024-02-09 00:00:00\n",
      "* FAILED ON UAA_FSM.YS *\n",
      "------------------------------------------------------------------------\n",
      "Generating Schedule for CCIDs using Hitpath Offer ID: 12762\n",
      "------------------------------------------------------------------------\n",
      "12762 Home Warranty\n",
      "#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#\n",
      "Searching for available date for SVT_AMD.PL\n",
      "     No existing P drop, we will add in a drop.\n",
      "     The earliest available week to test with proper gapping is 2024-03-28 00:00:00\n",
      "* FAILED ON SVT_AMD.PL *\n",
      "#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#\n",
      "Searching for available date for SVT_B2.F\n",
      "     No existing P drop, we will add in a drop.\n",
      "     The earliest available week to test with proper gapping is 2024-03-24 00:00:00\n",
      "* FAILED ON SVT_B2.F *\n",
      "#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#\n",
      "Searching for available date for SVT_AL.PL.2\n",
      "     No existing P drop, we will add in a drop.\n",
      "     The earliest available week to test with proper gapping is 2024-03-10 00:00:00\n",
      "* FAILED ON SVT_AL.PL.2 *\n",
      "#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#\n",
      "Searching for available date for SVT_AL.PL\n",
      "     No existing P drop, we will add in a drop.\n",
      "     The earliest available week to test with proper gapping is 2024-03-10 00:00:00\n",
      "* FAILED ON SVT_AL.PL *\n",
      "#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#\n",
      "Searching for available date for SVT_AL.PL.3\n",
      "     No existing P drop, we will add in a drop.\n",
      "     The earliest available week to test with proper gapping is 2024-02-19 00:00:00\n",
      "* FAILED ON SVT_AL.PL.3 *\n",
      "------------------------------------------------------------------------\n",
      "Generating Schedule for CCIDs using Hitpath Offer ID: 12762\n",
      "------------------------------------------------------------------------\n",
      "12762 Home Warranty\n",
      "#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#\n",
      "Searching for available date for DSS_RHD.CC\n",
      "     No existing P drop, we will add in a drop.\n",
      "     The earliest available week to test with proper gapping is 2024-02-25 00:00:00\n",
      "* FAILED ON DSS_RHD.CC *\n",
      "#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#\n",
      "Searching for available date for DSS_ZM.PL.2\n",
      "     No existing P drop, we will add in a drop.\n",
      "     The earliest available week to test with proper gapping is 2024-02-10 00:00:00\n",
      "* FAILED ON DSS_ZM.PL.2 *\n",
      "------------------------------------------------------------------------\n",
      "Generating Schedule for CCIDs using Hitpath Offer ID: 12762\n",
      "------------------------------------------------------------------------\n",
      "12762 Home Warranty\n",
      "#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#\n",
      "Searching for available date for FLC_PN.SWP\n",
      "     No existing P drop, we will add in a drop.\n",
      "     The earliest available week to test with proper gapping is 2024-03-22 00:00:00\n",
      "* FAILED ON FLC_PN.SWP *\n",
      "#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#\n",
      "Searching for available date for FLC_EDM.247L\n",
      "     No existing P drop, we will add in a drop.\n",
      "     The earliest available week to test with proper gapping is 2024-03-25 00:00:00\n",
      "* FAILED ON FLC_EDM.247L *\n",
      "#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#\n",
      "Searching for available date for FLC_I.RC\n",
      "     No existing P drop, we will add in a drop.\n",
      "     The earliest available week to test with proper gapping is 2024-03-11 00:00:00\n",
      "* FAILED ON FLC_I.RC *\n",
      "#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#\n",
      "Searching for available date for FLC_WWM.YFA.2\n",
      "     No existing P drop, we will add in a drop.\n",
      "     The earliest available week to test with proper gapping is 2024-02-13 00:00:00\n",
      "* FAILED ON FLC_WWM.YFA.2 *\n",
      "#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#\n",
      "Searching for available date for FLC_PN.FC\n",
      "     No existing P drop, we will add in a drop.\n",
      "     The earliest available week to test with proper gapping is 2024-02-04 00:00:00\n",
      "* FAILED ON FLC_PN.FC *\n",
      "#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#--#\n",
      "Searching for available date for FLC_NPD.RTO\n",
      "     No existing P drop, we will add in a drop.\n",
      "     The earliest available week to test with proper gapping is 2024-03-21\n",
      "* FAILED ON FLC_NPD.RTO *\n",
      "\u001b[1mCCID SCHEDULE GENERATION IS DONE, PLEASE CHECK OUTPUT BEFORE RUNNING SCHEDULE CODE.\n"
     ]
    }
   ],
   "source": [
    "reading_cobra()\n",
    "cobra['Drop'] = cobra['Drop'].str.replace('Drop ', '').astype(int)\n",
    "Account_P_Max = get_max_P_drops(df = cobra)\n",
    "content_feedback_report = read_content_feedback_report(\n",
    "                    directory_path = '/Users/garrettchao/Desktop/contentfeedbackloop/Content Feedback Loop Reports/')\n",
    "\n",
    "content_feedback_report = content_feedback_report.rename(columns={'HitPath ID': 'Hitpath Offer ID'})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#- Functions -#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "print(\"Starting process to create CCID testing schedule.\")\n",
    "\n",
    "# create empty df to store schedule\n",
    "\n",
    "columns = [\n",
    "    'Date',\n",
    "    'Affiliate ID_DP.DS',\n",
    "    'Drop Number',\n",
    "    'Time',\n",
    "    'Segment',\n",
    "    'Send Strategy',\n",
    "    'Offer',\n",
    "    'Limit',\n",
    "    'Offset',\n",
    "    'Creative',\n",
    "    'Job Name',\n",
    "    'Can it Test?'\n",
    "]\n",
    "\n",
    "# Create an empty DataFrame\n",
    "upcoming_CT_schedule = pd.DataFrame()\n",
    "content_feedback_report['Hitpath Offer ID'] = content_feedback_report['Hitpath Offer ID'].astype('str').str.split('.',expand = True)[0]\n",
    "content_feedback_report = content_feedback_report[content_feedback_report['Hitpath Offer ID']!='nan']\n",
    "for index, row in content_feedback_report.iterrows():\n",
    "#for hitpath_offer_ID in content_feedback_report['Hitpath Offer ID']:\n",
    "\n",
    "    # drop, CCID, hitpath info\n",
    "    hitpath_offer_ID = row['Hitpath Offer ID']\n",
    "    CCIDs_to_Test = row[\"Content ID's\"]\n",
    "    sc = row['Shortcode']\n",
    "    #days_restrictions = offer_wk[offer_wk['Hitpath Offer ID'] == hitpath_offer_ID]['Day Restrictions'].iloc[0]\n",
    "    \n",
    "    print(\"------------------------------------------------------------------------\")\n",
    "    print(\"Generating Schedule for CCIDs using Hitpath Offer ID:\", hitpath_offer_ID)\n",
    "    print(\"------------------------------------------------------------------------\")\n",
    "    \n",
    "    #offer_vertical = offer_wk[offer_wk['Hitpath Offer ID'] == hitpath_offer_ID]['Vertical'].iloc[0]\n",
    "    \n",
    "    filtered_df = offer_wk[offer_wk['Hitpath Offer ID'] == hitpath_offer_ID]\n",
    "\n",
    "    # check if the filtered DataFrame is not empty\n",
    "    if not filtered_df.empty:\n",
    "        offer_vertical = filtered_df['Vertical'].iloc[0]\n",
    "    else:\n",
    "        offer_vertical = None\n",
    "    \n",
    "    if (offer_vertical == 'Loan') & (sc == 'UAA'):\n",
    "        print(\"We can't test loan offer in UAA\")\n",
    "    else: \n",
    "        offer_gaps_df = get_offer_vertical_gaps(df = lexi)\n",
    "        lexi_sc = lexi[lexi['Shortcode Name'] == sc]\n",
    "        account_rankings_df = get_account_rankings(df = lexi_sc, offer = hitpath_offer_ID,cobra = cobra)\n",
    "        DPPub_test = list(account_rankings_df['shortcode_DP.SV'])\n",
    "        \n",
    "        \n",
    "        success_counter = 0\n",
    "        for DPPub in DPPub_test:\n",
    "\n",
    "            try:\n",
    "                CT_eligibility_df = check_CT_eligiblity(\n",
    "                                                    default_max_drops = 2,\n",
    "                                                    lexi = lexi,\n",
    "                                                    cobra = cobra,\n",
    "                                                    offer = hitpath_offer_ID)\n",
    "                drop_date_info = get_drop_date(offer_gaps_df = offer_gaps_df,\n",
    "                                            CT_eligibility_df = CT_eligibility_df,\n",
    "                                                account = DPPub, \n",
    "                                            offer = hitpath_offer_ID, \n",
    "                                            cobra = cobra, \n",
    "                                            end_date = 14)\n",
    "                drop_date = drop_date_info[0]\n",
    "                \n",
    "                # drop info to put into auto scheduler\n",
    "                Date_output = drop_date\n",
    "                DPPub_output = DPPub\n",
    "                \n",
    "                Drop_Number_output = choose_drop_slot(cobra = cobra,\n",
    "                                                    account = DPPub,\n",
    "                                                    date = pd.Timestamp(Date_output),\n",
    "                                                    offer = hitpath_offer_ID,\n",
    "                                                    P_drop_dict = Account_P_Max,\n",
    "                                                    eligibility_df = CT_eligibility_df)\n",
    "                cobra_output = get_cobra_slot(cobra = cobra,\n",
    "                                        account = DPPub,\n",
    "                                        date = pd.Timestamp(Date_output),\n",
    "                                        offer = hitpath_offer_ID,\n",
    "                                        Drop_Number = Drop_Number_output,\n",
    "                                        CCIDs = CCIDs_to_Test)  \n",
    "\n",
    "                #Drop_Number_output = 1\n",
    "                Time_output = cobra_output[0]\n",
    "                Segment = cobra_output[1]\n",
    "                Send_Strategy_output = 'CT'\n",
    "                Hitpath_OfferID_output = hitpath_offer_ID\n",
    "                #Scheduling_Name_output = offer_wk[offer_wk['Hitpath Offer ID'] == hitpath_offer_ID]['Scheduling Name'].iloc[0] # NEED TO CHANGE TO SMS \n",
    "                Offer_output = cobra_output[2]\n",
    "                Limit_output = cobra_output[3]\n",
    "                Offset_output = cobra_output[4]\n",
    "                Creative_output = cobra_output[5]\n",
    "                Job_Name_output = cobra_output[6] \n",
    "                if cobra_output[7] == 0:\n",
    "                    drop_date_info[1] = 0 \n",
    "                Eligibility_output = + drop_date_info[1]\n",
    "                \n",
    "                # Update Cobra \n",
    "                ## so it does not repeat the same accounts if it's scheduling multiple batches of the same offer\n",
    "                cobra_row = pd.DataFrame({'Date':[Date_output],\n",
    "                                        'shortcode_DP.SV': [DPPub_output],\n",
    "                                        'Hitpath Offer ID': [hitpath_offer_ID],\n",
    "                                        'Campaign ID':[Offer_output],\n",
    "                                        'Send Strategy': [Send_Strategy_output],\n",
    "                                        'Drop': [Drop_Number_output]\n",
    "                                        })\n",
    "                \n",
    "                cobra.loc[((cobra['shortcode_DP.SV']==DPPub_output) & (cobra['Date']==Date_output) & (cobra['Drop'] ==Drop_Number_output )), 'Hitpath Offer ID'] = hitpath_offer_ID\n",
    "\n",
    "                drop_row = [[Date_output, DPPub_output, Drop_Number_output, Time_output, Segment, Send_Strategy_output,\n",
    "                            Offer_output, Limit_output, Offset_output, Creative_output, Job_Name_output, Eligibility_output]]\n",
    "                \n",
    "                upcoming_CT_schedule = upcoming_CT_schedule._append(drop_row, ignore_index=True)\n",
    "                \n",
    "                success_counter = success_counter + drop_date_info[1]\n",
    "                if success_counter == 2:\n",
    "                    print(\"** We have found\", success_counter, \"CTs\", \"ending search for\", hitpath_offer_ID, \"**\")\n",
    "                    print(\" \")\n",
    "                    print(\" \")\n",
    "                    break\n",
    "                    \n",
    "            except:\n",
    "                print(\"* FAILED ON\", DPPub, \"*\")\n",
    "            \n",
    " \n",
    "    # Rename the columns\n",
    "    #upcoming_CT_schedule.columns = columns \n",
    "upcoming_CT_schedule = upcoming_CT_schedule.rename(columns=dict(zip(upcoming_CT_schedule.columns, columns)))\n",
    "upcoming_CT_schedule = upcoming_CT_schedule[upcoming_CT_schedule['Can it Test?'] == 1]      \n",
    "upcoming_CT_schedule = upcoming_CT_schedule.drop('Can it Test?', axis=1)\n",
    "\n",
    "\n",
    "    #CT_pipeline_output = prep_for_CT_pipeline_smartsheet(df = upcoming_CT_schedule, offer_wk = offer_wk)\n",
    "upcoming_CT_schedule.to_csv('Content Testing Schedule.csv', index=False)  \n",
    "    #CT_pipeline_output.to_csv('Content Testing Pipeline Output.csv', index=False)\n",
    "\n",
    "    # upload content testing schedule to Google Sheet\n",
    "\n",
    "\n",
    "print(bold + \"CCID SCHEDULE GENERATION IS DONE, PLEASE CHECK OUTPUT BEFORE RUNNING SCHEDULE CODE.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b10f551f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Affiliate ID_DP.DS</th>\n",
       "      <th>Drop Number</th>\n",
       "      <th>Time</th>\n",
       "      <th>Segment</th>\n",
       "      <th>Send Strategy</th>\n",
       "      <th>Offer</th>\n",
       "      <th>Limit</th>\n",
       "      <th>Offset</th>\n",
       "      <th>Creative</th>\n",
       "      <th>Job Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-03-29</td>\n",
       "      <td>FLC_EDM.247L</td>\n",
       "      <td>2</td>\n",
       "      <td>10:00 AM PST</td>\n",
       "      <td>FLC_EDM.247L_30DC_NEXL</td>\n",
       "      <td>CT</td>\n",
       "      <td>12113 - AVL SL FLC</td>\n",
       "      <td>3500.0</td>\n",
       "      <td>0</td>\n",
       "      <td>12113.SC.FLC.453521</td>\n",
       "      <td>SS_FLC_EDM-247L-30DC-NEXL_12113_CT_29Mar24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-03-21</td>\n",
       "      <td>FLC_PN.SWP</td>\n",
       "      <td>2</td>\n",
       "      <td>10:00 AM PST</td>\n",
       "      <td>FLC_PN.SWP_37DC</td>\n",
       "      <td>CT</td>\n",
       "      <td>12113 - AVL SL FLC</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>12113.SC.FLC.453521</td>\n",
       "      <td>SS_FLC_PN-SWP-37DC_12113_CT_21Mar24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date Affiliate ID_DP.DS  Drop Number          Time  \\\n",
       "0 2024-03-29       FLC_EDM.247L            2  10:00 AM PST   \n",
       "1 2024-03-21         FLC_PN.SWP            2  10:00 AM PST   \n",
       "\n",
       "                  Segment Send Strategy               Offer   Limit  Offset  \\\n",
       "0  FLC_EDM.247L_30DC_NEXL            CT  12113 - AVL SL FLC  3500.0       0   \n",
       "1         FLC_PN.SWP_37DC            CT  12113 - AVL SL FLC  6000.0       0   \n",
       "\n",
       "              Creative                                    Job Name  \n",
       "0  12113.SC.FLC.453521  SS_FLC_EDM-247L-30DC-NEXL_12113_CT_29Mar24  \n",
       "1  12113.SC.FLC.453521         SS_FLC_PN-SWP-37DC_12113_CT_21Mar24  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upcoming_CT_schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e373945d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nfor index, row in content_feedback_report.iterrows():\\n\\n    # variables\\n    hitpath_offer_ID  = content_feedback_report[\\'HitPath Offer ID\\'].iloc[index]\\n    CCIDs  = content_feedback_report[\"Content ID\\'s\"].iloc[index]\\n    OfferType = content_feedback_report[\"Offer Type\"].iloc[index]\\n    Days_to_test = content_feedback_report[\"Days to Test\"].iloc[index]\\n\\n    print(\"------------------------------------------------------------------------\")\\n    print(\"Generating Schedule for CCIDs using Hitpath Offer ID:\", hitpath_offer_ID)\\n    print(\"------------------------------------------------------------------------\")\\n    \\n    offer_gaps_df = get_offer_vertical_gaps(df = lexi, EIMT = EIMT)\\n    account_rankings_df = get_account_rankings(df = lexi, offer = hitpath_offer_ID)\\n    DPPub_test = list(account_rankings_df[\\'DP&Pub\\'])\\n    \\n    success_counter = 0\\n    for DPPub in DPPub_test:\\n\\n        try:\\n            CT_eligibility_df = check_CT_eligiblity(CT_settings_url = \\'https://docs.google.com/spreadsheets/d/1TmWNe9MYmAB9s2MyH0UOOS1ZKdJKYl0AY6jgL6grHcc/edit#gid=1087982961\\',\\n                                                 default_max_drops = 3,\\n                                                 lexi = lexi,\\n                                                 cobra = cobra,\\n                                                 dataset_convertor = Dataset_to_DPPub,\\n                                                 offer = hitpath_offer_ID)\\n            \\n        except:\\n            print(\"* FAILED ON\", DPPub, \"*\")\\n            pass\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "for index, row in content_feedback_report.iterrows():\n",
    "\n",
    "    # variables\n",
    "    hitpath_offer_ID  = content_feedback_report['HitPath Offer ID'].iloc[index]\n",
    "    CCIDs  = content_feedback_report[\"Content ID's\"].iloc[index]\n",
    "    OfferType = content_feedback_report[\"Offer Type\"].iloc[index]\n",
    "    Days_to_test = content_feedback_report[\"Days to Test\"].iloc[index]\n",
    "\n",
    "    print(\"------------------------------------------------------------------------\")\n",
    "    print(\"Generating Schedule for CCIDs using Hitpath Offer ID:\", hitpath_offer_ID)\n",
    "    print(\"------------------------------------------------------------------------\")\n",
    "    \n",
    "    offer_gaps_df = get_offer_vertical_gaps(df = lexi, EIMT = EIMT)\n",
    "    account_rankings_df = get_account_rankings(df = lexi, offer = hitpath_offer_ID)\n",
    "    DPPub_test = list(account_rankings_df['DP&Pub'])\n",
    "    \n",
    "    success_counter = 0\n",
    "    for DPPub in DPPub_test:\n",
    "\n",
    "        try:\n",
    "            CT_eligibility_df = check_CT_eligiblity(CT_settings_url = 'https://docs.google.com/spreadsheets/d/1TmWNe9MYmAB9s2MyH0UOOS1ZKdJKYl0AY6jgL6grHcc/edit#gid=1087982961',\n",
    "                                                 default_max_drops = 3,\n",
    "                                                 lexi = lexi,\n",
    "                                                 cobra = cobra,\n",
    "                                                 dataset_convertor = Dataset_to_DPPub,\n",
    "                                                 offer = hitpath_offer_ID)\n",
    "            \n",
    "        except:\n",
    "            print(\"* FAILED ON\", DPPub, \"*\")\n",
    "            pass\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98767794",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
