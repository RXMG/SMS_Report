{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a81877ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import infrastructure\n",
    "import filepath \n",
    "import pygsheets\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "import openpyxl \n",
    "import os\n",
    "import gspread\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# use creds to create a client to interact with the Google Drive API\n",
    "gc = pygsheets.authorize(service_account_file=filepath.service_account_location)\n",
    "mamba = gc.open_by_url('https://docs.google.com/spreadsheets/d/12vqSDueybprNphtsw7gXR5vmgcPG6_5ZNcnWzNpiasY/edit#gid=1238872091') \n",
    "pipeline_test = gc.open_by_url(\"https://docs.google.com/spreadsheets/d/16vrHMWs0ambcBJ1sC0SqpYVG8SOSWlbg1N22-bF49v8/edit#gid=1963352944\")\n",
    "pipeline_test_wks = pipeline_test.worksheet('title','Testing Scheduler')\n",
    "pipeline_test_offer_info = pipeline_test.worksheet('title','Pub Info')\n",
    "oi_df = pipeline_test_offer_info.get_as_df()\n",
    "pl_df = pipeline_test_wks.get_as_df()\n",
    "# open worksheet - mamba\n",
    "schedule_wks  =  mamba.worksheet('title','New Mamba')\n",
    "schedule_df = schedule_wks.get_as_df()\n",
    "#la nina\n",
    "lanina = infrastructure.get_lanina()\n",
    "lanina1 =  lanina[~(lanina['Content Approval Status'].str.contains('Paused|Not Approved|Failed Testing', na = False)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a3c41e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Publisher Config\n",
    "gc = pygsheets.authorize(service_account_file=filepath.service_account_location)\n",
    "publisher_config = gc.open_by_url(\n",
    "    'https://docs.google.com/spreadsheets/d/1Tzda6Djr3zQmOhWu7Ief3GVR9Cjaml8238CeX7chj_U/edit#gid=1620368362') \n",
    "pub_config = publisher_config[0].get_as_df()\n",
    "pub_config.rename(columns={'DP.DS or DP.sV': 'DP.SV'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65692843",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:smartsheet.smartsheet:{\"request\": {\"command\": \"GET https://api.smartsheet.com/2.0/sheets/795F6FwrFH953PC5V2PHCVP9FG5R7CphWVcQ5RQ1?view=grid&filterId=5028946508730244\"}}\n"
     ]
    }
   ],
   "source": [
    "import smartsheet\n",
    "\n",
    "# Smartsheet setup\n",
    "smartsheet_token = '3g2rkftxrKUZfFaYx8roGBhXUKo1njSLp8vdX'\n",
    "smart = smartsheet.Smartsheet(smartsheet_token)\n",
    "\n",
    "# Smartsheet: Get data from a specific sheet\n",
    "sheet_id = '795F6FwrFH953PC5V2PHCVP9FG5R7CphWVcQ5RQ1?view=grid&filterId=5028946508730244'\n",
    "sheet = smart.Sheets.get_sheet(sheet_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab2317ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_and_construct_full_string(input_string, df):\n",
    "    # extract the parts of the input string\n",
    "    parts = input_string.split('_')\n",
    "    # Reconstruct the prefix including the second portion\n",
    "    prefix_including_second_portion = '_'.join(parts[:2])\n",
    "    \n",
    "    # Find rows in the dataframe where DP.SV contains the second portion\n",
    "    second_portion = parts[1]\n",
    "    matching_rows = df[df['DP.SV'].astype(str).str.contains(second_portion, na=False)]\n",
    "    \n",
    "    # Check if any row is found\n",
    "    if not matching_rows.empty:\n",
    "        # Use the first matching row for this example\n",
    "        first_match = matching_rows.iloc[0]\n",
    "        # Construct the new string using the reconstructed prefix and the PUBID from the first matching row\n",
    "        new_string = f\"{prefix_including_second_portion}_{int(first_match['PUBID'])}\"\n",
    "        return new_string\n",
    "    else:\n",
    "        return \"No matching PUBID found in the dataframe for the given portion.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d92d097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_offer(df, dp_sv, hpid):\n",
    "    code = dp_sv.split('_')[0]\n",
    "    \n",
    "    # Filter the DataFrame based on the code and check if the offer contains the extracted code\n",
    "    filtered_df = df[df['SS Offers (updated)'].str.startswith(hpid) & df['SS Offers (updated)'].str.contains(code)]\n",
    "    \n",
    "    # Check if any row matches the criteria\n",
    "    if not filtered_df.empty:\n",
    "        return filtered_df['SS Offers (updated)'].iloc[0]\n",
    "    else:\n",
    "        return \"No offer found.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b5d2c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_map = {col.id: col.title for col in sheet.columns}\n",
    "\n",
    "# Process the data\n",
    "processed_data = []\n",
    "for row in sheet.rows:\n",
    "    date_format = '%-m/%d/%y'\n",
    "    # Create a dictionary for each row mapping column names to cell values\n",
    "    row_data = {column_map[cell.column_id]: cell.value for cell in row.cells if cell.column_id in column_map}\n",
    "\n",
    "    # Check if the row has a \"Pending\" status in the 'Result' column\n",
    "    if row_data.get('Result') == 'Pending':\n",
    "        # Extract the required data for each drop\n",
    "        for i in range(1, 4):  # Assuming up to 3 drops\n",
    "            drop_date = row_data.get(f'Drop {i} Date')\n",
    "            drop_date = dt.datetime.strptime(drop_date, '%Y-%m-%d').strftime(date_format)\n",
    "            \n",
    "            drop_segment = row_data.get(f'Drop {i} Dataset_Segment')\n",
    "            dataset = match_and_construct_full_string(drop_segment, pub_config)\n",
    "            drop_number = \"2\"\n",
    "            time = \"10:00 AM PST\"\n",
    "            send_strategy = \"PT\"\n",
    "            if \"OW\" in str(row_data.get('Hitpath Offer ID')):\n",
    "                hitpath_offer_id = row_data.get('Hitpath Offer ID')\n",
    "            else:\n",
    "                hitpath_offer_id = str(int(row_data.get('Hitpath Offer ID')))\n",
    "            scheduling_name = find_offer(oi_df,dataset,hitpath_offer_id)\n",
    "            \n",
    "            offset = \"0\"\n",
    "            creative = \"\"\n",
    "            job_name = \"\"\n",
    "            \n",
    "            drop_limit = int(row_data.get(f'Drop {i} Limit'))\n",
    "           \n",
    "            # Only process rows where drop date is available\n",
    "            if drop_date:\n",
    "                processed_row = [drop_date, dataset, drop_number, time, drop_segment, send_strategy, scheduling_name,\n",
    "                                 drop_limit, offset, creative, job_name]\n",
    "                processed_data.append(processed_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aacc76fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['3/22/24',\n",
       "  'MBC_PN.SWP_461500',\n",
       "  '2',\n",
       "  '10:00 AM PST',\n",
       "  'MBC_PN.SWP_1PLD.30DC_TMO',\n",
       "  'PT',\n",
       "  '13070 - MBC',\n",
       "  2600,\n",
       "  '0',\n",
       "  '',\n",
       "  ''],\n",
       " ['3/22/24',\n",
       "  'MBC_PN.FC_461653',\n",
       "  '2',\n",
       "  '10:00 AM PST',\n",
       "  'MBC_PN.FC_21DC',\n",
       "  'PT',\n",
       "  '13070 - MBC',\n",
       "  2600,\n",
       "  '0',\n",
       "  '',\n",
       "  ''],\n",
       " ['3/22/24',\n",
       "  'FLC_EDM.247L_461227',\n",
       "  '2',\n",
       "  '10:00 AM PST',\n",
       "  'FLC_EDM.247L_30DC_NEXL',\n",
       "  'PT',\n",
       "  '13070 - FLC',\n",
       "  2500,\n",
       "  '0',\n",
       "  '',\n",
       "  '']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a235982",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_rows = pd.DataFrame(processed_data, columns=[\n",
    "    \"Testing Date\",  \n",
    "    \"Shortcode_DP.SV\",  \n",
    "    \"Drop Number\",  \n",
    "    \"Time\",\n",
    "    \"Segment\",\n",
    "    \"Send Strategy\",  \n",
    "    \"Offer\",\n",
    "    \"Limit\", \n",
    "    \"Offset\",\n",
    "    \"Creative\",\n",
    "    \"Job Name\"\n",
    "    \n",
    "])\n",
    "\n",
    "# Adjust data types to match original dataframe\n",
    "new_rows[\"Drop Number\"] = new_rows[\"Drop Number\"].astype(int)\n",
    "new_rows[\"Limit\"] = new_rows[\"Limit\"].astype(int)\n",
    "new_rows[\"Offset\"] = new_rows[\"Offset\"].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04e3d925",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_empty_row = len(pipeline_test_wks.get_col(1, include_tailing_empty=False)) + 1\n",
    "\n",
    "# Append the DataFrame to the worksheet\n",
    "pipeline_test_wks.set_dataframe(new_rows, (first_empty_row, 1), copy_head=False)  # (row, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da573ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_df = pipeline_test_wks.get_as_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aceebd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the row index in the schedule sheet\n",
    "def find_schedule_row(schedule_df, shortcode):\n",
    "    matching_rows = schedule_df[schedule_df['Dataset'] == shortcode].index\n",
    "    return matching_rows[0] if not matching_rows.empty else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b355e129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the column index for the date\n",
    "def find_date_column(schedule_wks, test_date):\n",
    "    for idx, col in enumerate(schedule_wks.get_row(2, include_tailing_empty=False)[3:], start=4):\n",
    "        if pd.to_datetime(col).date() == test_date.date():\n",
    "            return idx\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7df728e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_to_letters(column):\n",
    "    \"\"\"Convert a column number to a column letter.\"\"\"\n",
    "    string = \"\"\n",
    "    while column > 0:\n",
    "        column, remainder = divmod(column - 1, 26)\n",
    "        string = chr(65 + remainder) + string\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "956c26fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in pl_df.iterrows():  \n",
    "    if len(row['Offer'].split()) == 4:\n",
    "        sc = row['Offer'].split()[3]\n",
    "        hitpath = row['Offer'].split()[0]\n",
    "        df_content= lanina1[(lanina1['OfferIDs'] == hitpath) & (lanina1['Type'] == sc) & (lanina1['Channel'] =='SC')][['Reporting Content ID','Content Approval Status', 'Content']]  \n",
    "        ccid  = df_content['Reporting Content ID'].values[0]\n",
    "        col_idx = 10\n",
    "        col_letter = column_to_letters(col_idx)\n",
    "        cell_address = f\"{col_letter}{index+2}\"\n",
    "        pipeline_test_wks.update_value(cell_address, ccid)\n",
    "    elif len(row['Offer'].split()) == 3:\n",
    "        sc = row['Offer'].split()[2]\n",
    "        hitpath = row['Offer'].split()[0]\n",
    "        df_content= lanina1[(lanina1['OfferIDs'] == int(hitpath)) & (lanina1['Type'] == sc) & (lanina1['Channel'] =='SC')][['Reporting Content ID','Content Approval Status', 'Content']]  \n",
    "        ccid  = df_content['Reporting Content ID'].values[0]\n",
    "        col_idx = 10\n",
    "        col_letter = column_to_letters(col_idx)\n",
    "        cell_address = f\"{col_letter}{index+2}\"\n",
    "        pipeline_test_wks.update_value(cell_address, ccid)\n",
    "    else:\n",
    "        sc = row['Offer'].split()[4]\n",
    "        hitpath = row['Offer'].split()[0]\n",
    "        df_content= lanina1[(lanina1['OfferIDs'] == int(hitpath)) & (lanina1['Type'] == sc) & (lanina1['Channel'] =='SC')][['Reporting Content ID','Content Approval Status', 'Content']]  \n",
    "        ccid  = df_content['Reporting Content ID'].values[0]\n",
    "        col_idx = 10\n",
    "        col_letter = column_to_letters(col_idx)\n",
    "        cell_address = f\"{col_letter}{index+2}\"\n",
    "        pipeline_test_wks.update_value(cell_address, ccid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85a359ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_df = pipeline_test_wks.get_as_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3553a8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over pl_df and update the schedule sheet\n",
    "for index, row in pl_df.iterrows():\n",
    "    shortcode = row['Shortcode_DP.SV']\n",
    "    test_date = pd.to_datetime(row['Testing Date'])\n",
    "    drop_number = row['Drop Number']\n",
    "    time = row['Time']\n",
    "    segment = row['Segment']\n",
    "    send_strategy = row['Send Strategy']\n",
    "    offer = row['Offer']\n",
    "    limit = row['Limit']\n",
    "    offset = row['Offset']\n",
    "    \n",
    "    creative = row['Creative']\n",
    "    job_name = row['Job Name']\n",
    "    p_limit = \"Total - \"+(str(limit+1))\n",
    "    p_offset = str(limit+1)\n",
    "    \n",
    "    schedule_row_idx = find_schedule_row(schedule_df, shortcode)\n",
    "    date_col_idx = find_date_column(schedule_wks, test_date)\n",
    "\n",
    "    if schedule_row_idx is not None and date_col_idx is not None:\n",
    "        values_to_update = [time, segment, send_strategy, offer, limit, offset, creative, job_name]\n",
    "\n",
    "        # update cells with new data\n",
    "        for i, value in enumerate(values_to_update):\n",
    "            col_letter = column_to_letters(date_col_idx)\n",
    "            cell_address = f\"{col_letter}{schedule_row_idx + 12 + i}\"\n",
    "            schedule_wks.update_value(cell_address, value)\n",
    "        \n",
    "        cell_address = f\"{col_letter}{schedule_row_idx + 7}\"\n",
    "        schedule_wks.update_value(cell_address, p_limit)\n",
    "        cell_address = f\"{col_letter}{schedule_row_idx + 8}\"\n",
    "        schedule_wks.update_value(cell_address, p_offset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
