{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0084d56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import filepath\n",
    "import send_email \n",
    "import pygsheets\n",
    "import infrastructure \n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import filepath \n",
    "localfolder = filepath.output_folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "819879e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "current_date = datetime.now()\n",
    "two_months_ago = current_date - timedelta(days=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5c693fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sms_new = pd.read_csv(filepath.output_folder+'SS_LC_merged_data_new.csv')\n",
    "# #sms_new['Revenue'].sum()\n",
    "# revenue_sum = sms_new[sms_new['Date'].isna()]['Revenue'].sum()\n",
    "# revenue_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba09ba30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_4212\\2355827091.py:1: DtypeWarning: Columns (1,2,3,4,6,8,9,10,11,12,14,16,26,27,28,29,30,31,32,33,34,35,36,37,44,48,51,54,56,58,59,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  merged_data = pd.read_csv(filepath.output_folder+'SS_LC_merged_data.csv')\n"
     ]
    }
   ],
   "source": [
    "merged_data = pd.read_csv(filepath.output_folder+'SS_LC_merged_data.csv')\n",
    "merged_data['Date'] = pd.to_datetime(merged_data['Date'])\n",
    "twomonthsago_new = merged_data[(merged_data['Date'].isna()) | (merged_data['Date'] <= two_months_ago)]\n",
    "twomonthsago_new.to_csv(localfolder + 'SS_LC_merged_data_twomonthsago.csv', index =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05a5bbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6942aa51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\lilig\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pygsheets\\worksheet.py:1554: UserWarning: At least one column name in the data frame is an empty string. If this is a concern, please specify include_tailing_empty=False and/or ensure that each column containing data has a name.\n",
      "  warnings.warn('At least one column name in the data frame is an empty string. If this is a concern, please specify include_tailing_empty=False and/or ensure that each column containing data has a name.')\n",
      "D:\\Users\\lilig\\AppData\\Local\\anaconda3\\Lib\\site-packages\\dateutil\\parser\\_parser.py:1207: UnknownTimezoneWarning: tzname EST identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15029726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\lilig\\AppData\\Local\\anaconda3\\Lib\\site-packages\\dateutil\\parser\\_parser.py:1207: UnknownTimezoneWarning: tzname EST identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_4212\\845277884.py:170: DtypeWarning: Columns (7,9,10,11,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  combined_csv = pd.read_csv(localfolder + 'SMS_master_revenue.csv', dtype={'advertiser_name':'str','campaign_name':'str'})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7426282.401051002\n",
      "7426282.401051002\n",
      "15854293.0\n"
     ]
    }
   ],
   "source": [
    "#import publisher from google sheet \n",
    "gc = pygsheets.authorize(service_account_file=filepath.service_account_location)\n",
    "rxmgref = gc.open_by_url('https://docs.google.com/spreadsheets/d/1Tzda6Djr3zQmOhWu7Ief3GVR9Cjaml8238CeX7chj_U/edit#gid=1620368362') \n",
    "publisher_raw  = rxmgref.worksheet('title','Publisher Configurations').get_as_df()\n",
    "publisher_raw = publisher_raw.drop_duplicates(subset='PUBID', keep='last')\n",
    "publisher_raw.to_csv(localfolder+'smartsheet/Publisher.csv')\n",
    "publisher = publisher_raw[['DP.DS or DP.sV','PUBID']].drop_duplicates()\n",
    "\n",
    "# import SMS OMS\n",
    "offer_sheet = infrastructure.get_smartsheet('offers_sms')\n",
    "\n",
    "## import La Nina. \n",
    "gc = pygsheets.authorize(service_account_file=filepath.service_account_location)\n",
    "lanina = gc.open_by_url('https://docs.google.com/spreadsheets/d/1obszkCQoE0ELOR1O0CrLVETUEmEIWlGuyAmK3FgWSJg/edit#gid=1099746391') \n",
    "lanina_sheet  = lanina.worksheet('title','La Nina (Current)').get_as_df()\n",
    "lanina_sheet.to_csv(localfolder+'smartsheet/La_Nina.csv')\n",
    "\n",
    "## EMIT \n",
    "emit = infrastructure.get_smartsheet('emit')\n",
    "email_pubid = emit['Revenue Pub ID'].astype(str).str.split('.',expand = True)[0].unique().tolist()\n",
    "\n",
    "jobs = pd.read_csv(localfolder + 'SMS_SC_SS_Jobs.csv', dtype={'job_id' :'Int64'})\n",
    "jobs['start_tstamp'] = pd.to_datetime(jobs['start_tstamp'])\n",
    "jobs = jobs[jobs['start_tstamp'] > two_months_ago]\n",
    "\n",
    "creative_stats = pd.read_csv(localfolder + 'SMS_SC_SS_CreativesStats.csv', dtype={'CreativeId' :'str'})\n",
    "creative_stats['Tstamp'] = pd.to_datetime(creative_stats['Tstamp'])\n",
    "creative_stats['Tstamp'] = creative_stats['Tstamp'].dt.tz_localize(None)\n",
    "creative_stats = creative_stats[creative_stats['Tstamp'] > two_months_ago]\n",
    "\n",
    "raw_creative_stats = creative_stats\n",
    "offers = pd.read_csv(localfolder + 'SMS_SC_SS_Offers.csv', usecols = ['name','hitpath_offer_id', 'type','status','redirect_type','conversion_event','conversion_payout','currency'] )\n",
    "#historic_data = pd.read_csv(localfolder + 'SS_data.csv')#  data before Nov1\n",
    "\n",
    "flows = pd.read_csv(localfolder + 'SMS_SC_SS_Flows.csv')\n",
    "flows['Period'] = pd.to_datetime(flows['Period'].str[:10],format='mixed')\n",
    "flows = flows[flows['Period'] > two_months_ago]\n",
    "\n",
    "# add some information in flows\n",
    "flows.loc[flows['Name'] =='SM.SRV.DSS-PR-SFW', 'Name'] = 'SM.SRV-DSS-PR-SFW'\n",
    "flows['Revenue Source'] = 'Short Code - SS Flow'\n",
    "flows['Code_Type'] = 'Short Code' \n",
    "flows['Shortcode Name'] = flows['Name'].str.split('-',expand = True)[1]\n",
    "flows['Shortcode1'] = flows['Shortcode'].str.extract('(\\d{5})')\n",
    "flows['Offer'] = flows['OfferName'].str.extract(r'^(.*?)(?:\\s*\\(|\\s*\\[|$)')\n",
    "flows = flows.drop('Shortcode',axis = 1)\n",
    "flows['Dataset'] = flows['Name'].str.split('-',expand = True)[0]\n",
    "flows['Dataset'] = flows['Dataset'].str.replace('JM.ONP','JET.ONP')\n",
    "flows['Dataset'] = flows['Dataset'].str.replace('JM.NTC','JET.NTC')\n",
    "flows.loc[flows['Name'].str.contains('DUP', na = False), 'Dataset'] =  flows['Name'].str.split('-',expand = True)[1]\n",
    "#flows.loc[flows['Name'].str.contains('DUP', na = False), 'Shortcode Name'] =  flows['Name'].str.split('-',expand = True)[1].str.replace(\"I.\",\"\")\n",
    "flows.loc[flows['Name'].str.contains('I.A4F', na = False), 'Dataset'] = 'I.A4F'\n",
    "flows.loc[flows['Name'].str.contains('I.MFA', na = False), 'Dataset'] = 'I.MFA'\n",
    "##added\n",
    "flows.loc[flows['Name'].str.contains('I.N3G', na = False), 'Dataset'] = 'I.N3G'\n",
    "\n",
    "flows.loc[flows['CountUnsub']==0,'CountUnsub'] = flows['CountAutoresponderStop']\n",
    "#flows.loc[flows['CountDeliver']==0, 'CountDeliver'] = flows['CountSent']\n",
    "#Add send stategy\n",
    "flows['Send Strategy'] = np.where(\n",
    "    flows['OfferName'].str.contains('PR|HE|SE'),'P','AR')\n",
    "\n",
    "flows = flows.rename(columns=({'CountSent':'Delivered', 'CostDeliver':'Cost','CountClick':'Clicks','Shortcode1':'Shortcode', 'CountUnsub':'Optout'}))\n",
    "flows['Date'] = flows['Period']\n",
    "#flows['Date'] = pd.to_datetime(flows['Period'].str[:10],format='mixed')\n",
    "flows['AR Flow ID']=flows['Id'].astype('str')\n",
    "flows['AR Day'] = flows['OfferName'].str.extract('(\\d+)\\s*\\(')\n",
    "flows.loc[flows['AR Day'].isna(),'AR Day'] =  flows['OfferName'].str.extract('AR(\\d)')[0]\n",
    "flows.loc[flows['AR Day'].isna(),'AR Day'] = 'Null'\n",
    "flows['Hitpath ID'] = flows['OfferName'].str.extract(r'(\\d{4,5})')\n",
    "flows.loc[flows['OfferName'].str.contains( 'OW',na = False), 'Hitpath ID'] =flows['OfferName'].str.split(\"_|[| ]\",expand= True)[3]\n",
    "flows['AR Flow'] = flows['AR Flow ID'] + '_Day_' + flows['Shortcode Name']+'_'+  flows['AR Day']\n",
    "flows_clean = flows[['Hitpath ID','Date','Dataset','Offer','Shortcode','Shortcode Name','Revenue Source','Code_Type','Delivered','Optout','Cost','Clicks','AR Flow','AR Day','AR Flow ID','Send Strategy']]\n",
    "flows_clean = flows_clean.merge(publisher[['DP.DS or DP.sV','PUBID']], left_on ='Dataset', right_on = 'DP.DS or DP.sV', how = 'left' )\n",
    "flows_clean = flows_clean.rename(columns=({'PUBID':'Affiliate_id'}))\n",
    "flows_clean.loc[flows_clean['Shortcode Name'] == 'FLC', 'Shortcode'] = '51797'\n",
    "flows_clean.loc[flows_clean['Shortcode Name'] == 'CSS', 'Shortcode'] = '70610'\n",
    "flows_clean.loc[flows_clean['Shortcode Name'] == 'HZB', 'Shortcode'] = '44345'\n",
    "flows_clean.loc[flows_clean['Shortcode Name'] == 'MBC', 'Shortcode'] = '80837'\n",
    "flows_clean.loc[flows_clean['Shortcode Name'] == 'DSS', 'Shortcode'] = '31232'\n",
    "flows_clean.loc[flows_clean['Shortcode Name'] == 'SVT', 'Shortcode'] = '61659'\n",
    "flows_clean.loc[flows_clean['Shortcode Name'] == 'UAA', 'Shortcode'] = '79743'\n",
    "flows_clean.loc[flows_clean['Shortcode Name'] == 'PRC', 'Shortcode'] = '18333164159'\n",
    "flows_clean.loc[flows_clean['Shortcode Name'] == 'UAATF', 'Shortcode'] = '18333641722'\n",
    "flows_clean.loc[flows_clean['Shortcode Name'].str.contains('A4F'), 'Shortcode'] = '13022952715'\n",
    "flows_clean.loc[flows_clean['Shortcode Name'].str.contains('MFA'), 'Shortcode'] = '18332686782'\n",
    "flows_clean.loc[flows_clean['Shortcode Name'].str.contains('N3G'), 'Shortcode'] = '18332685688'\n",
    "flows_clean.loc[flows_clean['Shortcode Name'].str.contains('FHB'), 'Shortcode'] = '18338987652'\n",
    "flows_clean.loc[flows_clean['Shortcode Name'].str.contains('THM'), 'Shortcode'] = '18336145644'\n",
    "flows_clean.loc[flows_clean['Shortcode Name'].str.contains('FRH'), 'Shortcode'] = '18333275117'\n",
    "flows_clean.loc[flows_clean['Shortcode'].str.contains( '51797',na = False), 'Shortcode Name'] = 'FLC'\n",
    "flows_clean.loc[flows_clean['Shortcode'].str.contains( '70610',na = False), 'Shortcode Name'] = 'CSS'\n",
    "flows_clean.loc[flows_clean['Shortcode'].str.contains( '44345',na = False), 'Shortcode Name'] = 'HZB'\n",
    "flows_clean.loc[flows_clean['Shortcode'].str.contains( '80837',na = False), 'Shortcode Name'] = 'MBC'\n",
    "flows_clean.loc[flows_clean['Shortcode'].str.contains( '31232',na = False), 'Shortcode Name'] = 'DSS'\n",
    "flows_clean.loc[flows_clean['Shortcode'].str.contains( '61659',na = False), 'Shortcode Name'] = 'SVT'\n",
    "flows_clean.loc[flows_clean['Shortcode'].str.contains( '79743',na = False), 'Shortcode Name'] = 'UAA'\n",
    "flows_clean.loc[flows_clean['Shortcode'].str.contains( '18333164159',na = False), 'Shortcode Name'] = 'PRC'\n",
    "flows_clean.loc[flows_clean['Shortcode'].str.contains( '18333641722',na = False), 'Shortcode Name'] = 'UAATF'\n",
    "flows_clean.loc[flows_clean['Shortcode'].str.contains( '13022952715',na = False), 'Shortcode Name'] = 'A4FLC'\n",
    "flows_clean.loc[flows_clean['Shortcode'].str.contains( '15612023538',na = False), 'Shortcode Name'] = 'A4FLC'\n",
    "flows_clean.loc[flows_clean['Shortcode'].str.contains( '18332686782',na = False), 'Shortcode Name'] = 'MFATF'\n",
    "flows_clean.loc[flows_clean['Shortcode'].str.contains( '18332685688',na = False), 'Shortcode Name'] = 'N3GTF'\n",
    "flows_clean.loc[flows_clean['Shortcode'].str.contains( '18338987652',na = False), 'Shortcode Name'] = 'FBHTF'\n",
    "flows_clean.loc[flows_clean['Shortcode'].str.contains( '18336145644',na = False), 'Shortcode Name'] = 'THMTF'\n",
    "flows_clean.loc[flows_clean['Shortcode'].str.contains( '18333275117',na = False), 'Shortcode Name'] = 'FRHTF'\n",
    "# change some segment name \n",
    "creative_stats['Segment'] = creative_stats['Segment'].str.replace('WW.YFA','WWM.YFA')\n",
    "creative_stats['Segment'] = creative_stats['Segment'].str.replace('ARM.CR','AI.CC')\n",
    "creative_stats['Segment'] = creative_stats['Segment'].str.replace('RH.3CS','RHD.CC')\n",
    "creative_stats['Segment'] = creative_stats['Segment'].str.replace('ED.247L','EDM.247L')\n",
    "creative_stats['Segment'] = creative_stats['Segment'].str.replace('PA.SWP','PA.PS')\n",
    "creative_stats['Segment'] = creative_stats['Segment'].str.replace('FM.YS','FSM.YS')\n",
    "creative_stats['Segment'] = creative_stats['Segment'].str.replace('CM_','CM.OSR_')\n",
    "creative_stats['Segment'] = creative_stats['Segment'].str.replace('CM.OSR.OSR','CM.OSR')\n",
    "creative_stats['Segment'] = creative_stats['Segment'].str.replace('CM.OSR.OSR.OSR','CM.OSR')\n",
    "creative_stats = creative_stats.rename(columns=({'DeliverCount':'Delivered', 'TotalCost':'Cost','ClickCount':'Clicks','UnsubCount':'Optout'}))\n",
    "creative_id_na = creative_stats.loc[creative_stats['CreativeId'].isna()]\n",
    "jobs_optout = jobs[['id','optout']]\n",
    "creative_stats =  creative_stats.merge(jobs_optout, left_on = 'JobId' ,right_on='id', how='left')\n",
    "jobid_count = creative_stats['JobId'].value_counts().to_dict()\n",
    "creative_stats['Optout'] = creative_stats.apply(lambda row: row['optout'] / jobid_count[row['JobId']], axis=1)\n",
    "creative_stats = creative_stats.groupby(['JobId', 'Tstamp', 'Offer', 'Segment', 'CreativeId', 'CreativeName','Creative']).sum().reset_index()\n",
    "print(creative_stats['Delivered'].sum())\n",
    "jobs['segments'] = jobs['segments'].str.replace('WW.YFA','WWM.YFA')\n",
    "jobs['segments'] = jobs['segments'].str.replace('ARM.CR','AI.CC')\n",
    "jobs['segments'] = jobs['segments'].str.replace('RH.3CS','RHD.CC')\n",
    "jobs['segments'] = jobs['segments'].str.replace('ED.247L','EDM.247L')\n",
    "jobs['segments'] = jobs['segments'].str.replace('PA.SWP','PA.PS')\n",
    "jobs['segments'] = jobs['segments'].str.replace('FM.YS','FSM.YS')\n",
    "jobs['segments'] = jobs['segments'].str.replace('CM_','CM.OSR_')\n",
    "jobs['segments'] = jobs['segments'].str.replace('CM.OSR.OSR','CM.OSR')\n",
    "jobs['segments'] = jobs['segments'].str.replace('CM.OSR.OSR.OSR','CM.OSR')\n",
    "\n",
    "\n",
    "#clean jobs file\n",
    "#drop columns with nulls\n",
    "jobs.isna().sum()\n",
    "drop_columns_jobs = ['remote_status']\n",
    "jobs.drop(columns=drop_columns_jobs, inplace=True, axis=1)\n",
    "jobs = jobs.rename(columns={ 'id':'job_id', 'name':'job_name'})\n",
    "#convert date fields to correct format\n",
    "jobs['start_tstamp'] = pd.to_datetime(jobs['start_tstamp'])\n",
    "jobs['end_tstamp'] = pd.to_datetime(jobs['end_tstamp'])\n",
    "jobs['scheduled_tstamp'] = pd.to_datetime(jobs['scheduled_tstamp'],format = 'mixed')\n",
    "jobs['Scheduling Time'] = jobs['scheduled_tstamp'].dt.tz_convert('US/Pacific').dt.strftime('%Y-%m-%d %H:%M')\n",
    "creative_stats =  creative_stats.merge(jobs[['job_id','Scheduling Time']], left_on = 'JobId' ,right_on='job_id', how='left')\n",
    "creative_id_na =  creative_id_na.merge(jobs[['job_id','Scheduling Time']], left_on = 'JobId' ,right_on='job_id', how='left')\n",
    "\n",
    "#clean creative_stats\n",
    "creative_stats.isna().sum()\n",
    "creative_stats['Tstamp'] = pd.to_datetime(creative_stats['Tstamp'])\n",
    "\n",
    "#clean offers\n",
    "offers.isna().sum()\n",
    "offers = offers.rename(columns={ 'id':'offer_id', 'name':'offer'})\n",
    "\n",
    "#####engagement stats for all sends#####\n",
    "#merge above df with reveneu file to get all of reveneu information\n",
    "## combine all months revenue CSVs into one master-revenue file.\n",
    "os.chdir(localfolder + \"SMS Rev\")\n",
    "\n",
    "all_files = [i for i in glob.glob('*.{}'.format('csv'))]\n",
    "#combine all files in the list\n",
    "combined_csv = pd.concat([pd.read_csv(f, dtype={'advertiser_name':'str','campaign_name':'str'}, low_memory=False) for f in all_files])\n",
    "#combined_csv = pd.concat([pd.read_csv(f) for f in all_files ])\n",
    "####Exporting master revenue file####\n",
    "combined_csv.to_csv( localfolder + \"SMS_master_revenue.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "combined_csv = pd.read_csv(localfolder + 'SMS_master_revenue.csv', dtype={'advertiser_name':'str','campaign_name':'str'})\n",
    "daily_revenue = combined_csv\n",
    "daily_revenue['date'] = pd.to_datetime(daily_revenue['date'],format = 'mixed').dt.date\n",
    "daily_revenue_gpby = daily_revenue.groupby(['affiliate_id','date'])['amount'].sum().reset_index()\n",
    "jump_page_csv = pd.read_csv( localfolder+\"jumppage_click.csv\")\n",
    "jumppage_clicks = jump_page_csv.rename(columns = {'subid_1':\"subid_2\"})\n",
    "print(combined_csv['amount'].sum())\n",
    "combined_csv = pd.concat([combined_csv,jumppage_clicks])\n",
    "print(combined_csv['amount'].sum())\n",
    "print(combined_csv['Jump Page Clicks'].sum())\n",
    "\n",
    "#check subids\n",
    "subs = [\"subid_1\",\"subid_2\", \"subid_5\"]\n",
    "def sid(d):\n",
    "    sid = \"subid_3\"\n",
    "    for j in subs:\n",
    "        if (d[f\"{j} uc\"]>=5) & (d[f\"{j} ucs\"]==0):\n",
    "            sid = j\n",
    "    return d[sid]\n",
    "for i in subs:\n",
    "    combined_csv[i] = combined_csv[i].astype(str)\n",
    "    combined_csv[f\"{i} uc\"] = combined_csv[i].str.count(\"_\")\n",
    "    combined_csv[f\"{i} ucs\"] = combined_csv[i].str.count(\":\")\n",
    "#merge all subids from 1,2 and 5 in subid\n",
    "combined_csv[\"sub_id\"] = combined_csv.apply(sid,axis=1)\n",
    "combined_csv['sub_id'] = combined_csv['sub_id'].str.replace(\n",
    "    'SS_SC_HZB_44345_461871_0_13074_868123_',\n",
    "    'SS_TF_MFA_18332686782_461871_AR0_13074_868123_',\n",
    "    regex=False\n",
    ")\n",
    "########## SS BEGIN #########\n",
    "combined_csv['first_split']=combined_csv['sub_id'].str.split('_').str[0]\n",
    "\"\"\" \n",
    "jumppage_clicks[\"sub_id\"] = jumppage_clicks[\"subid_1\"]\n",
    "subs = [\"sub_id\"]\n",
    "# doing same cleaning with jumpapge files  \n",
    "for i in subs:\n",
    "    jumppage_clicks[i] = jumppage_clicks[i].astype(str)\n",
    "    jumppage_clicks[f\"{i} uc\"] = jumppage_clicks[i].str.count(\"_\")\n",
    "#merge all subids from 1,2 and 5 in subid\n",
    "\n",
    "jumppage_clicks['first_split']=jumppage_clicks['sub_id'].str.split('_').str[0]\n",
    "\"\"\"\n",
    "# get the date based on subid date \n",
    "combined_csv = combined_csv.reset_index(drop=True)\n",
    "combined_csv['Dash_Date_from_subid'] = combined_csv['sub_id'].str.extract(r'(\\d{1,2}[A-Za-z]{3}\\d{2})')\n",
    "combined_csv['Dash_Date_from_subid'] = pd.to_datetime(combined_csv['Dash_Date_from_subid'], format='%d%b%y',errors='coerce')\n",
    "combined_csv['date'] = pd.to_datetime(combined_csv['date'],errors='coerce')\n",
    "combined_csv.loc[combined_csv['Dash_Date_from_subid'].isna(),'Dash_Date_from_subid'] = combined_csv['date']\n",
    "combined_csv['date'] = combined_csv['Dash_Date_from_subid']\n",
    "combined_csv = combined_csv[combined_csv['date'] > two_months_ago]\n",
    "combined_csv.loc[combined_csv['campaign_id']==8202]\n",
    "combined_csv.loc[(combined_csv['affiliate_id'] == 460001) & (combined_csv['site_id'].isna()== False),'affiliate_id'] = combined_csv['site_id']\n",
    "combined_csv['affiliate_id'] = combined_csv['affiliate_id'].astype('Int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67030e9f",
   "metadata": {},
   "source": [
    "## Long Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4569f9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Long Code data from MD\n",
    "\n",
    "# lc = gc.open_by_url('https://docs.google.com/spreadsheets/d/1De9UzMw5POUuS90s7dMKsYNTQVnxTj1dvAvwMhOCC5U/edit#gid=0') \n",
    "# lc_wk_df  = lc.worksheet('title','Long Code').get_as_df()\n",
    "# lc_wk_df['SMS Cost'] = lc_wk_df['SMS Cost'].str.split('$',expand = True)[1].astype('float')\n",
    "# # import MD_\n",
    "# md_optout = pd.read_csv(localfolder + 'MD_optout.csv', index_col= False)\n",
    "# md_optout[['campaign','Date']]=md_optout['campaign'].str.split('\\nStarted At: ',expand = True)\n",
    "# md_optout['Date'] = pd.to_datetime(md_optout['Date'], format='%Y-%m-%d %H:%M:%S',errors='coerce')\n",
    "# md_optout['campaign'] = md_optout['campaign'].str.extract('([a-zA-Z].*)', expand=True)\n",
    "\n",
    "# lc_wk_df['Date'] = pd.to_datetime(lc_wk_df['Date'], errors='coerce')\n",
    "# lc_wk_df = lc_wk_df[lc_wk_df['Date'] > two_months_ago]\n",
    "# lc_wk_df = lc_wk_df.merge(md_optout[['Date','campaign','optout']], left_on = ['Offer','Date'], right_on = ['campaign','Date'], how = 'left')\n",
    "# # rename to Optout\n",
    "# lc_wk_df = lc_wk_df.rename(columns = {'Offer':'Sub_Id','optout':'Optout'})\n",
    "# lc_rev = combined_csv.loc[(combined_csv['first_split']=='MD') | (combined_csv['first_split']=='TB') ]\n",
    "# #lc_rev = combined_csv.loc[(combined_csv['first_split']=='MD') ]\n",
    "# lc_rev.loc[lc_rev['Dash_Date_from_subid'].isna(), 'Dash_Date_from_subid'] = lc_rev['date']\n",
    "# lc_rev =lc_rev.rename(columns = {\"Dash_Date_from_subid\":'Date','sub_id':'Sub_Id'})\n",
    "# #  define the pattern\n",
    "# #pattern = r'(.+\\d{1,2}[A-Za-z]{3}\\d{2})'\n",
    "# # extract the pattern\n",
    "# #lc_rev['Sub_Id'] = lc_rev['sub_id'].str.extract(pattern)\n",
    "# lc_rev_summary = lc_rev.groupby(['Sub_Id','Date'])['amount'].sum().reset_index()\n",
    "# lc_df = pd.concat([lc_wk_df,lc_rev_summary],axis = 0, ignore_index=True)\n",
    "# # remove Date from lc_df\n",
    "# lc_df['Date']= pd.to_datetime(lc_df['Date'],errors='coerce')\n",
    "# lc_df = lc_df.groupby(['Sub_Id','Date']).sum().reset_index()  \n",
    "# lc_df = lc_df.fillna(0)\n",
    "                           \n",
    "                           \n",
    "# lc_df.loc[lc_df['Sub_Id'] == '01JUN23_8838_W4_EDU_SVT','Sub_Id'] = 'MD_LC_OMG_SVT_460918__8838_OT_01JUN23'\n",
    "# lc_df.loc[lc_df['Sub_Id'] == '05Jun23_MD_LC_OMG_SVT_460918_W1_11714_T1','Sub_Id'] = 'MD_LC_OMG_SVT_460918__11714_OT_05Jun23'\n",
    "# lc_df.loc[lc_df['Sub_Id'] == '11714-2_OMG_SVT','Sub_Id'] = 'MD_LC_OMG_SVT_460918__11714_OT_'\n",
    "# lc_df.loc[lc_df['Sub_Id'] == 'MD_LC_OMG_SVT_460919_00066_9088__29Jul23','Sub_Id'] = 'MD_LC_OMG_SVT_460919_00066_9088_P_29Jul23'\n",
    "# lc_df.loc[lc_df['Sub_Id'] == 'MD_LC_OMG_SVT_460920_00068_12076__30Jul23','Sub_Id']  = 'MD_LC_OMG_SVT_460920_00068_12076_P_30Jul23'\n",
    "# lc_df['split_column'] = lc_df['Sub_Id'].str.split('_')\n",
    "# lc_df['subid_uc'] = lc_df['Sub_Id'].str.count('_')\n",
    "# #lc_df['Date'] = lc_df.loc[lc_df['Date'].isna(), 'Date'] = pd.to_datetime(lc_df['split_column'].str[8], format='%d%b%y',errors='coerce')\n",
    "# lc_df['Send Strategy'] = lc_df['split_column'].str[7]\n",
    "# lc_df['Hitpath_Offer_ID'] = lc_df['split_column'].str[6]\n",
    "# # detect a 6digit number that start with \"46\" from Sub_Id\n",
    "# lc_df['Affiliate_Id'] = lc_df['Sub_Id'].str.extract(r'(46\\d{4})')\n",
    "# #lc_df['Affiliate_Id'] = lc_df['split_column'].str[4]\n",
    "# lc_df['Long Code Content ID'] = lc_df['split_column'].str[5]\n",
    "# lc_df['Shortcode Name'] = ''\n",
    "# lc_df.loc[lc_df['split_column'].str[3] == 'SVT','Shortcode Name' ] = 'SVTLC'\n",
    "# lc_df.loc[lc_df['split_column'].str[3] == 'UAA','Shortcode Name' ] = 'UAALC'\n",
    "# lc_df['Send Strategy'] = lc_df['Send Strategy'].str.replace('T0','OT')\n",
    "# lc_df['Send Strategy'] = lc_df['Send Strategy'].str.replace('T1','OT') \n",
    "# lc_df.loc[lc_df['split_column'].str[0] == 'TB', 'Revenue Source'] = 'Long Code - Textback'\n",
    "# lc_df.loc[lc_df['split_column'].str[0] == 'MD', 'Revenue Source'] = 'Long Code - Mobile Drips'\n",
    "# lc_df.loc[lc_df['split_column'].str[0] == 'TB', 'Send Strategy'] = 'P'\n",
    "# lc_df.loc[lc_df['split_column'].str[0] == 'TB', 'Long Code Content ID'] = np.nan\n",
    "\n",
    "# # currently we don't consider split test content option \n",
    "# lc_df['amount'] = lc_df['amount'].fillna(0)\n",
    "# print(lc_df.loc[lc_df['split_column'].str[0] == 'MD',]['amount'].sum())\n",
    "# lc_lanina = lanina_sheet[(lanina_sheet['Long Code Content ID'].isna() == False) & ((lanina_sheet['Long Code Content ID'] !=\"\"))]\n",
    "# lc_lanina['Long Code Content ID'] = lc_lanina['Long Code Content ID'].astype(str).str.zfill(5)\n",
    "# lc_df_full = lc_df.merge(lc_lanina[['Long Code Content ID','Reporting Content ID','Content']],copy = False, how= 'left', on = 'Long Code Content ID')\n",
    "# print(lc_df_full.loc[lc_df_full['split_column'].str[0] == 'MD',]['amount'].sum())\n",
    "# lc_df_full['Code_Type'] = 'Long Code'\n",
    "# lc_df_full_11 = lc_df_full[lc_df_full['Date'] >= '2022-11-01']\n",
    "\n",
    "\n",
    "# lc_df_full = lc_df_full.rename(columns = {'Qty':'Sent','Daily Success Qty':'Delivered','Fail Qty': 'Undelivered',\\\n",
    "#                             'Clicks Qty':'Clicks','SMS Cost':'Cost',\\\n",
    "#                             'amount':'Revenue','Long Code Content ID':'Creative_Id', 'Reporting Content ID':'Creativename','Content':'Creative'})\n",
    "\n",
    "# lc_df_full = lc_df_full[['Date','Affiliate_Id', 'Hitpath_Offer_ID','Sent','Delivered','Undelivered','Clicks','Optout',\\\n",
    "#        'Cost','Revenue', 'Send Strategy', 'Shortcode Name', 'Revenue Source', 'Code_Type','Sub_Id','Creative_Id','Creativename','Creative']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236ba57b",
   "metadata": {},
   "source": [
    "## Short Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afb14764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_csv['date'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a166fb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_4212\\1824205372.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  combined_csv_ss_only.loc[combined_csv_ss_only['offer_id'].str.contains(\"^(\\d{4,5}).*|OW\", regex = True, na = False) == False , 'offer_id'] =  combined_csv_ss_only['split_column'].str[5]\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_4212\\1824205372.py:21: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  combined_csv_ss_only.loc[combined_csv_ss_only['offer_id'].str.contains(\"^(\\d{4,5}).*|OW\", regex = True, na = False) == False, 'offer_id'] =  combined_csv_ss_only['split_column'].str[4]\n"
     ]
    }
   ],
   "source": [
    "combined_csv['date'] = pd.to_datetime(combined_csv['date'])\n",
    "combined_csv_ss_only = combined_csv[(combined_csv['first_split']=='SS')  |  (combined_csv['first_split']=='SMS')] \n",
    "combined_csv_ss_only = combined_csv_ss_only[combined_csv_ss_only['date']>='2022-11-01'].reset_index(drop=True)\n",
    "\n",
    "combined_csv_ss_only['subid_uc'] = combined_csv_ss_only.sub_id.str.count('_')\n",
    "#combined_csv_ss_only['subid_uc']=combined_csv.sub_id.str.count('_')\n",
    "\n",
    "#ignoring the subids that  are not formatted correctly ex: SS_HZB_{{datasource_id}}_{{job_id}}_ALL.SMS_10434_SM_44345_{{today_d}}{{today_mon}}{{today_yy}}_1_{{member_id}}\n",
    "#they have very less revenue under them\n",
    "#these are with sub id uc <12\n",
    "combined_csv_ss_only = combined_csv_ss_only[combined_csv_ss_only['subid_uc'] < 12]\n",
    "\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['advertiser_name'] == 'NIC','campaign_id'] = 10267\n",
    "Offer_name = combined_csv_ss_only.groupby(['campaign_id','campaign_name']).count().reset_index()[['campaign_id','campaign_name']]\n",
    "\n",
    "# get jump page sum and revenue sum \n",
    "combined_csv_ss_only['split_column'] = combined_csv_ss_only['sub_id'].str.split('_')\n",
    "combined_csv_ss_only.loc[(combined_csv_ss_only['first_split'] == 'SS')  , 'offer_id'] = combined_csv_ss_only['split_column'].str[6]\n",
    "combined_csv_ss_only.loc[(combined_csv_ss_only['first_split'] == 'SMS')  , 'offer_id'] = combined_csv_ss_only['split_column'].str[7]\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['offer_id'].str.contains(\"^(\\d{4,5}).*|OW\", regex = True, na = False) == False , 'offer_id'] =  combined_csv_ss_only['split_column'].str[5]\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['offer_id'].str.contains(\"^(\\d{4,5}).*|OW\", regex = True, na = False) == False, 'offer_id'] =  combined_csv_ss_only['split_column'].str[4]\n",
    "\n",
    "combined_csv_ss_only['campaign_id']= combined_csv_ss_only['campaign_id'].astype('str').str.split('.',expand = True)[0]\n",
    "\n",
    "combined_csv_ss_only['Jump Page Version'] = '0'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['offer_id'].str.contains('v',na = False),'Jump Page Version'] = combined_csv_ss_only['offer_id'].str.split('v',expand = True)[1]\n",
    "\n",
    "\n",
    "#combined_csv_ss_only['c1'] = combined_csv_ss_only['sub_id'].str[:-9]\n",
    "\n",
    "# import offer wall database\n",
    "ofwall_df = infrastructure.get_smartsheet('ow_sms')\n",
    "ofwall_df = ofwall_df.rename(columns = {'Hitpath Offer ID':'Offer Wall ID'})\n",
    "offer_wall_id = ofwall_df['Offer Wall ID'].astype(str).unique().tolist()\n",
    "combined_csv_ss_only['Offer Type'] = 'Single Offer'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['offer_id'].isin(offer_wall_id), 'Offer Type'] = 'Offer Wall'\n",
    "offerwall = combined_csv_ss_only.loc[(combined_csv_ss_only['Offer Type']=='Offer Wall') |((combined_csv_ss_only['offer_id']!= combined_csv_ss_only['campaign_id'])& (combined_csv_ss_only['campaign_id'].isna() == False) &  (combined_csv_ss_only['campaign_id'] != 'nan') & (combined_csv_ss_only['offer_id'].str.contains(\"v\",na = True) == False))  ,]\n",
    "\n",
    "combined_csv_ss_only['Offer Wall Id'] = ''\n",
    "combined_csv_ss_only.loc[(combined_csv_ss_only['Offer Type']=='Offer Wall')  , 'Offer Wall Id'] = combined_csv_ss_only['offer_id']\n",
    "#combined_csv_ss_only['offer_id'] = combined_csv_ss_only['offer_id'].astype('str').str.extract(r'\\b(\\d{4,5}).*')\n",
    "#combined_csv_ss_only.loc[combined_csv_ss_only['Offer Type']=='Offer Wall' , 'campaign_id'] = combined_csv_ss_only['offer_id'].str.split('OW',expand = True)[0]\n",
    "combined_csv_ss_only.loc[(combined_csv_ss_only['Offer Type']=='Offer Wall')  , 'campaign_id'] = combined_csv_ss_only['offer_id']\n",
    "combined_csv_ss_only.loc[(combined_csv_ss_only['campaign_id'].isna()) | (combined_csv_ss_only['campaign_id'] == 'nan'),'campaign_id' ] = combined_csv_ss_only['offer_id']\n",
    "combined_csv_ss_only.loc[(combined_csv_ss_only['campaign_id'] != combined_csv_ss_only['offer_id'])  ,'campaign_id'] = combined_csv_ss_only['offer_id']\n",
    "# rename offer id to offer wall id\n",
    "\n",
    "\n",
    "combined_csv_ss_only = combined_csv_ss_only[['date', 'campaign_id', 'Offer Wall Id','Jump Page Version',\n",
    "       'affiliate_id',  'amount', 'Jump Page Clicks',\n",
    "        'sub_id', 'first_split','Dash_Date_from_subid', 'subid_uc','Offer Type']]\n",
    "\n",
    "#combined_csv_ss_only['amount'] = combined_csv_ss_only['amount'].fillna(0)\n",
    "combined_csv_ss_only[ 'Jump Page Clicks'] = combined_csv_ss_only[ 'Jump Page Clicks'].fillna(0)\n",
    "#combined_csv_ss_only = combined_csv_ss_only.groupby(['date', 'campaign_id', 'affiliate_id', 'Jump Page Version','Offer Wall Id', 'sub_id', 'first_split','Dash_Date_from_subid', 'subid_uc','Offer Type'])[[ 'amount', 'Jump Page Clicks']].sum().reset_index()\n",
    "\n",
    "combined_csv_ss_only = combined_csv_ss_only.groupby([\n",
    "    'date', 'campaign_id', 'affiliate_id', 'Jump Page Version',\n",
    "    'Offer Wall Id', 'sub_id', 'first_split', 'Dash_Date_from_subid',\n",
    "    'subid_uc', 'Offer Type'\n",
    "]).agg({\n",
    "    'amount': ['sum', 'count'],  # Summing and counting for the 'amount' column\n",
    "    'Jump Page Clicks': 'sum'     # Summing for the 'Jump Page Clicks' column\n",
    "}).reset_index()\n",
    " \n",
    "combined_csv_ss_only.columns = [\n",
    "    'date', 'campaign_id', 'affiliate_id', 'Jump Page Version',\n",
    "    'Offer Wall Id', 'sub_id', 'first_split', 'Dash_Date_from_subid',\n",
    "    'subid_uc', 'Offer Type', 'amount','conversions','Jump Page Clicks'\n",
    "]\n",
    "\n",
    "# use for verification \n",
    "jumppageclicks1 = combined_csv_ss_only['Jump Page Clicks'].sum()\n",
    "revenue1 = combined_csv_ss_only['amount'].sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "418b1c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_4212\\3624828272.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  combined_csv_ss_only_reformatjobs['Job_id'] = combined_csv_ss_only_reformatjobs['sub_id'].str.extract(f'({\"|\".join(job_ids)})')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1689142.0\n"
     ]
    }
   ],
   "source": [
    "#*** Extracting creative information. ****#\n",
    "# find creative id from list of creative ids from creativestats file.\n",
    "combined_csv_ss_only['split_column'] = combined_csv_ss_only['sub_id'].str.split('_')\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only[combined_csv_ss_only['first_split'] == 'SS'].index , 'creative_id'] = combined_csv_ss_only['split_column'].str[7]\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only[combined_csv_ss_only['first_split'] == 'SMS'].index , 'creative_id'] = combined_csv_ss_only['split_column'].str[8]\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only[combined_csv_ss_only['first_split'] == 'SS'].index , 'Job_id'] = combined_csv_ss_only['split_column'].str[5]\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only[combined_csv_ss_only['first_split'] == 'SMS'].index , 'Job_id'] = combined_csv_ss_only['split_column'].str[6]\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only[combined_csv_ss_only['first_split'] == 'SS'].index , 'Code_Type'] = combined_csv_ss_only['split_column'].str[1]\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only[combined_csv_ss_only['first_split'] == 'SMS'].index , 'Code_Type'] = combined_csv_ss_only['split_column'].str[2] \n",
    "\n",
    "# \n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['Code_Type'] == 'TF', 'Code_Type' ] ='Toll Free' \n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['Code_Type'] == 'SC', 'Code_Type' ] ='Short Code' \n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['Code_Type'] == 'FLC', 'Code_Type' ] ='Short Code'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['Code_Type'] == 'LC', 'Code_Type' ] ='Long Code'\n",
    "# some AR subids in december formatted incorrectly, some residuals formatted incorrectly.\n",
    "#creative ids are length>6, creative ids len =1 are ARs, ignoring both the cases and repalcing creative ids with nans for rest of the length values\n",
    "combined_csv_ss_only['creative_idlen'] = combined_csv_ss_only['creative_id'].str.len()\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only[~combined_csv_ss_only['creative_idlen'].isin([1,6])].index,'creative_id']=np.nan \n",
    "\n",
    "# identify shortcode \n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['sub_id'].str.contains('FLC', na=False), 'Shortcode Name'] = 'FLC'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['sub_id'].str.contains('CSS', na=False), 'Shortcode Name'] = 'CSS'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['sub_id'].str.contains('HZB', na=False), 'Shortcode Name'] = 'HZB'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['sub_id'].str.contains('MBC', na=False), 'Shortcode Name'] = 'MBC'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['sub_id'].str.contains('DSS', na=False), 'Shortcode Name'] = 'DSS'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['sub_id'].str.contains('SVT', na=False), 'Shortcode Name'] = 'SVT'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['sub_id'].str.contains('UAA', na=False), 'Shortcode Name'] = 'UAA'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['sub_id'].str.contains('PRC', na=False), 'Shortcode Name'] = 'PRC'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['sub_id'].str.contains('UAATF', na=False), 'Shortcode Name'] = 'UAATF'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['sub_id'].str.contains('MFA', na=False), 'Shortcode Name'] = 'MFATF'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['sub_id'].str.contains('A4F', na=False), 'Shortcode Name'] = 'A4FLC'\n",
    "#added\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['sub_id'].str.contains('N3G', na=False), 'Shortcode Name'] = 'N3GTF'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['sub_id'].str.contains('FBH', na=False), 'Shortcode Name'] = 'FBHTF'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['sub_id'].str.contains('THM', na=False), 'Shortcode Name'] = 'THMTF'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['sub_id'].str.contains('FRH', na=False), 'Shortcode Name'] = 'FRHTF'\n",
    "combined_csv_ss_only.loc[(combined_csv_ss_only['Shortcode Name'] == 'SVT') & (combined_csv_ss_only['Code_Type'] == 'LC' ) , 'Shortcode Name'] = 'SVTLC'\n",
    "combined_csv_ss_only.loc[(combined_csv_ss_only['Shortcode Name'] == 'UAA') & (combined_csv_ss_only['Code_Type'] == 'LC' ) , 'Shortcode Name'] = 'UAALC'\n",
    "\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['Shortcode Name'] == 'FLC', 'Shortcode'] = '51797'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['Shortcode Name'] == 'CSS', 'Shortcode'] = '70610'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['Shortcode Name'] == 'HZB', 'Shortcode'] = '44345'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['Shortcode Name'] == 'MBC', 'Shortcode'] = '80837'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['Shortcode Name'] == 'DSS', 'Shortcode'] = '31232'\n",
    "combined_csv_ss_only.loc[(combined_csv_ss_only['Shortcode Name'] == 'SVT') , 'Shortcode'] = '61659'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['Shortcode Name'] == 'UAA', 'Shortcode'] = '79743'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['Shortcode Name'] == 'PRC', 'Shortcode'] = '18333164159'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['Shortcode Name'] == 'UAATF', 'Shortcode'] = '18333641722'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['Shortcode Name'] == 'MFATF', 'Shortcode'] = '18332686782'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['Shortcode Name'] == 'A4FLC', 'Shortcode'] = '13022952715'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['Shortcode Name'] == 'THMTF', 'Shortcode'] = '18336145644'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['Shortcode Name'] == 'FRHTF', 'Shortcode'] = '18333275117'\n",
    "#added\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['Shortcode Name'] == 'N3GTF', 'Shortcode'] = '18332685688'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['Shortcode Name'] == 'FBHTF', 'Shortcode'] = '18338987652'\n",
    "\n",
    "#reformat jobs from residuals and AR\n",
    "combined_csv_ss_only['job_idlen'] = combined_csv_ss_only['Job_id'].str.len()\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only[~combined_csv_ss_only['job_idlen'].isin([1,6])].index,'Job_id']=np.nan \n",
    "combined_csv_ss_only_reformatjobs = combined_csv_ss_only[combined_csv_ss_only['Job_id'].isna() ]\n",
    "jobs['job_id'] =jobs['job_id'].astype('str')\n",
    "\"\"\"\n",
    "for i in jobs['job_id']:\n",
    "    check_len = combined_csv_ss_only_reformatjobs[combined_csv_ss_only_reformatjobs['sub_id'].str.contains(i)]\n",
    "    if len(check_len) > 0:\n",
    "        combined_csv_ss_only_reformatjobs.loc[check_len.index,'Job_id'] = i\n",
    "\"\"\"\n",
    "# Extract the job IDs to a set for faster membership testing\n",
    "creative_stats['JobId'] = creative_stats['JobId'].astype('str')\n",
    "job_ids = set(jobs['job_id']) | set(creative_stats['JobId']) - set(['0'])\n",
    "#creativeid = set(creative_stats['Creative_Id'])\n",
    "#combined_csv_ss_only['creative_id'] = combined_csv_ss_only_reformatjobs['sub_id'].str.extract(f'({\"|\".join(creativeid)})')\n",
    "\n",
    "# Use str.extract to extract the job ID from the sub_id column\n",
    "combined_csv_ss_only_reformatjobs['Job_id'] = combined_csv_ss_only_reformatjobs['sub_id'].str.extract(f'({\"|\".join(job_ids)})')\n",
    "\n",
    "combined_csv_ss_only =   pd.concat([combined_csv_ss_only_reformatjobs,(combined_csv_ss_only[~combined_csv_ss_only['Job_id'].isna()])] )\n",
    "\n",
    "combined_csv_ss_only = combined_csv_ss_only[combined_csv_ss_only['creative_idlen']!= 8]\n",
    "combined_csv_ss_only = combined_csv_ss_only[~combined_csv_ss_only['creative_idlen'].isna()]\n",
    "#get all creatives information.\n",
    "combined_csv_ss_only['Job_id'] = combined_csv_ss_only['Job_id'].astype('float')\n",
    "combined_csv_ss_only['creative_id'] = combined_csv_ss_only['creative_id'].replace('6Oct22', np.nan)\n",
    "combined_csv_ss_only['creative_id'] = combined_csv_ss_only['creative_id'].astype('float')\n",
    "\n",
    "creative_stats['CreativeId'] = creative_stats['CreativeId'].astype('float')\n",
    "# make sure we don't want to merge jobid = 0 \n",
    "#combined_csv_ss_only = combined_csv_ss_only.groupby(['Job_id', 'creative_id'])['amount'].sum().reset_index()\n",
    "#combined_csv_ss_creative = combined_csv_ss_only.merge(creative_stats, left_on = ['Job_id', 'creative_id'], right_on = ['JobId', 'CreativeId'], how = 'left')\n",
    "combined_csv_ss_only['date']  = pd.to_datetime(combined_csv_ss_only['date'] )\n",
    "combined_csv_ss_only['date'] = combined_csv_ss_only['date'].dt.date\n",
    "\n",
    "print(combined_csv_ss_only['Jump Page Clicks'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26b5860",
   "metadata": {},
   "source": [
    "### SS flow, SS jobs and SS data without jobid(such as optin) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d53402d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_4212\\2142074811.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  combined_csv_ss_ar_flow['flow_abbr'] = 'FLOW'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "713605.0\n",
      "715028.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_4212\\2142074811.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  flows_clean_no_delivered['Affiliate_Id']= flows_clean_no_delivered['Affiliate_Id1']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       job_id                                          job_name  \\\n",
      "35700  659072                   SS_MBC_I-RC-15DC_6322_P_27Apr24   \n",
      "35701  659073              SS_FLC_I-RC-30DC-VZN_12440_P_27Apr24   \n",
      "35702  659074                 SS_HZB_CM-OSR-30DC_6322_P_27Apr24   \n",
      "35703  659075             SS_MBC_CM-OSR-30DC-TMO_6322_P_27Apr24   \n",
      "35704  659076                SS_SVT_AMD-PL-30DC_11593_P_27Apr24   \n",
      "...       ...                                               ...   \n",
      "39230  678003             SS_DSS_GR-PL-30DC-TMO_12972_P_25Jun24   \n",
      "39231  678004                 SS_FLC_GR-PL-30DC_12610_P_25Jun24   \n",
      "39232  678005          SS_DSS_JET-ZTA-1PLD-21DC_11600_P_25Jun24   \n",
      "39233  678431      SS_MBC_PN-FC-30PDC-OI-JUN23_13349_MI_25Jun24   \n",
      "39234  678432  SS_FLC_PN-FC-30PDC-VZN-OI-JUN23_13113_MI_25Jun24   \n",
      "\n",
      "                    offer  shortcode        start_tstamp          end_tstamp  \\\n",
      "35700  6322 - BB VAST MBC      80837 2024-04-27 12:30:40 2024-04-27 12:37:04   \n",
      "35701   12440 - AO PJ FLC      51797 2024-04-27 15:01:21 2024-04-27 15:05:29   \n",
      "35702  6322 - BB VAST HZB      44345 2024-04-27 15:01:11 2024-04-27 15:04:20   \n",
      "35703  6322 - BB VAST MBC      80837 2024-04-27 12:31:00 2024-04-27 12:32:56   \n",
      "35704  11593 - PA W2D SVT      61659 2024-04-27 12:05:39 2024-04-27 12:09:21   \n",
      "...                   ...        ...                 ...                 ...   \n",
      "39230         12972 - DSS      31232 2024-06-25 13:00:54 2024-06-25 13:09:39   \n",
      "39231  12610 - AVL CL FLC      51797 2024-06-25 13:01:03 2024-06-25 13:08:42   \n",
      "39232   11600 - WI UG DSS      31232 2024-06-25 13:01:11 2024-06-25 13:15:29   \n",
      "39233        13349 -  MBC      80837 2024-06-25 16:22:02 2024-06-25 16:23:33   \n",
      "39234         13113 - FLC      51797 2024-06-25 16:25:43 2024-06-25 16:26:13   \n",
      "\n",
      "               scheduled_tstamp       status_text  \\\n",
      "35700 2024-04-27 16:30:09+00:00  Sending Complete   \n",
      "35701 2024-04-27 19:00:57+00:00  Sending Complete   \n",
      "35702 2024-04-27 19:00:55+00:00  Sending Complete   \n",
      "35703 2024-04-27 16:30:46+00:00  Sending Complete   \n",
      "35704 2024-04-27 16:05:32+00:00  Sending Complete   \n",
      "...                         ...               ...   \n",
      "39230 2024-06-25 17:00:39+00:00  Sending Complete   \n",
      "39231 2024-06-25 17:00:47+00:00  Sending Complete   \n",
      "39232 2024-06-25 17:00:49+00:00  Sending Complete   \n",
      "39233 2024-06-25 20:21:19+00:00  Sending Complete   \n",
      "39234 2024-06-25 20:25:18+00:00  Sending Complete   \n",
      "\n",
      "                            segments  total  ...  delivered  not_delivered  \\\n",
      "35700                  MBC_I.RC_15DC   8746  ...       8713             43   \n",
      "35701              FLC_I.RC_30DC_VZN   6120  ...       5951            326   \n",
      "35702                HZB_CM.OSR_30DC   4653  ...       4218            434   \n",
      "35703            MBC_CM.OSR_30DC_TMO   1371  ...       1354             21   \n",
      "35704                SVT_AMD.PL_30DC   1745  ...       1685             10   \n",
      "...                              ...    ...  ...        ...            ...   \n",
      "39230             DSS_GR.PL_30DC_TMO    829  ...        797             14   \n",
      "39231                 FLC_GR.PL_30DC   1144  ...       1133             30   \n",
      "39232          DSS_JET.ZTA_1PLD.21DC   1905  ...       1786             18   \n",
      "39233  MBC_PN.SWP_30PDC_TMO_OI_JUN23   4072  ...       4050             63   \n",
      "39234   FLC_PN.FC_30PDC_VZN_OI_JUN23   1173  ...       1163            128   \n",
      "\n",
      "       optout  clicks  conversions  revenue     cost  ecpm      roi  \\\n",
      "35700     124    1733            0      0.0  13.0860   0.0 -13.0860   \n",
      "35701      47     417            0      0.0   8.9265   0.0  -8.9265   \n",
      "35702      22     707            0      0.0  31.6050   0.0 -31.6050   \n",
      "35703       8     182            0      0.0   2.0325   0.0  -2.0325   \n",
      "35704       3      83            0      0.0   2.6160   0.0  -2.6160   \n",
      "...       ...     ...          ...      ...      ...   ...      ...   \n",
      "39230      14     141            0      0.0   1.2240   0.0  -1.2240   \n",
      "39231      28     163            0      0.0   1.6995   0.0  -1.6995   \n",
      "39232      11     500            0      0.0   2.8500   0.0  -2.8500   \n",
      "39233      12     105            0      0.0   6.0825   0.0  -6.0825   \n",
      "39234       5      13            0      0.0   1.7445   0.0  -1.7445   \n",
      "\n",
      "        Scheduling Time  \n",
      "35700  2024-04-27 09:30  \n",
      "35701  2024-04-27 12:00  \n",
      "35702  2024-04-27 12:00  \n",
      "35703  2024-04-27 09:30  \n",
      "35704  2024-04-27 09:05  \n",
      "...                 ...  \n",
      "39230  2024-06-25 10:00  \n",
      "39231  2024-06-25 10:00  \n",
      "39232  2024-06-25 10:00  \n",
      "39233  2024-06-25 13:21  \n",
      "39234  2024-06-25 13:25  \n",
      "\n",
      "[3533 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "combined_csv_ss_only['Hitpath ID'] = combined_csv_ss_only['campaign_id']\n",
    "#combined_csv_ss_only['Dash_Date_from_subid'] = pd.to_datetime(combined_csv_ss_only['Dash_Date_from_subid'], format=\"%d%b%y\")\n",
    "# flow data (51797 & 80837 & jobid = 0)\n",
    "#  by 2023/4/11, we don't include creative id. Subid has that info\n",
    "\n",
    "flows_clean1 = flows_clean.groupby(['Hitpath ID', 'Date','Offer', 'Dataset', 'Shortcode', 'Shortcode Name',\\\n",
    "       'Revenue Source', 'Code_Type', 'AR Flow', 'AR Day', 'AR Flow ID', 'DP.DS or DP.sV','Affiliate_id','Send Strategy']).sum().reset_index()\n",
    "flows_clean1.loc[flows_clean1['AR Day']=='Null', 'AR Day'] = 0 \n",
    "flows_clean1['AR Day'] = flows_clean1['AR Day'].astype(int)\n",
    "flows_clean1 = flows_clean1.groupby(['Hitpath ID', 'Date', 'Offer','Dataset', 'Shortcode', 'Shortcode Name',\n",
    "       'Revenue Source', 'Code_Type',  'DP.DS or DP.sV',\n",
    "       'Affiliate_id','Send Strategy']).sum().reset_index()\n",
    "flows_clean1['AR Day'] = flows_clean1['AR Day'].astype(str)\n",
    "\n",
    "combined_csv_ss_only.loc[(combined_csv_ss_only['split_column'].str[5].str.contains('AR',na = False)) & (combined_csv_ss_only['Hitpath ID'].isna()) ,'Hitpath ID'] =combined_csv_ss_only['split_column'].str[6].str.extract(r'\\b(\\d{4,5}).*')\n",
    "combined_csv_ss_ar_flow = combined_csv_ss_only[(combined_csv_ss_only['sub_id'].str.contains('AR|PR|HE|SE')) |  (combined_csv_ss_only['Job_id']==0)]\n",
    "combined_csv_ss_ar_flow['flow_abbr'] = 'FLOW'\n",
    "combined_csv_ss_ar_flow.loc[(combined_csv_ss_ar_flow['sub_id'].str.contains('AR|PR|HE|SE')),'flow_abbr'] = combined_csv_ss_ar_flow['split_column'].str[5]\n",
    "flows_clean1['flow_abbr'] = 'FLOW'\n",
    "flows_clean1.loc[(flows_clean1['Offer'].str.contains('AR|PR|HE|SE')),'flow_abbr'] = flows_clean1['Offer'].str.split(\"_\",expand = True)[2]\n",
    "####add conversions\n",
    "combined_csv_ss_ar_flow1 = combined_csv_ss_ar_flow.groupby(['affiliate_id','Hitpath ID','Shortcode','flow_abbr','Dash_Date_from_subid','Offer Type','Jump Page Version','Offer Wall Id'],dropna = False)[['amount','conversions','Jump Page Clicks']].sum().reset_index()\n",
    "combined_csv_ss_ar_flow1 = combined_csv_ss_ar_flow1.rename(columns=({'amount': 'Revenue','affiliate_id':'affiliate_id1'}))\n",
    "flows_clean1 = flows_clean1.groupby(['Hitpath ID', 'Date', 'Offer','flow_abbr','Dataset', 'Shortcode', 'Shortcode Name',\\\n",
    "       'Revenue Source', 'Code_Type', 'AR Day', \\\n",
    "       'DP.DS or DP.sV', 'Affiliate_id']).sum().reset_index()\n",
    "print(combined_csv_ss_ar_flow1['Jump Page Clicks'].sum())\n",
    "unique_id_count = combined_csv_ss_ar_flow1[['Dash_Date_from_subid','Hitpath ID','Shortcode','affiliate_id1']].value_counts().reset_index(name = 'count')\n",
    "combined_csv_ss_ar_flow1 = combined_csv_ss_ar_flow1.merge(unique_id_count, on = ['Dash_Date_from_subid','Hitpath ID','Shortcode','affiliate_id1'],how = 'left')\n",
    "flows_clean2 = flows_clean1.merge(combined_csv_ss_ar_flow1, left_on = ['Date','Hitpath ID','Shortcode','Affiliate_id','flow_abbr'],right_on = ['Dash_Date_from_subid','Hitpath ID','Shortcode','affiliate_id1','flow_abbr'],how = 'outer')\n",
    "flows_clean2['count'] = flows_clean2['count'].fillna(1)\n",
    "flows_clean2['Delivered'] = flows_clean2['Delivered'] /flows_clean2['count']\n",
    "flows_clean2['Optout'] = flows_clean2['Optout'] /flows_clean2['count']\n",
    "flows_clean2['Cost'] = flows_clean2['Cost'] /flows_clean2['count']\n",
    "flows_clean2['Clicks'] = flows_clean2['Clicks'] /flows_clean2['count']\n",
    "print(flows_clean2['Jump Page Clicks'].sum())\n",
    "\n",
    "flows_clean2['Revenue Source'] = 'Short Code - SS Flow'\n",
    "flows_clean2['Code_Type'] = 'Short Code'\n",
    "flows_clean2.loc[flows_clean2['Date'].isna(), 'Date'] = flows_clean2['Dash_Date_from_subid']\n",
    "flows_clean2.loc[(flows_clean2['Shortcode'] == '51797') & (flows_clean2['Shortcode Name'].isna()), 'Shortcode Name'] = 'FLC'\n",
    "flows_clean2.loc[(flows_clean2['Shortcode'] == '80837') & (flows_clean2['Shortcode Name'].isna()), 'Shortcode Name'] = 'MBC'\n",
    "flows_clean2['Revenue'] =  flows_clean2['Revenue'].fillna(0)\n",
    "flows_clean2.columns = flows_clean2.columns.str.title()\n",
    "#flows_clean2.loc[flows_clean2['Affiliate_Id'].isna(),'Affiliate_Id' ]  = flows_clean2['Affiliate_Id1']\n",
    "flows_clean_no_delivered = flows_clean2.loc[flows_clean2['Affiliate_Id'].isna(), ] \n",
    "flows_clean_no_delivered['Affiliate_Id']= flows_clean_no_delivered['Affiliate_Id1']\n",
    "flows_clean_no_delivered = flows_clean_no_delivered.dropna(axis =1 , how ='all') \n",
    "flows_clean2 = flows_clean2.loc[flows_clean2['Affiliate_Id'].isna() == False, ] \n",
    "\n",
    "check_no_match = combined_csv_ss_ar_flow.merge(flows_clean1, how = 'left', right_on = ['Date','Hitpath ID','Shortcode','Affiliate_id'],left_on = ['Dash_Date_from_subid','Hitpath ID','Shortcode','affiliate_id'])\n",
    "check_no_match1 = check_no_match[check_no_match['Delivered'].isna()]\n",
    "check_no_match_with_flow = check_no_match1.dropna(axis =1 , how ='all') \n",
    "\n",
    "\n",
    "#jobs without creative or job \n",
    "###Add \n",
    "combined_csv_ss_only1 = combined_csv_ss_only[~((combined_csv_ss_only['sub_id'].str.contains('AR|PR|HE|SE')) |  (combined_csv_ss_only['Job_id']==0))]\n",
    "combined_csv_ss_only1 = pd.concat([combined_csv_ss_only1,check_no_match_with_flow]).reset_index(drop = True)\n",
    "combined_csv_ss_creative_na = combined_csv_ss_only1[(combined_csv_ss_only1['creative_id'].isna())| (combined_csv_ss_only1['creative_id']<100000)| ( (combined_csv_ss_only1['Job_id'].isna()))]\n",
    "\n",
    "#jobs with creative and job \n",
    "combined_csv_ss_creative_notna = combined_csv_ss_only1[~((combined_csv_ss_only1['creative_id'].isna()) | (combined_csv_ss_only1['creative_id']<100000)| ( (combined_csv_ss_only1['Job_id'].isna())))]\n",
    "\n",
    "# get revenue and delivery stats for jobs with creatives\n",
    "\"\"\" \n",
    "combined_csv_ss_creative_notna = combined_csv_ss_creative_notna.groupby(['Job_id', 'creative_id'])['amount'].sum().reset_index()\n",
    "merge_frame = creative_stats[['JobId', 'CreativeId']].append(combined_csv_ss_creative_notna[['Job_id', 'creative_id']]).drop_duplicates()\n",
    "creative_stats = creative_stats[creative_stats['Tstamp']>= '2022-11-01']\n",
    "print(combined_csv_ss_creative_notna['amount'].sum())\n",
    "combined_csv_ss_creative_notna = combined_csv_ss_creative_notna.merge(creative_stats, left_on = ['Job_id', 'creative_id'], right_on = ['JobId', 'CreativeId'], how = 'left')\n",
    "print(combined_csv_ss_creative_notna['amount'].sum())\n",
    "\"\"\" \n",
    "# get revenue and delivery stats for jobs with creatives\n",
    "\n",
    "###add conversions\n",
    "combined_csv_ss_creative_notna = combined_csv_ss_creative_notna.groupby(['Job_id', 'creative_id','Jump Page Version','Offer Wall Id','Offer Type'])[['amount','conversions','Jump Page Clicks']].sum().reset_index()\n",
    "creative_stats11 = creative_stats[creative_stats['Tstamp']>= '2022-11-01']\n",
    "creative_stats11 = creative_stats11.rename(columns=({'JobId': 'Job_id', 'CreativeId':'creative_id'}))\n",
    "creative_stats11['Job_id'] = creative_stats11['Job_id'].astype('int')\n",
    "\n",
    "merge_frame = pd.concat([creative_stats11[['Job_id', 'creative_id']],combined_csv_ss_creative_notna[['Job_id', 'creative_id']]]).drop_duplicates()\n",
    "merge_frame['creative_id'] = merge_frame['creative_id'].astype('int')\n",
    "creative_stats11['creative_id'] = creative_stats11['creative_id'].astype('int')\n",
    "merge_frame = merge_frame.merge(combined_csv_ss_creative_notna, how = 'left')\n",
    "merge_frame = merge_frame.merge(creative_stats11, how = 'left')\n",
    "\n",
    "combined_csv_ss_creative_notna = merge_frame.fillna(0)\n",
    "\n",
    "##add conversions\n",
    "combined_csv_ss_creative_notna = combined_csv_ss_creative_notna[['Offer','Job_id', 'creative_id','Jump Page Version','Offer Wall Id','Offer Type', 'CreativeName',\\\n",
    "       'Creative', 'Delivered', 'Cost', 'Optout', 'Clicks', 'amount','conversions','Jump Page Clicks']]\n",
    "combined_csv_ss_creative_notna = combined_csv_ss_creative_notna.rename(columns=({'amount': 'Revenue'}))\n",
    "#combined_csv_ss_creative_notna = combined_csv_ss_creative_notna.drop_duplicates()\n",
    "#combined_csv_ss_creative_notna = combined_csv_ss_creative_notna.groupby(['date','Offer','Job_id','CreativeId', 'CreativeName', 'Creative', 'Delivered', 'Cost', 'Unsubcount', 'Clicks']).sum('Revenue').reset_index()\n",
    "\n",
    "#get jobs information from jobs file\n",
    "jobs['job_id'] =jobs['job_id'].astype('int')\n",
    "combined_csv_ss_creative_notna = combined_csv_ss_creative_notna.merge(jobs[['job_id','job_name', 'shortcode', 'start_tstamp',\n",
    "       'end_tstamp', 'scheduled_tstamp', 'status_text', 'segments','Scheduling Time']], left_on = 'Job_id', right_on = 'job_id', how = 'left')\n",
    "combined_csv_ss_creative_notna['date'] = pd.to_datetime(combined_csv_ss_creative_notna['scheduled_tstamp']).dt.strftime(\"%Y-%m-%d\")\n",
    "combined_csv_ss_creative_notna = combined_csv_ss_creative_notna.drop(columns ='job_id')\n",
    "combined_csv_ss_creative_notna.columns = combined_csv_ss_creative_notna.columns.str.title()\n",
    "# create send strategy \n",
    "combined_csv_ss_creative_notna['Send Strategy'] = np.nan \n",
    "combined_csv_ss_creative_notna.loc[ combined_csv_ss_creative_notna['Job_Name'].str.split('_|[|]| ').str[-2:].astype('str').apply(lambda l: 'P' in l),'Send Strategy'] = 'P'\n",
    "combined_csv_ss_creative_notna.loc[ combined_csv_ss_creative_notna['Job_Name'].str.split('_|[|]| ').str[-2:].astype('str').apply(lambda l: 'T' in l),'Send Strategy'] = 'OT'\n",
    "combined_csv_ss_creative_notna.loc[ combined_csv_ss_creative_notna['Job_Name'].str.split('_|[|]| ').str[-2:].astype('str').apply(lambda l: 'OT' in l),'Send Strategy'] = 'OT'\n",
    "combined_csv_ss_creative_notna.loc[ combined_csv_ss_creative_notna['Job_Name'].str.split('_|[|]| ').str[-2:].astype('str').apply(lambda l: 'PT' in l),'Send Strategy'] = 'PT'\n",
    "combined_csv_ss_creative_notna.loc[ combined_csv_ss_creative_notna['Job_Name'].str.split('_|[|]| ').str[-2:].astype('str').apply(lambda l: 'AR' in l),'Send Strategy'] = 'AR'\n",
    "combined_csv_ss_creative_notna.loc[ combined_csv_ss_creative_notna['Job_Name'].str.split('_|[|]| ').str[-2:].astype('str').apply(lambda l: 'CT' in l),'Send Strategy'] = 'CT'\n",
    "combined_csv_ss_creative_notna.loc[ combined_csv_ss_creative_notna['Job_Name'].str.split('_|[|]| ').str[-2:].astype('str').apply(lambda l: 'MI' in l),'Send Strategy'] = 'MI'\n",
    "combined_csv_ss_creative_notna.loc[ combined_csv_ss_creative_notna['Job_Name'].str.split('_|[|]| ').str[-2:].astype('str').apply(lambda l: 'JT' in l),'Send Strategy'] = 'JT'\n",
    "# we didn't define the right send strategy for the following jobs in mamba\n",
    "combined_csv_ss_creative_notna.loc[combined_csv_ss_creative_notna['Job_Name'].str.contains('SS_FLC_PN-FC-21DC-VZN_12305_P_01Nov23', na = False),'Send Strategy'] ='JT'\n",
    "combined_csv_ss_creative_notna.loc[combined_csv_ss_creative_notna['Job_Name'].str.contains('SS_FLC_PN-FC-21DC-VZN_12305v1_CT_01Nov23', na = False),'Send Strategy'] ='JT'\n",
    "\n",
    "combined_csv_ss_creative_notna = combined_csv_ss_creative_notna.reset_index(drop=True)\n",
    "\n",
    "#get revenue stats for jobs without creatives.\n",
    "\n",
    "##add conversions\n",
    "combined_csv_ss_creative_na = combined_csv_ss_creative_na[['date','Job_id', 'campaign_id','amount','sub_id','conversions','Jump Page Clicks','Jump Page Version','Offer Wall Id','Offer Type']]\n",
    "combined_csv_ss_creative_na = combined_csv_ss_creative_na.rename(columns=({'amount': 'Revenue'}))\n",
    "#get delivery and click stats for jobs without creatives from job file\n",
    " \n",
    "creative_id_na['Date'] = pd.to_datetime(creative_id_na['Tstamp']).dt.strftime(\"%Y-%m-%d\")\n",
    "creative_id_na11=creative_id_na[creative_id_na['Date'] >= '2022-11-01']\n",
    "creative_id_na11['Campaign_Id'] = creative_id_na11['Offer'].str.split(' ',expand = True)[0]\n",
    "creative_id_na11.loc[creative_id_na11['Campaign_Id'].str.isdigit() == False,'Campaign_Id' ]  = np.nan\n",
    "creative_id_na11['Campaign_Id']  = creative_id_na11['Campaign_Id'].astype(float)\n",
    "creative_id_na11['Shortcode Name'] = ''\n",
    "creative_id_na11.loc[creative_id_na11['Offer'].str.contains('CSS', na = False),'Shortcode Name'] = 'CSS'\n",
    "creative_id_na11.loc[creative_id_na11['Offer'].str.contains('HZB', na = False),'Shortcode Name'] = 'HZB'\n",
    "# the jobid didn't use in the combined_csv_ss_creative_notna\n",
    "#unjoined_creative_stats11 = creative_stats11[~creative_stats11['Job_id'].isin(combined_csv_ss_creative_notna['Job_Id'])]\n",
    "creative_stats11['Date'] = pd.to_datetime(creative_stats11['Tstamp']).dt.strftime(\"%Y-%m-%d\")\n",
    "creative_stats11['Campaign_Id'] = creative_stats11['Offer'].str.split(' ',expand = True)[0]\n",
    "creative_stats11.loc[creative_stats11['Campaign_Id'].str.isdigit() == False,'Campaign_Id' ]  = np.nan\n",
    "creative_stats11['Campaign_Id']  = creative_stats11['Campaign_Id'].astype(float)\n",
    "creative_stats11['Shortcode Name'] = ''\n",
    "creative_stats11.loc[creative_stats11['Offer'].str.contains('CSS', na = False),'Shortcode Name'] = 'CSS'\n",
    "creative_stats11.loc[creative_stats11['Offer'].str.contains('HZB', na = False),'Shortcode Name'] = 'HZB'\n",
    "\n",
    "#\n",
    "combined_csv_ss_creative_na.loc[combined_csv_ss_creative_na['sub_id'].str.contains('70610' ,na=False ),'shortcode']= '70610'\n",
    "combined_csv_ss_creative_na.loc[combined_csv_ss_creative_na['sub_id'].str.contains('44345',na=False),'shortcode']= '44345'\n",
    "combined_csv_ss_creative_na.loc[combined_csv_ss_creative_na['sub_id'].str.contains('70610' ,na=False ),'Shortcode Name']= 'CSS'\n",
    "combined_csv_ss_creative_na.loc[combined_csv_ss_creative_na['sub_id'].str.contains('44345',na=False),'Shortcode Name']= 'HZB'\n",
    "combined_csv_ss_creative_na['campaign_id'] = combined_csv_ss_creative_na['sub_id'].str.split('_',expand = True)[6]\n",
    "combined_csv_ss_creative_na.loc[combined_csv_ss_creative_na['campaign_id'].str.isdigit()== False, 'campaign_id'] = combined_csv_ss_creative_na['sub_id'].str.split('_',expand = True)[5]\n",
    "combined_csv_ss_creative_na.loc[combined_csv_ss_creative_na['campaign_id'].str.isdigit()== False, 'campaign_id'] = combined_csv_ss_creative_na['sub_id'].str.split('_',expand = True)[4]\n",
    "combined_csv_ss_creative_na['affiliate_id'] = combined_csv_ss_creative_na['sub_id'].str.extract(r'(46\\d{4})')\n",
    "combined_csv_ss_creative_na.loc[combined_csv_ss_creative_na['campaign_id'].str.isdigit() == False,'campaign_id' ]  = np.nan\n",
    "combined_csv_ss_creative_na['campaign_id'] = combined_csv_ss_creative_na['campaign_id'].astype(float)\n",
    "#combined_csv_ss_creative_na['date'] = pd.to_datetime(combined_csv_ss_creative_na['scheduled_tstamp']).dt.strftime(\"%Y-%m-%d\")\n",
    "print(jobs)\n",
    "job_11 = jobs[jobs['scheduled_tstamp'] >= '2022-11-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "611cc0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(flows_clean[(flows_clean['Date']>= '2023-10-01') &(flows_clean['Date']< '2023-11-01')]['Delivered'].sum())\n",
    "# print(flows_clean1[(flows_clean1['Date']>= '2023-10-01') &(flows_clean1['Date']< '2023-11-01')]['Delivered'].sum())\n",
    "# print(flows_clean2[(flows_clean2['Date']>= '2023-10-01') & (flows_clean2['Date']< '2023-11-01')]['Delivered'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f0062bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_csv_ss_creative_na = combined_csv_ss_creative_na.merge(creative_id_na11[['Date','Scheduling Time','JobId','Offer','Campaign_Id', 'Segment', 'CreativeId', 'CreativeName','Shortcode Name',\n",
    "       'Creative','Delivered',  'Optout', 'Clicks', 'Cost']], left_on = ['Job_id','campaign_id','date','Shortcode Name'], right_on = ['JobId','Campaign_Id','Date','Shortcode Name'], how = 'outer', copy = False)\n",
    "combined_csv_ss_creative_na = combined_csv_ss_creative_na.merge(job_11[['job_id','job_name', 'offer', 'shortcode', 'scheduled_tstamp', 'status_text', 'segments', \n",
    "        'delivered', 'optout', 'clicks', 'cost']], left_on = ['Job_id'], right_on =['job_id'] , how = 'left' , copy = False)\n",
    "combined_csv_ss_creative_na.loc[combined_csv_ss_creative_na['date'].isna(), 'date'] = combined_csv_ss_creative_na['Date']\n",
    "combined_csv_ss_creative_na.loc[combined_csv_ss_creative_na['campaign_id'].isna(), 'campaign_id'] = combined_csv_ss_creative_na['Campaign_Id']\n",
    "combined_csv_ss_creative_na.loc[combined_csv_ss_creative_na['Offer'].isna(), 'Offer'] = combined_csv_ss_creative_na['offer']\n",
    "combined_csv_ss_creative_na.loc[combined_csv_ss_creative_na['Delivered'].isna(), 'Delivered'] = combined_csv_ss_creative_na['delivered']\n",
    "combined_csv_ss_creative_na.loc[combined_csv_ss_creative_na['Optout'].isna(), 'Optout'] = combined_csv_ss_creative_na['optout']\n",
    "combined_csv_ss_creative_na.loc[combined_csv_ss_creative_na['Clicks'].isna(), 'Clicks'] = combined_csv_ss_creative_na['clicks']\n",
    "combined_csv_ss_creative_na.loc[combined_csv_ss_creative_na['Cost'].isna(), 'Cost'] = combined_csv_ss_creative_na['cost']\n",
    "combined_csv_ss_creative_na.loc[combined_csv_ss_creative_na['Job_id'].isna(), 'Job_id'] = combined_csv_ss_creative_na['JobId']\n",
    "combined_csv_ss_creative_na = combined_csv_ss_creative_na.drop(columns = ['job_id','Campaign_Id','Date','offer', 'delivered', 'optout', 'clicks', 'cost','JobId'])\n",
    "#combined_csv_ss_creative_na = combined_csv_ss_creative_na.reset_index()\n",
    "#combined_csv_ss_creative_na['date'] = pd.to_datetime(combined_csv_ss_creative_na['scheduled_tstamp']).dt.strftime(\"%Y-%m-%d\")\n",
    "combined_csv_ss_creative_na.columns = combined_csv_ss_creative_na.columns.str.title()\n",
    "\n",
    "# create send strategy \n",
    "combined_csv_ss_creative_na['Send Strategy'] = np.nan \n",
    "combined_csv_ss_creative_na.loc[ combined_csv_ss_creative_na['Job_Name'].str.split('_|[|]| ').str[-2:].astype('str').apply(lambda l: 'P' in l),'Send Strategy'] = 'P'\n",
    "combined_csv_ss_creative_na.loc[ combined_csv_ss_creative_na['Job_Name'].str.split('_|[|]| ').str[-2:].astype('str').apply(lambda l: 'T' in l),'Send Strategy'] = 'OT'\n",
    "combined_csv_ss_creative_na.loc[ combined_csv_ss_creative_na['Job_Name'].str.split('_|[|]| ').str[-2:].astype('str').apply(lambda l: 'OT' in l),'Send Strategy'] = 'OT'\n",
    "combined_csv_ss_creative_na.loc[ combined_csv_ss_creative_na['Job_Name'].str.split('_|[|]| ').str[-2:].astype('str').apply(lambda l: 'PT' in l),'Send Strategy'] = 'PT'\n",
    "combined_csv_ss_creative_na.loc[ combined_csv_ss_creative_na['Job_Name'].str.split('_|[|]| ').str[-2:].astype('str').apply(lambda l: 'AR' in l),'Send Strategy'] = 'AR'\n",
    "combined_csv_ss_creative_na.loc[ combined_csv_ss_creative_na['Job_Name'].str.split('_|[|]| ').str[-2:].astype('str').apply(lambda l: 'CT' in l),'Send Strategy'] = 'CT'\n",
    "combined_csv_ss_creative_na.loc[ combined_csv_ss_creative_na['Job_Name'].str.split('_|[|]| ').str[-2:].astype('str').apply(lambda l: 'MI' in l),'Send Strategy'] = 'MI'\n",
    "combined_csv_ss_creative_na.loc[ combined_csv_ss_creative_na['Job_Name'].str.split('_|[|]| ').str[-2:].astype('str').apply(lambda l: 'JT' in l),'Send Strategy'] = 'JT'\n",
    "combined_csv_ss_creative_na.loc[combined_csv_ss_creative_na['Job_Id'] == 0, 'Send Strategy'] = 'AR'\n",
    "combined_csv_ss_creative_na.loc[combined_csv_ss_creative_na['Job_Id'] == 0, 'Revenue Source'] =  'Short Code - SS Jobs'\n",
    "#combined_csv_ss_creative_na = combined_csv_ss_creative_na.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e949691",
   "metadata": {},
   "source": [
    "### Other Revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12ddb62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_4212\\2422394425.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  combined_csv_ss_exclude['date'] = pd.to_datetime(combined_csv_ss_exclude['date'])\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_4212\\2422394425.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  combined_csv_push['date'] = pd.to_datetime(combined_csv_push['date'])\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_4212\\2422394425.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  combined_csv_w1['Send Strategy'] = 'AR'\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_4212\\2422394425.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  combined_csv_w1['Revenue Source'] = 'Short Code - SS Jobs'\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_4212\\2422394425.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  combined_csv_w1['date'] = pd.to_datetime(combined_csv_w1['date'])\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_4212\\2422394425.py:39: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  combined_csv_ss_last =  combined_csv_ss_rest[ ((combined_csv['affiliate_name'].str.lower().str.contains(\"push\", na = False) == False)) & (combined_csv_ss_rest['subid_2'].str.lower().str.contains(\"w1\", na = False) == False)]\n"
     ]
    }
   ],
   "source": [
    "#case #3: revenue for email \n",
    "combined_csv_ss_exclude = combined_csv\n",
    "combined_csv_ss_exclude['affiliate_id'] = combined_csv_ss_exclude['affiliate_id'].astype(str).str.split(\".\",expand = True)[0]\n",
    "combined_csv_ss_exclude['site_id'] = combined_csv_ss_exclude['site_id'].astype(str).str.split(\".\",expand = True)[0]\n",
    "\n",
    "combined_csv_ss_exclude =  combined_csv_ss_exclude.loc[(combined_csv_ss_exclude['affiliate_id'].isin(email_pubid)) | (combined_csv_ss_exclude['site_id'].isin(email_pubid)),] \n",
    "combined_csv_ss_exclude['date'] = pd.to_datetime(combined_csv_ss_exclude['date'])\n",
    "combined_csv_ss_exclude = combined_csv_ss_exclude[[ 'amount', 'date']]\n",
    "combined_csv_ss_exclude = combined_csv_ss_exclude.rename(columns=({'amount': 'Revenue','date': 'Date'}))\n",
    "combined_csv_ss_exclude_11 = combined_csv_ss_exclude[combined_csv_ss_exclude['Date']>='2022-11-01'].reset_index(drop=True)\n",
    "# case #4: push revenue \n",
    "email_pubid_int = emit['Revenue Pub ID'].unique().tolist()\n",
    "combined_csv_ss_rest =  combined_csv[(combined_csv['first_split']!='SS') & (combined_csv['first_split']!='SMS') & (combined_csv['affiliate_id'].astype(float).isin(email_pubid_int)== False) & (combined_csv['site_id'].astype(float).isin(email_pubid_int)== False) & (combined_csv['first_split']!='MD') &  (combined_csv['first_split']!='TB')  ] \n",
    "#combined_csv_ss_rest =  combined_csv[(combined_csv['first_split']!='SS') & (combined_csv['first_split']!='SMS') & (combined_csv['subid_2'].str.contains('FLC|MBC',na = False)== False)& (combined_csv['affiliate_id'].astype(float).isin(email_pubid_int)== False) & (combined_csv['site_id'].astype(float).isin(email_pubid_int)== False) & (combined_csv['first_split']!='MD')   ] \n",
    "\n",
    "combined_csv_push = combined_csv_ss_rest.loc[combined_csv_ss_rest['affiliate_name'].str.lower().str.contains(\"push\", na = False),]\n",
    "combined_csv_push['date'] = pd.to_datetime(combined_csv_push['date'])\n",
    "combined_csv_push = combined_csv_push[[ 'amount', 'date']]\n",
    "combined_csv_push = combined_csv_push.rename(columns=({'amount': 'Revenue','date': 'Date'}))\n",
    "combined_csv_push_11 =  combined_csv_push[combined_csv_push['Date']>='2022-11-01'].reset_index(drop=True)\n",
    "\n",
    "# case #6: previos AR job\n",
    "combined_csv_w1= combined_csv_ss_rest.loc[combined_csv_ss_rest['subid_2'].str.lower().str.contains(\"_w1\", na = False),]\n",
    "combined_csv_w1['Send Strategy'] = 'AR'\n",
    "combined_csv_w1['Revenue Source'] = 'Short Code - SS Jobs'\n",
    "combined_csv_w1['date'] = pd.to_datetime(combined_csv_w1['date'])\n",
    "combined_csv_w1 = combined_csv_w1.rename(columns=({'amount': 'Revenue','date': 'Date','affiliate_id':'Affiliate_Id', 'subid_2':'Sub_Id'}))\n",
    "combined_csv_w1['Hitpath Id'] = combined_csv_w1['campaign_id'].astype('str').str.split('.',expand = True)[0]\n",
    "# split subid_2 with \"_\"\n",
    "combined_csv_w1['Shortcode Name'] = np.nan\n",
    "combined_csv_w1.loc[combined_csv_w1['Date']>= '2022-11-01','Shortcode Name'] = combined_csv_w1['Sub_Id'].str.split(\"_\",expand = True)[2]\n",
    "## when shortcode name is 460654\n",
    "combined_csv_w1.loc[combined_csv_w1['Shortcode Name'] == '460654', 'Shortcode Name' ] = np.nan\n",
    "## when shortcode name is PLV, it's long code \n",
    "combined_csv_w1 = combined_csv_w1[['Revenue', 'Date', 'Send Strategy', 'Revenue Source', 'Hitpath Id', 'Affiliate_Id', 'Sub_Id','Shortcode Name']]\n",
    "combined_csv_w1_11 = combined_csv_w1[combined_csv_w1['Date']>='2022-11-01']\n",
    "# case #5: revenue the rest of the revenue. \n",
    "\n",
    "combined_csv_ss_last =  combined_csv_ss_rest[ ((combined_csv['affiliate_name'].str.lower().str.contains(\"push\", na = False) == False)) & (combined_csv_ss_rest['subid_2'].str.lower().str.contains(\"w1\", na = False) == False)] \n",
    "combined_csv_ss_last_11 = combined_csv_ss_last[combined_csv_ss_last['date']>='2022-11-01'].reset_index(drop=True)\n",
    "\n",
    "# identify Revenue Sourc\n",
    "combined_csv_ss_creative_na['Revenue Source'] = 'Short Code - SS Jobs'\n",
    "combined_csv_ss_creative_notna['Revenue Source'] = 'Short Code - SS Jobs'\n",
    "combined_csv_ss_exclude['Revenue Source'] = 'Email'\n",
    "combined_csv_push['Revenue Source'] = 'Push'\n",
    "#historic_data['Revenue Source'] = 'Short Code - SS Jobs'\n",
    "#historic_data['Send Strategy'] = np.nan\n",
    "\n",
    "#combined_csv_ss_creative_na = combined_csv_ss_creative_na.drop_duplicates()\n",
    "SS_New_data = pd.concat([combined_csv_ss_creative_notna,combined_csv_ss_creative_na,flows_clean2,combined_csv_ss_exclude, combined_csv_push,combined_csv_w1], axis=0)\n",
    "SS_New_data['Ecpm'] = SS_New_data['Revenue'] * 1000/SS_New_data['Delivered']\n",
    "SS_New_data['Roi'] = SS_New_data['Cost'] - SS_New_data['Revenue']\n",
    "#SS_New_data['Sent'] = SS_New_data['Total']\n",
    "\n",
    "#combining historic data(before Nov 1) with new data to get SS full data.\n",
    "#SS_New_data = SS_New_data[historic_data.columns]\n",
    "\n",
    "\n",
    "#SS_Full_data = pd.concat([SS_New_data,historic_data], axis = 0)\n",
    "SS_Full_data = SS_New_data\n",
    "SS_Full_data['Date'] = pd.to_datetime(SS_Full_data['Date'])\n",
    "SS_Full_data = SS_Full_data.sort_values('Date', ascending=False)\n",
    "#SS_Full_data.loc[(SS_Full_data['Job_Id'].isna()) & (SS_Full_data['Revenue Source']==  'Short Code - SS Jobs')  ,'Revenue Source'] = 'Short Code - Opt In'\n",
    "SS_Full_data.loc[ (SS_Full_data['Job_Id'].isna()) & (SS_Full_data['Sub_Id'].str.contains('AR', na = False)) ,'Revenue Source'] =  'Short Code - SS Flow'\n",
    "SS_Full_data.loc[ (SS_Full_data['Job_Id'].isna())&  (SS_Full_data['Sub_Id'].str.contains('AR', na = False)) ,'Send Strategy'] = 'AR'\n",
    "SS_Full_data.loc[(SS_Full_data['Job_Id'].isna()) & (SS_Full_data['Revenue Source']==  'Short Code - Opt In') ,'Send Strategy'] = 'Opt In'\n",
    "SS_Full_data.loc[(SS_Full_data['Job_Id']==0) & (SS_Full_data['Revenue Source']==  'Short Code - SS Jobs') & (SS_Full_data['Send Strategy'].isna()) ,'Send Strategy'] = 'AR'\n",
    "SS_Full_data.to_csv(localfolder + 'SS_Fulldata.csv', index =False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abe1731",
   "metadata": {},
   "source": [
    "## long code MP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6a33296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ######## MP BEGIN ########\n",
    "# #Read MP file and clean. All stats present in MP_Campaigns file.\n",
    "\n",
    "# MP_campaigns = pd.read_csv(localfolder + 'SMS_SC_MP_Campaigns.csv')\n",
    "# MP_campaigns = MP_campaigns[~MP_campaigns['ACTDATE'].isna()]\n",
    "# MP_campaigns = MP_campaigns[~MP_campaigns['ACTDATE'].isin(['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])]\n",
    "# MP_campaigns['ACTDATE']= pd.to_datetime(MP_campaigns['ACTDATE'])\n",
    "# MP_campaigns = MP_campaigns[MP_campaigns['ACTDATE'] > two_months_ago]\n",
    "# #a = MP_campaigns.isna().sum()\n",
    "# drop_columns = ['Daily Opt Out', 'Unnamed: 28', 'Unnamed: 53', 'Reference', 'c1NEW' , 'Unnamed: 61', 'Unnamed: 62', 'Unnamed: 64', 'Unnamed: 72', 'Unnamed: 73']\n",
    "# MP_campaigns.drop(columns=drop_columns, inplace=True, axis=1)\n",
    "# MP_campaigns= MP_campaigns[~MP_campaigns['Done.'].isna()]\n",
    "# MP_campaigns.columns = MP_campaigns.columns.str.strip()\n",
    "\n",
    "# # remove % and $ symbold\n",
    "# MP_campaigns['REV'] = MP_campaigns['REV'].str.strip()\n",
    "# MP_campaigns['REV'] = MP_campaigns['REV'].replace('$ -', np.nan)\n",
    "# MP_campaigns['REV'] = MP_campaigns['REV'].str.replace('$','')\n",
    "# MP_campaigns['REV'] = MP_campaigns['REV'].str.replace(',','')\n",
    "# MP_campaigns['REV'] = MP_campaigns['REV'].astype('float')\n",
    "\n",
    "# MP_campaigns['COST'] = MP_campaigns['COST'].str.strip()\n",
    "# MP_campaigns['COST'] = MP_campaigns['COST'].replace('$ -', np.nan)\n",
    "# MP_campaigns['COST'] = MP_campaigns['COST'].str.replace('$','')\n",
    "# MP_campaigns['COST'] = MP_campaigns['COST'].astype('float')\n",
    "\n",
    "# MP_campaigns['eCPM'] = MP_campaigns['eCPM'].str.strip()\n",
    "# MP_campaigns['eCPM'] = MP_campaigns['eCPM'].replace('$ -', np.nan)\n",
    "# MP_campaigns['eCPM'] = MP_campaigns['eCPM'].str.replace('$','')\n",
    "# MP_campaigns['eCPM'] = MP_campaigns['eCPM'].astype('float')\n",
    "\n",
    "# MP_campaigns['gPROFIT'] = MP_campaigns['gPROFIT'].str.strip()\n",
    "# MP_campaigns['gPROFIT'] = MP_campaigns['gPROFIT'].str.replace('$','')\n",
    "# MP_campaigns['gPROFIT'] = MP_campaigns['gPROFIT'].replace('-',np.nan)\n",
    "# MP_campaigns['gPROFIT'] = MP_campaigns['gPROFIT'].replace(' -',np.nan)\n",
    "# MP_campaigns['gPROFIT'] = MP_campaigns['gPROFIT'].astype(str).str.replace('\\((.*)\\)', '-\\\\1')\n",
    "# MP_campaigns['gPROFIT'] = MP_campaigns['gPROFIT'].str.replace(',','')\n",
    "# MP_campaigns['gPROFIT'] = MP_campaigns['gPROFIT'].astype('float')\n",
    "\n",
    "# MP_campaigns['CLICK %'] = MP_campaigns['CLICK %'].str.strip()\n",
    "# MP_campaigns['CLICK %'] = MP_campaigns['CLICK %'].str.replace('%','')\n",
    "# MP_campaigns['CLICK %'] = MP_campaigns['CLICK %'].astype('float')\n",
    "\n",
    "# MP_campaigns['gMARGIN'] = MP_campaigns['gMARGIN'].str.strip()\n",
    "# MP_campaigns['gMARGIN'] = MP_campaigns['gMARGIN'].str.replace('%','')\n",
    "# MP_campaigns['gMARGIN'] = MP_campaigns['gMARGIN'].astype('float')\n",
    "\n",
    "# #MP_campaigns.to_csv(localfolder + 'MP_data.csv', index =False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cedc774",
   "metadata": {},
   "source": [
    "## Combine all data from all sources "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8421160f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_4212\\689150103.py:3: DtypeWarning: Columns (0,4,5,6,7,15,17,18,19,20,21,22,24,27,29,34,35,36,37,39,40,43) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  SS_data = pd.read_csv(localfolder + 'SS_Fulldata.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "374429.14589800005\n",
      "374429.14589800005\n"
     ]
    }
   ],
   "source": [
    "#4 merge SS and MP: not possible as there are columns in MP not in SS.\n",
    "#LC_data = pd.read_csv(localfolder + 'SMS_LC_Campaigns_clean.csv')\n",
    "SS_data = pd.read_csv(localfolder + 'SS_Fulldata.csv')\n",
    "\n",
    "# lc_df_full = lc_df_full.reset_index(drop=True)\n",
    "# LC_data = LC_data.reset_index(drop=True)\n",
    "# LC_data = pd.concat([lc_df_full,LC_data], axis=0, ignore_index=True)\n",
    "# LC_data.loc[LC_data['Affiliate_Id'].isna(),'Affiliate_Id'] = LC_data['pubID']\n",
    "# LC_data.loc[LC_data['Hitpath_Offer_ID'].isna(),'Hitpath_Offer_ID' ] = LC_data['sid']\n",
    "# LC_data['Affiliate_Id'] = LC_data['Affiliate_Id'].astype(str).str.split('.',expand = True)[0]\n",
    "# LC_data['Hitpath_Offer_ID'] = LC_data['Hitpath_Offer_ID'].astype(str).str.split('.',expand = True)[0]\n",
    "SS_data['Code_Type'] = 'Short Code'\n",
    "# LC_data['Code_Type'] = 'Long Code'\n",
    "SS_data.loc[SS_data['Revenue Source'] == 'Email', 'Code_Type'] = 'Email'\n",
    "SS_data.loc[SS_data['Revenue Source'] == 'Push', 'Code_Type'] = 'Push'\n",
    "########Newly added\n",
    "SS_data.loc[SS_data['Shortcode'].astype(str).str.replace('.0','').str.len() ==11, 'Code_Type'] = 'Long Code'\n",
    "SS_data.loc[\n",
    "    (SS_data['Shortcode'].astype(str).str.replace('.0','').str.len() == 11) &\n",
    "    (SS_data['Shortcode'].astype(str).str.startswith('183')),\n",
    "    'Code_Type'\n",
    "] = 'Toll Free'\n",
    "\n",
    "SS_data.loc[SS_data['Shortcode'].astype(str).str.replace('.0','').str.len() ==11, 'Revenue Source'] = 'Long Code'\n",
    "SS_data.loc[\n",
    "    (SS_data['Shortcode'].astype(str).str.replace('.0','').str.len() == 11) &\n",
    "    (SS_data['Shortcode'].astype(str).str.startswith('183')),\n",
    "    'Revenue Source'\n",
    "] = 'Toll Free'\n",
    "\n",
    "SS_data['Hitpath_Offer_ID'] = SS_data['Offer'].astype('str').str.extract(r'\\b(\\d{4,5}).*')\n",
    "SS_data.loc[SS_data['Hitpath_Offer_ID'].isna(),'Hitpath_Offer_ID'] = SS_data['Hitpath Id'].astype(str).str.split('.',expand = True)[0]\n",
    "SS_data['Affiliate_Id'] = SS_data['Affiliate_Id'].astype(str).str.split('.',expand = True)[0]\n",
    "#LC_data= LC_data.rename(columns=({'eCPM' : 'Ecpm', 'Clicked': 'Clicks'}))\n",
    "\n",
    "\n",
    "#merge short code and longcode\n",
    "#merged_data = pd.concat([SS_data,LC_data], axis=0, ignore_index=True)\n",
    "merged_data =SS_data\n",
    "\n",
    "# merge offer Wall \n",
    "\n",
    "merged_data['Hitpath_Offer_ID'] = merged_data['Hitpath_Offer_ID'].astype(str).str.split('.',expand = True)[0]\n",
    "merged_data.loc[merged_data['Revenue Source']=='Short Code - SS Flow','Hitpath_Offer_ID'] =merged_data['Hitpath Id'].astype('str').str.split('.',expand = True)[0]\n",
    "merged_data['Offer Wall Id'] = merged_data['Offer Wall Id'].astype('str').str.split('.',expand = True)[0]\n",
    "merged_data.loc[ (merged_data['Offer Wall Id'].isin(offer_wall_id)),'Hitpath_Offer_ID'] = merged_data['Offer Wall Id']\n",
    "merged_data['Offer Type'] =  'Single Offer'\n",
    "merged_data.loc[ (merged_data['Offer Wall Id'].isin(offer_wall_id)),'Offer Type'] = 'Offer Wall'\n",
    "merged_data['DP.SV'] = merged_data['Dp.Ds Or Dp.Sv']\n",
    "merged_data.loc[merged_data['DP.SV'].isnull(), 'DP.SV'] =  merged_data['Segments'].str.split('_',expand = True)[1]\n",
    "dict_publisher = {'DP.DS or DP.sV':['WWM.YFA','ZM.PL'], 'PUBID': ['461680','461681']}\n",
    "dict_publisher = pd.DataFrame(dict_publisher)\n",
    "new_publisher = pd.concat([publisher,dict_publisher])\n",
    "new_publisher = new_publisher.reset_index(drop=True)\n",
    "# publisher drop na rows\n",
    "new_publisher = new_publisher[~new_publisher['DP.DS or DP.sV'].isna()]\n",
    "new_publisher = new_publisher[['DP.DS or DP.sV','PUBID']]\n",
    "new_publisher['PUBID'] = new_publisher['PUBID'].astype(str).str.split('.',expand = True)[0]\n",
    "merged_data = merged_data.merge(new_publisher[['DP.DS or DP.sV','PUBID']], left_on ='DP.SV', right_on = 'DP.DS or DP.sV', how = 'left' )\n",
    "merged_data.loc[(merged_data['Affiliate_Id'].isna()) |  (merged_data['Affiliate_Id']=='nan'), 'Affiliate_Id'] = merged_data['PUBID']\n",
    "print(merged_data['Revenue'].sum())\n",
    "publisher_copy = publisher\n",
    "publisher_copy['DP.DS or DP.sV'] = publisher_copy['DP.DS or DP.sV'].str.replace('WWM.YFA','WWM.YFA.2')\n",
    "publisher_copy['DP.DS or DP.sV'] = publisher_copy['DP.DS or DP.sV'].str.replace('ZM.PL','ZM.PL.2')\n",
    "publisher_copy = publisher_copy[['DP.DS or DP.sV','PUBID']].drop_duplicates()\n",
    "merged_data = merged_data.merge(publisher_copy, left_on ='Affiliate_Id', right_on = 'PUBID', how = 'left')\n",
    "#merged_data = merged_data.merge(new_publisher[['DP.DS or DP.sV','PUBID']], left_on ='Affiliate_Id', right_on = 'PUBID', how = 'left' )\n",
    "merged_data.loc[(merged_data['DP.SV'].isna()) |  (merged_data['DP.SV']=='nan'),'DP.SV'] = merged_data['DP.DS or DP.sV_y']\n",
    "print(merged_data['Revenue'].sum())\n",
    "merged_data['Affiliate_Id'] = merged_data['Affiliate_Id'].astype(str).str.split('.',expand = True)[0]\n",
    "merged_data['DP&Pub'] = merged_data['DP.SV']+'_'+ merged_data['Affiliate_Id']\n",
    "\n",
    "##add conversions\n",
    "merged_data = merged_data[['Date','Scheduling Time', 'Offer','Hitpath_Offer_ID','DP.SV','Affiliate_Id', 'DP&Pub','Job_Id', 'Job_Name','Creative_Id',\n",
    "                           'Creativename','Creative','Send Strategy', 'Shortcode', 'Start_Tstamp','Segments', 'Revenue','Conversions','Jump Page Clicks', \n",
    "                           'Delivered', 'Optout', 'Clicks','Cost', 'Ecpm', 'Dataset', 'Code_Type','Revenue Source','Ar Day','Sub_Id','Campaign_Id',\n",
    "                           'Roi','Shortcode Name','Jump Page Version','Offer Wall Id','Offer Type','Flow_Abbr']]\n",
    "merged_data.loc[merged_data['Clicks'] == '','Clicks'] = np.nan\n",
    "merged_data['Clicks'] = merged_data['Clicks'].astype(float)\n",
    "merged_data.loc[merged_data['Shortcode'] == 51797, 'Shortcode Name'] = 'FLC'\n",
    "merged_data.loc[merged_data['Shortcode'] == 70610, 'Shortcode Name'] = 'CSS'\n",
    "merged_data.loc[merged_data['Shortcode'] == 44345, 'Shortcode Name'] = 'HZB'\n",
    "merged_data.loc[merged_data['Shortcode'] == 80837, 'Shortcode Name'] = 'MBC'\n",
    "merged_data.loc[merged_data['Shortcode'] == 31232, 'Shortcode Name'] = 'DSS'\n",
    "merged_data.loc[merged_data['Shortcode'] == 61659, 'Shortcode Name'] = 'SVT'\n",
    "merged_data.loc[merged_data['Shortcode'] == 79743, 'Shortcode Name'] = 'UAA'\n",
    "merged_data.loc[merged_data['Shortcode'] == 18333164159, 'Shortcode Name'] = 'PRC'\n",
    "merged_data.loc[merged_data['Shortcode'] == 18333641722, 'Shortcode Name'] = 'UAATF'\n",
    "merged_data.loc[merged_data['Shortcode'] == 13022952715, 'Shortcode Name'] = 'A4FLC'\n",
    "merged_data.loc[merged_data['Shortcode'] == 15612023538, 'Shortcode Name'] = 'A4FLC'\n",
    "merged_data.loc[merged_data['Shortcode'] == 18332686782, 'Shortcode Name'] = 'MFATF'\n",
    "#added\n",
    "merged_data.loc[merged_data['Shortcode'] == 18332685688, 'Shortcode Name'] = 'N3GTF'\n",
    "merged_data.loc[merged_data['Shortcode'] == 18338987652, 'Shortcode Name'] = 'FBHTF'\n",
    "merged_data.loc[merged_data['Shortcode'] == 18333275117, 'Shortcode Name'] = 'FRHTF'\n",
    "merged_data.loc[merged_data['Shortcode Name'] == 'FLC', 'Shortcode'] = '51797'\n",
    "merged_data.loc[merged_data['Shortcode Name'] == 'CSS', 'Shortcode'] = '70610'\n",
    "merged_data.loc[merged_data['Shortcode Name'] == 'HZB', 'Shortcode'] = '44345'\n",
    "merged_data.loc[merged_data['Shortcode Name'] == 'MBC', 'Shortcode'] = '80837'\n",
    "merged_data.loc[merged_data['Shortcode Name'] == 'DSS', 'Shortcode'] = '31232'\n",
    "merged_data.loc[merged_data['Shortcode Name'] == 'SVT', 'Shortcode'] = '61659'\n",
    "merged_data.loc[merged_data['Shortcode Name'] == 'UAA', 'Shortcode'] = '79743'\n",
    "merged_data.loc[merged_data['Shortcode Name'] == 'PRC', 'Shortcode'] = '18333164159'\n",
    "merged_data.loc[merged_data['Shortcode Name'] == 'UAATF', 'Shortcode'] = '18333641722'\n",
    "merged_data.loc[merged_data['Shortcode Name'] == 'A4FLC', 'Shortcode'] = '13022952715'\n",
    "merged_data.loc[merged_data['Shortcode Name'] == 'MFATF', 'Shortcode'] = '18332686782'\n",
    "#added\n",
    "merged_data.loc[merged_data['Shortcode Name'] == 'N3GTF', 'Shortcode'] = '18332685688'\n",
    "merged_data.loc[merged_data['Shortcode Name'] == 'FBHTF', 'Shortcode'] = '18338987652'\n",
    "merged_data.loc[merged_data['Shortcode Name'] == 'FRHTF', 'Shortcode'] = '18333275117'\n",
    "merged_data.loc[merged_data['Delivered'] == '','Delivered'] = np.nan\n",
    "merged_data['Delivered'] =  merged_data['Delivered'].astype(float)\n",
    "merged_data.loc[(merged_data['Shortcode Name'] == 'HZB'), 'Cost' ] = 0.007 * merged_data['Delivered']\n",
    "merged_data.loc[(merged_data['Shortcode Name'] == 'MBC') | (merged_data['Shortcode Name'] == 'FLC'),'Cost']  =  0.00563 *  merged_data['Delivered'] +  merged_data['Cost']\n",
    "merged_data.loc[(merged_data['Code_Type'] == 'Short Code')&((merged_data['Shortcode Name'] == 'UAA') | (merged_data['Shortcode Name'] == 'SVT') | (merged_data['Shortcode Name'] == 'DSS')), 'Cost']  =  0.00504 *  merged_data['Delivered'] \n",
    "# we need to add sending cost for MFATF and A4FLC\n",
    "Offer_name[ 'campaign_id'] = Offer_name[ 'campaign_id'].astype('str').str.split('.',expand = True)[0]\n",
    "merged_data = merged_data.merge(Offer_name,left_on = 'Hitpath_Offer_ID',right_on = 'campaign_id',how = 'left')\n",
    "merged_data['Job_Id']  = merged_data['Job_Id'].astype(str).str.split('.',expand = True)[0].str.strip()\n",
    "merged_data['Date'] = pd.to_datetime(merged_data['Date'],format ='mixed').dt.strftime(\"%Y-%m-%d\")\n",
    "merged_data['shortcode_DP.SV'] = merged_data['Shortcode Name'] + \"_\" + merged_data['DP.SV']\n",
    "merged_data.loc[merged_data['shortcode_DP.SV']=='_', 'shortcode_DP.SV'] = np.nan\n",
    "# calculate opportunity cost\n",
    "merged_data['Opportunity Cost Send Strategy'] =  True\n",
    "merged_data.loc[merged_data['Send Strategy'].isin(['Null','Opt In',np.nan]), 'Opportunity Cost Send Strategy'] = False\n",
    "merged_data = merged_data.sort_values('Date')\n",
    "temp1= merged_data.groupby(['shortcode_DP.SV','Opportunity Cost Send Strategy','Date']).agg({'Revenue':'sum','Delivered':'sum'}).reset_index()\n",
    "temp1[['rolling Revenue','rolling Delivered']] = temp1.groupby('shortcode_DP.SV').shift(1).rolling(30, min_periods=5)[['Revenue','Delivered']].sum().reset_index(drop=True)\n",
    "temp1['Dataset_Agg_30D_eCPM'] = temp1['rolling Revenue'] * 1000/ temp1['rolling Delivered']\n",
    "dataset_agg_eCPM =  temp1[['shortcode_DP.SV','Date','Opportunity Cost Send Strategy','Dataset_Agg_30D_eCPM']]\n",
    "merged_data = merged_data.merge(dataset_agg_eCPM, how = 'left')\n",
    "merged_data['Opportunity Cost'] = merged_data['Revenue'] - merged_data['Dataset_Agg_30D_eCPM'] * merged_data['Delivered'] /1000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba10e07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_df[\"Hitpath_Offer_ID\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98799b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "374429.14589800005\n"
     ]
    }
   ],
   "source": [
    "print(merged_data['Revenue'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b63366cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add offer & vertical gap\n",
    "oms = infrastructure.get_smartsheet('offers_sms')\n",
    "oms.rename(columns ={'Hitpath Offer ID':'Hitpath_Offer_ID'}, inplace = True)\n",
    "oms.rename(columns ={\"Vertical\": \"Offer Vertical\"}, inplace = True)\n",
    "offer_vertical=oms[['Hitpath_Offer_ID','Offer Vertical']]\n",
    "offer_vertical = offer_vertical.dropna(subset=['Hitpath_Offer_ID'])\n",
    "offer_vertical['Hitpath_Offer_ID'] = offer_vertical['Hitpath_Offer_ID'].astype(str).replace(r'\\.0$','', regex=True)\n",
    "merged_data= merged_data.merge(offer_vertical, how = 'left', on = \"Hitpath_Offer_ID\")\n",
    "merged_data['Date'] = pd.to_datetime(merged_data['Date'])\n",
    "merged_data.sort_values(by = ['shortcode_DP.SV','Hitpath_Offer_ID','Send Strategy','Date'], ascending = True, inplace = True)\n",
    "merged_data['Offer Gap'] = merged_data.groupby(['shortcode_DP.SV','Send Strategy','Hitpath_Offer_ID'])['Date'].diff()\n",
    "merged_data['Vertical Gap'] = merged_data.groupby(['shortcode_DP.SV','Send Strategy',\"Offer Vertical\"])['Date'].diff()\n",
    "merged_data['Vertical Gap'] = merged_data['Vertical Gap'].dt.days\n",
    "merged_data['Offer Gap'] = merged_data['Offer Gap'].dt.days\n",
    "merged_data.loc[merged_data['Send Strategy'] != 'P', 'Offer Gap'] = np.nan\n",
    "merged_data.loc[merged_data['Send Strategy'] != 'P', 'Vertical Gap'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1fdf3688",
   "metadata": {},
   "outputs": [],
   "source": [
    "#added ecpm ratio \n",
    "merged_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "merged_data['Dataset_Agg_30D_eCPM'] = merged_data['Dataset_Agg_30D_eCPM'].replace(0, np.nan)\n",
    "merged_data['eCPM ratio'] = merged_data['Ecpm']/merged_data['Dataset_Agg_30D_eCPM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05d6fe29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_4212\\3635888808.py:2: DtypeWarning: Columns (1,2,3,4,6,8,9,10,11,12,14,16,26,27,28,29,30,31,32,33,34,35,36,37,44,48,51,54,56,58,59,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  twomonthsago = pd.read_csv(localfolder + 'SS_LC_merged_data_twomonthsago.csv', index_col= False)\n"
     ]
    }
   ],
   "source": [
    "#Combine history data, sort date and export both SS_LC_merged_data and data from two months ago\n",
    "twomonthsago = pd.read_csv(localfolder + 'SS_LC_merged_data_twomonthsago.csv', index_col= False)\n",
    "twomonthsago['Hitpath_Offer_ID'] = twomonthsago['Hitpath_Offer_ID'].astype(str).str.replace('.0', '')\n",
    "combined_df = pd.concat([twomonthsago, merged_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa7fa164",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['Date'] = pd.to_datetime(combined_df['Date'])\n",
    "combined_df = combined_df.sort_values(by = 'Date',ascending = False)\n",
    "combined_df.to_csv(localfolder + 'SS_LC_merged_data.csv', index =False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67fffb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['Date'] = pd.to_datetime(combined_df['Date'])\n",
    "twomonthsago_new = combined_df[(combined_df['Date'].isna()) | (combined_df['Date'] <= two_months_ago)]\n",
    "twomonthsago_new.to_csv(localfolder + 'SS_LC_merged_data_twomonthsago.csv', index =False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a067a9",
   "metadata": {},
   "source": [
    "# P&L DATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "775dca2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_4212\\4062370451.py:1: DtypeWarning: Columns (1,2,3,4,6,8,9,10,11,12,14,16,26,27,28,29,30,31,32,33,34,35,36,37,44,48,51,54,56,58,59,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  merged_data = pd.read_csv(localfolder + 'SS_LC_merged_data.csv')\n"
     ]
    }
   ],
   "source": [
    "merged_data = pd.read_csv(localfolder + 'SS_LC_merged_data.csv')\n",
    "engagement_daily = merged_data.groupby(['Date','Affiliate_Id'])[['Delivered','Clicks','Jump Page Clicks','Optout','Cost']].sum()\n",
    "engagement_daily = engagement_daily[~np.all(engagement_daily==0,axis = 1)]\n",
    "engagement_daily = engagement_daily.reset_index()\n",
    "daily_revenue_gpby = daily_revenue_gpby.rename(columns = {'amount':'Revenue','affiliate_id': 'Affiliate_Id', 'date': 'Date'})\n",
    "#Pl_data = daily_revenue_gpby.merge(engagement_daily, left_on =['affiliate_id','date'] ,right_on = ['Affiliate_Id','Date'],copy = False, how = 'outer')\n",
    "daily_revenue_gpby['Date'] = pd.to_datetime(daily_revenue_gpby['Date'])\n",
    "engagement_daily['Date'] =  pd.to_datetime(engagement_daily['Date'])\n",
    "Pl_data = pd.concat([daily_revenue_gpby,engagement_daily], axis=0, ignore_index=True)\n",
    "Pl_data['PUBID'] =Pl_data['Affiliate_Id'].astype('str').str.split(\".\",expand = True)[0]\n",
    "Pl_data = Pl_data.fillna(0)\n",
    "Pl_data = Pl_data[[ 'Date', 'PUBID','Revenue', 'Delivered', 'Clicks',\\\n",
    "      'Jump Page Clicks', 'Optout','Cost']]\n",
    "Pl_data = Pl_data.groupby(['PUBID','Date']).sum().reset_index()\n",
    "#Pl_data.to_csv(localfolder+'p&l_data.csv',index =False)\n",
    "\n",
    "api_key = pd.read_csv(localfolder+'SMS_SC_SS_Apikey.csv')\n",
    "#publisher1 = publisher\n",
    "#publisher1['DP.DS or DP.sV'] = publisher1['DP.DS or DP.sV'].str.replace('WWM.YFA.2','WWM.YFA')\n",
    "#publisher1['DP.DS or DP.sV'] = publisher1['DP.DS or DP.sV'].str.replace('ZM.PL.2','ZM.PL')\n",
    "api_key_sms = api_key.merge(new_publisher, left_on = 'DP.SV',right_on ='DP.DS or DP.sV',how = 'inner' )\n",
    "api_key_sms = api_key_sms[['DP.SV','PUBID','Date', 'AcceptedTotal', 'AcceptedNew',\\\n",
    "       'AcceptedDuplicate', 'RejectedTotal', 'RejectedMobile',\\\n",
    "       'RejectedBlacklist', 'RejectedData', 'CostData', 'CostChecks',\\\n",
    "       'CostTotal']]\n",
    "api_key_sms['Date'] = pd.to_datetime(api_key_sms['Date'])\n",
    "Pl_data['Date'] =  pd.to_datetime(Pl_data['Date'] )\n",
    "Pl_data['PUBID']  = Pl_data['PUBID'].astype('str').str.split(\".\",expand = True)[0]\n",
    "api_key_sms['PUBID']  = api_key_sms['PUBID'].astype('str').str.split(\".\",expand = True)[0]\n",
    "rev_accept_df = pd.concat([Pl_data, api_key_sms], axis=0, ignore_index=True)\n",
    "rev_accept_df = rev_accept_df.fillna(0)\n",
    "rev_accept_df['PUBID']  = rev_accept_df['PUBID'].astype('str').str.split(\".\",expand = True)[0]\n",
    "rev_accept_df = rev_accept_df.drop(columns = 'DP.SV')\n",
    "rev_accept_df = rev_accept_df.groupby(['PUBID','Date']).sum().reset_index()\n",
    "\n",
    "\n",
    "sms_post = pd.read_csv(localfolder + 'SMSposts.csv')\n",
    "sms_post['Date'] = pd.to_datetime(sms_post['Dates'])\n",
    "sms_post['PUBID'] = sms_post['Pub Id'].astype('str').str.split(\".\",expand = True)[0]\n",
    "sms_post = sms_post.drop(columns = ['Dates','Pub Id','Pub Name'])\n",
    "post_data = pd.concat([rev_accept_df,sms_post], axis=0, ignore_index=True)\n",
    "post_data = post_data.fillna(0)\n",
    "post_data = post_data.groupby(['PUBID','Date']).sum().reset_index()\n",
    "publisher_raw['PUBID'] = publisher_raw['PUBID'].astype('str').str.split(\".\",expand = True)[0]\n",
    "post_data = post_data.merge(publisher_raw[['PUBID','PUBLISHER' ,'DP.DS or DP.sV','DMA', 'INTERNAL STATUS',\n",
    "       'DATA TEAM STATUS', 'Sub Vertical', 'SCOPE', 'COMPANY (DP)','DP ID']], how = 'left', on = 'PUBID')\n",
    "post_data.to_csv(localfolder+'p&l_data.csv',index =False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96a344fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7141.0\n",
      "7141.0\n",
      "7141.000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_4212\\4158485992.py:31: DtypeWarning: Columns (1,2,3,4,6,8,9,10,11,12,14,16,26,27,28,29,30,31,32,33,34,35,36,37,44,48,51,54,56,58,59,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  merged_data = pd.read_csv(localfolder + 'SS_LC_merged_data.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7010.65\n"
     ]
    }
   ],
   "source": [
    "# offerwall reporting \n",
    "import infrastructure \n",
    "offer_sheet = infrastructure.get_smartsheet('offers_sms')\n",
    "offer_sheet = offer_sheet[offer_sheet['Hitpath Offer ID'].isna() == False]\n",
    "offer_sheet.rename(columns ={'Hitpath Offer ID':'Hitpath ID'}, inplace = True)\n",
    "offer_sheet['Hitpath ID'] = offer_sheet['Hitpath ID'].astype(str).str.split(\".\",expand = True)[0]\n",
    "offerwall = offerwall.rename(columns = {'offer_id':'Offer Wall ID'})\n",
    "print(offerwall[ 'amount'].sum())\n",
    "offerwall_update = offerwall.merge(offer_sheet[['Hitpath ID','Scheduling Name']], copy = False, how = 'left', left_on = 'Offer Wall ID', right_on = 'Hitpath ID')\n",
    "print(offerwall_update[ 'amount'].sum())\n",
    "offerwall_update = offerwall_update.rename(columns = {'Scheduling Name':\"Offer Wall Scheduling Name\" })\n",
    "offerwall_update = offerwall_update.merge(offer_sheet[['Hitpath ID','Scheduling Name']], copy = False, how = 'left', left_on = 'campaign_id', right_on = 'Hitpath ID')\n",
    "offerwall_update.loc[offerwall_update['Scheduling Name'].isna(),'Scheduling Name' ] = offerwall_update['campaign_name']\n",
    "offerwall_update = offerwall_update.rename(columns = {'Scheduling Name':\"Child Offer Campaign Name\" })\n",
    "\n",
    "offerwall_update[['Dash_Date_from_subid','affiliate_id','Offer Type','campaign_id',\"Child Offer Campaign Name\",'sub_id','Offer Wall ID',\"Offer Wall Scheduling Name\" ]] = offerwall_update[['Dash_Date_from_subid','affiliate_id','Offer Type','campaign_id',\"Child Offer Campaign Name\",'sub_id','Offer Wall ID',\"Offer Wall Scheduling Name\" ]].fillna(\"NULL\")\n",
    "offerwall_rev = offerwall_update.groupby(['Dash_Date_from_subid','affiliate_id','Offer Type','campaign_id',\"Child Offer Campaign Name\",'sub_id','Offer Wall ID',\"Offer Wall Scheduling Name\" ]).agg({\n",
    "            'amount': ['sum', 'count'],  # Summing and counting for the 'amount' column\n",
    "            'Jump Page Clicks': 'sum'     # Summing for the 'Jump Page Clicks' column\n",
    "        }).reset_index()\n",
    " \n",
    "offerwall_rev.columns = [\n",
    "'Dash_Date_from_subid','affiliate_id','Offer Type','campaign_id',\"Child Offer Campaign Name\",'sub_id','Offer Wall ID',\"Offer Wall Scheduling Name\",'amount','conversions','Jump Page Clicks'\n",
    "]\n",
    "print(offerwall_rev[ 'amount'].sum())\n",
    "offerwall_rev['split_column'] = offerwall_rev['sub_id'].str.split('_')\n",
    "offerwall_rev['Send Strategy'] = 'P'\n",
    "offerwall_rev.loc[offerwall_rev['split_column'].str[5].str.contains(\"AR\",na = False),'Send Strategy'] = 'AR'\n",
    "offerwall_rev.loc[offerwall_rev['split_column'].str[5].str.contains(\"CT\",na = False),'Send Strategy'] = 'CT'\n",
    "offerwall_rev = offerwall_rev.rename(columns = {'amount':'Child Offer Revenue','Jump Page Clicks':'Child Offer Jump Page Clicks', 'campaign_id':'Child Offer ID'})\n",
    "merged_data = pd.read_csv(localfolder + 'SS_LC_merged_data.csv')\n",
    "offerwall_engage_merge = merged_data[(merged_data['Offer Type'] == 'Offer Wall')]\n",
    "offerwall_engage_merge = offerwall_engage_merge.add_suffix('- Offer Wall')\n",
    "# The delivered volume is biased, if we don't have conversions & jump page clicks for a child offer, then it won't have the delivered and engagement stat.  \n",
    "offerwall_engage_subid = offerwall_engage_merge.groupby(['Sub_Id- Offer Wall'])[['Delivered- Offer Wall','Clicks- Offer Wall','Optout- Offer Wall','Cost- Offer Wall','Revenue- Offer Wall','Jump Page Clicks- Offer Wall']].sum().reset_index()\n",
    "offerwall_engage =  offerwall_engage_merge.groupby(['Date- Offer Wall','Affiliate_Id- Offer Wall','Hitpath_Offer_ID- Offer Wall','Send Strategy- Offer Wall'])[['Delivered- Offer Wall','Clicks- Offer Wall','Optout- Offer Wall','Cost- Offer Wall','Revenue- Offer Wall','Jump Page Clicks- Offer Wall']].sum().reset_index()\n",
    "offerwall_rev['affiliate_id'] = offerwall_rev['affiliate_id'].astype(str).str.split('.',expand = True)[0]\n",
    "offerwall_rev['Offer Wall ID'] = offerwall_rev['Offer Wall ID'].astype(str).str.split('.',expand = True)[0]\n",
    "offerwall_engage['Date- Offer Wall'] = pd.to_datetime(offerwall_engage['Date- Offer Wall'])\n",
    "offerwall_rev['Dash_Date_from_subid'] = pd.to_datetime(offerwall_rev['Dash_Date_from_subid'])\n",
    "\"\"\"\n",
    "# offer wall list \n",
    "temp = ofwall_df[['Hitpath Offer ID','Offer List' ]]\n",
    "temp['Offer List'] = temp['Offer List'].str.replace(\"[\",\"\").str.replace(\"]\",\"\").str.split(\",\")\n",
    "offerwall_match = temp.explode('Offer List')\n",
    "offerwall_match['Offer List'] = offerwall_match['Offer List'].str.strip()\n",
    "offerwall_engage_subid = offerwall_engage.merge(offerwall_match, left_on = 'Hitpath_Offer_ID- Offer Wall', right_on = 'Hitpath Offer ID', how = 'left')\n",
    "\"\"\" \n",
    "# merge rev, engagement data together\n",
    "offerwall_performance = offerwall_rev.merge(offerwall_engage_subid, left_on = ['sub_id'], right_on =['Sub_Id- Offer Wall'], how = 'left' )\n",
    "offerwall_performance_1 = offerwall_performance.loc[offerwall_performance['Sub_Id- Offer Wall'].isna() == False]\n",
    "offerwall_performance_2 = offerwall_performance.loc[offerwall_performance['Sub_Id- Offer Wall'].isna()== True].drop(columns = ['Sub_Id- Offer Wall','Delivered- Offer Wall','Clicks- Offer Wall','Optout- Offer Wall','Cost- Offer Wall','Revenue- Offer Wall','Jump Page Clicks- Offer Wall'])\n",
    "offerwall_performance_2 = offerwall_performance_2.groupby(['Dash_Date_from_subid','affiliate_id','Offer Wall ID','Send Strategy','Child Offer ID'])[[ 'Child Offer Revenue', 'Child Offer Jump Page Clicks']].sum().reset_index()\n",
    "offerwall_engage['Hitpath_Offer_ID- Offer Wall'] = offerwall_engage['Hitpath_Offer_ID- Offer Wall'].astype(str).str.split(\".\",expand = True)[0]\n",
    "offerwall_engage['Affiliate_Id- Offer Wall'] = offerwall_engage['Affiliate_Id- Offer Wall'].astype(str).str.split(\".\",expand = True)[0]\n",
    "offerwall_performance_2 = offerwall_performance_2.merge(offerwall_engage, left_on = ['Dash_Date_from_subid','affiliate_id','Offer Wall ID','Send Strategy'], right_on = ['Date- Offer Wall','Affiliate_Id- Offer Wall','Hitpath_Offer_ID- Offer Wall','Send Strategy- Offer Wall'], how ='left')\n",
    "offerwall_report = pd.concat([offerwall_performance_1,offerwall_performance_2])\n",
    "offerwall_report = offerwall_performance_2.merge(ofwall_df, left_on = 'Offer Wall ID', right_on = 'Offer Wall ID', how = 'left')\n",
    "print(offerwall_report['Child Offer Revenue'].sum())\n",
    "offerwall_report.to_csv(localfolder + \"offerwall_data.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda1cd2b",
   "metadata": {},
   "source": [
    "# Offerwall_rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "02887b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7141.0\n",
      "7141.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_4212\\218841887.py:38: DtypeWarning: Columns (1,2,3,4,6,8,9,10,11,12,14,16,26,27,28,29,30,31,32,33,34,35,36,37,44,48,51,54,56,58,59,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  merged_data_ow = pd.read_csv(localfolder + 'SS_LC_merged_data.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Offer Wall Revenue 33539.575\n",
      "Offer Wall Delivered 2330234.0\n",
      "Offer Wall Revenue 6065.8\n",
      "Delivery 1760540.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_4212\\218841887.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_data_ow_only.drop(columns = ['Revenue','Jump Page Clicks'], inplace = True)\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_4212\\218841887.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp['Offer List'] = temp['Offer List'].str.replace(\"[\",\"\").str.replace(\"]\",\"\").str.split(\",\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6065.799999999999\n",
      "1760540.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n# Group by 'Item' and calculate subtotal to subtract for each item\\nsubtotals_to_subtract = new_ow_df[new_ow_df[ 'Child Offer ID'] != 'Total'].groupby(['Date','Hitpath_Offer_ID','Send Strategy','Affiliate_Id'])[['Revenue','Jump Page Clicks']].sum().reset_index()\\n\\n# Merge back on the main DataFrame to get the subtraction value per 'Item'\\ndf = pd.merge(new_ow_df, subtotals_to_subtract, on=['Date','Hitpath_Offer_ID','Send Strategy','Affiliate_Id'], how='left', suffixes=('', '_to_subtract'))\\n\\n# Subtract the subtotals from the Totals\\nnew_ow_df.loc[new_ow_df[ 'Child Offer ID'] == 'Total', 'Revenue'] -= df['Revenue_to_subtract']\\n#new_ow_df.loc[new_ow_df[ 'Child Offer ID'] == 'Total', 'Conversions'] -= df['Conversions_to_subtract']\\nnew_ow_df.loc[new_ow_df[ 'Child Offer ID'] == 'Total', 'Jump Page Clicks'] -= df['Jump Page Clicks_to_subtract']\\n# Drop the temporary columns\\ndf.drop(columns=['Revenue_to_subtract','Conversions_to_subtract','Jump Page Clicks_to_subtract'], inplace=True)\\n\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Second Approach to make offer wall report \n",
    "################\n",
    "\n",
    "offer_sheet = infrastructure.get_smartsheet('offers_sms')\n",
    "offer_sheet = offer_sheet[offer_sheet['Hitpath Offer ID'].isna() == False]\n",
    "offer_sheet['Hitpath Offer ID'] = offer_sheet['Hitpath Offer ID'].astype(str).str.split(\".\",expand = True)[0]\n",
    "offerwall = offerwall.rename(columns = {'offer_id':'Offer Wall ID'})\n",
    "offerwall.loc[offerwall['campaign_id']=='nan','campaign_id'] = offerwall['Offer Wall ID']\n",
    "offerwall.loc[(offerwall['Offer Type']!='Offer Wall') & (offerwall['Offer Wall ID']!= offerwall['campaign_id']) ,'Offer Type'] = 'JP Extra Rev'\n",
    "\n",
    "\n",
    "offerwall_update = offerwall.merge(offer_sheet[['Hitpath Offer ID','Scheduling Name']], copy = False, how = 'left', left_on = 'Offer Wall ID', right_on = 'Hitpath Offer ID')\n",
    "print(offerwall_update[ 'amount'].sum())\n",
    "offerwall_update = offerwall_update.rename(columns = {'Scheduling Name':\"Offer Wall Scheduling Name\" })\n",
    "#offerwall_update = offerwall_update.merge(offer_sheet[['Hitpath Offer ID','Scheduling Name']], copy = False, how = 'left', left_on = 'campaign_id', right_on = 'Hitpath Offer ID')\n",
    "#offerwall_update.loc[offerwall_update['Scheduling Name'].isna(),'Scheduling Name' ] = offerwall_update['campaign_name']\n",
    "#offerwall_update = offerwall_update.rename(columns = {'Scheduling Name':\"Child Offer Campaign Name\" })\n",
    "\n",
    "offerwall_update[['Dash_Date_from_subid','affiliate_id','Offer Type','campaign_id','sub_id','Offer Wall ID',\"Offer Wall Scheduling Name\" ]] = offerwall_update[['Dash_Date_from_subid','affiliate_id','Offer Type','campaign_id','sub_id','Offer Wall ID',\"Offer Wall Scheduling Name\" ]].fillna(\"NULL\")\n",
    "offerwall_rev = offerwall_update.groupby(['Dash_Date_from_subid','affiliate_id','Offer Type','campaign_id','sub_id','Offer Wall ID',\"Offer Wall Scheduling Name\" ]).agg({\n",
    "            'amount': ['sum', 'count'],  # Summing and counting for the 'amount' column\n",
    "            'Jump Page Clicks': 'sum'     # Summing for the 'Jump Page Clicks' column\n",
    "        }).reset_index()\n",
    " \n",
    "offerwall_rev.columns = [\n",
    "'Dash_Date_from_subid','affiliate_id','Offer Type','campaign_id','sub_id','Offer Wall ID',\"Offer Wall Scheduling Name\",'amount','conversions','Jump Page Clicks'\n",
    "]\n",
    "print(offerwall_rev[ 'amount'].sum())\n",
    "offerwall_rev['split_column'] = offerwall_rev['sub_id'].str.split('_')\n",
    "offerwall_rev['Send Strategy'] = 'P'\n",
    "offerwall_rev.loc[offerwall_rev['split_column'].str[5].str.contains(\"AR\",na = False),'Send Strategy'] = 'AR'\n",
    "offerwall_rev.loc[offerwall_rev['split_column'].str[5].str.contains(\"CT\",na = False),'Send Strategy'] = 'CT'\n",
    "offerwall_rev = offerwall_rev.rename(columns = {'amount':'Child Offer Revenue','Jump Page Clicks':'Child Offer Jump Page Clicks', 'campaign_id':'Child Offer ID'})\n",
    "\n",
    "#################\n",
    "merged_data_ow = pd.read_csv(localfolder + 'SS_LC_merged_data.csv')\n",
    "merged_data_ow['data type'] = 'Total'\n",
    "# offer type = offer wall \n",
    "merged_data_ow_only = merged_data_ow.loc[merged_data_ow['Offer Type'] == 'Offer Wall', ]\n",
    "print(\"Offer Wall Revenue\",str(merged_data_ow_only['Revenue'].sum()))\n",
    "print(\"Offer Wall Delivered\",str(merged_data_ow_only['Delivered'].sum()))\n",
    "print(\"Offer Wall Revenue\", str(offerwall_rev[offerwall_rev['Offer Type'] == 'Offer Wall']['Child Offer Revenue'].sum()))\n",
    "merged_data_ow_only.drop(columns = ['Revenue','Jump Page Clicks'], inplace = True)\n",
    "#all_ow_drops = merged_data_ow_only.groupby(['Date','Affiliate_Id','Hitpath_Offer_ID','Send Strategy']).count().reset_index()[['Date','Affiliate_Id','Hitpath_Offer_ID','Send Strategy']]\n",
    "# offer wall list \n",
    "temp = ofwall_df[['Offer Wall ID','Offer List' ]]\n",
    "temp['Offer List'] = temp['Offer List'].str.replace(\"[\",\"\").str.replace(\"]\",\"\").str.split(\",\")\n",
    "offerwall_match = temp.explode('Offer List')\n",
    "offerwall_match['Offer List'] = offerwall_match['Offer List'].str.strip()\n",
    "offerwall_match.columns = ['Hitpath Offer ID','Child Offer ID']\n",
    "offerwall_match['Child Offer Count'] = offerwall_match.groupby('Hitpath Offer ID')['Child Offer ID'].transform('count')\n",
    "# divide mertics by child offer count\n",
    "merged_data_ow_only = merged_data_ow_only.merge(offerwall_match,left_on = 'Hitpath_Offer_ID', right_on = 'Hitpath Offer ID', how = 'left')\n",
    "merged_data_ow_only['Adjusted Delivered'] = merged_data_ow_only['Delivered'] / merged_data_ow_only['Child Offer Count']\n",
    "merged_data_ow_only['Adjusted Optout'] = merged_data_ow_only['Optout'] / merged_data_ow_only['Child Offer Count']\n",
    "merged_data_ow_only['Adjusted Clicks'] = merged_data_ow_only['Clicks'] / merged_data_ow_only['Child Offer Count']\n",
    "merged_data_ow_only['Adjusted Cost'] = merged_data_ow_only['Cost'] / merged_data_ow_only['Child Offer Count']\n",
    "print('Delivery',merged_data_ow_only['Adjusted Delivered'].sum())\n",
    "# clean offerwall_rev\n",
    "offerwall_rev.drop(columns =[ 'split_column','Offer Wall Scheduling Name','conversions'], inplace = True)\n",
    "offerwall_rev.columns = ['Date','Affiliate_Id','Offer Type','Child Offer ID','Sub_Id','Hitpath_Offer_ID','Revenue','Jump Page Clicks','Send Strategy']\n",
    "# change data type\n",
    "merged_data_ow_only['Date'] = pd.to_datetime(merged_data_ow_only['Date'])\n",
    "offerwall_rev['Date'] = pd.to_datetime(offerwall_rev['Date'])\n",
    "merged_data_ow_only['Affiliate_Id'] = merged_data_ow_only['Affiliate_Id'].astype(str).str.split('.',expand = True)[0]\n",
    "offerwall_rev['Affiliate_Id'] = offerwall_rev['Affiliate_Id'].astype(str).str.split('.',expand = True)[0]\n",
    "offerwall_rev_only = offerwall_rev[offerwall_rev['Offer Type'] == 'Offer Wall'].drop(columns = ['Offer Type'])\n",
    "# merge subid then hitpath offer id\n",
    "#offerwall_engage_subid = merged_data_ow_only.groupby(['Sub_Id','Child Offer ID'])[['Delivered','Clicks','Optout','Cost','Adjusted Delivered','Adjusted Optout','Adjusted Clicks','Adjusted Cost']].sum().reset_index()\n",
    "offerwall_engage =  merged_data_ow_only.groupby(['Date','Affiliate_Id','Child Offer ID','Hitpath_Offer_ID','Send Strategy'])[['Delivered','Clicks','Optout','Cost','Adjusted Delivered','Adjusted Optout','Adjusted Clicks','Adjusted Cost']].sum().reset_index()\n",
    "#offerwall_performance = offerwall_engage_subid.merge(offerwall_rev_only, on =['Sub_Id','Child Offer ID'] ,how = 'outer',copy = False )\n",
    "#offerwall_performance_1 = offerwall_performance.loc[offerwall_performance['Sub_Id'].isna() == False]\n",
    "#offerwall_performance_2 = offerwall_performance.loc[offerwall_performance['Sub_Id'].isna()== True].drop(columns = ['Sub_Id'])\n",
    "#offerwall_performance_2 = offerwall_performance_2.groupby( ['Date','Affiliate_Id','Child Offer ID','Hitpath_Offer_ID','Send Strategy'])[[ 'Revenue', 'Jump Page Clicks']].sum().reset_index()\n",
    "#offerwall_performance_2 = offerwall_performance_2.merge(offerwall_engage,on = ['Date','Affiliate_Id','Child Offer ID','Hitpath_Offer_ID','Send Strategy'] , how ='left')\n",
    "#offerwall_report = pd.concat([offerwall_performance_1,offerwall_performance_2])\n",
    "offerwall_rev_only = offerwall_rev_only.groupby(['Date','Affiliate_Id','Child Offer ID','Hitpath_Offer_ID','Send Strategy'])[['Revenue','Jump Page Clicks']].sum().reset_index()\n",
    "new_ow_df = offerwall_engage.merge(offerwall_rev_only, on = ['Date','Affiliate_Id','Child Offer ID','Hitpath_Offer_ID','Send Strategy'], how = 'outer', copy = False)\n",
    "new_ow_df = new_ow_df.merge(offer_sheet[['Hitpath Offer ID','Scheduling Name']], copy = False, how = 'left', left_on = 'Child Offer ID', right_on = 'Hitpath Offer ID')\n",
    "new_ow_df.drop(columns = ['Hitpath Offer ID'], inplace = True)\n",
    "new_ow_df = new_ow_df.rename(columns = {'Scheduling Name':\"Child Offer Campaign Name\" })\n",
    "print(new_ow_df['Revenue'].sum())\n",
    "print(new_ow_df['Adjusted Delivered'].sum())\n",
    "merged_data_ow_out =  merged_data_ow.loc[merged_data_ow['Offer Type'] != 'Offer Wall', ]\n",
    "offerwall_rev_out = offerwall_rev.loc[offerwall_rev['Offer Type']!= 'Offer Wall',]\n",
    "merged_data_ow_out = pd.concat([merged_data_ow_out,offerwall_rev_out], axis = 0, ignore_index = True) \n",
    "new_ow_df['Offer Type'] = 'Offer Wall'\n",
    "all_combine_df = pd.concat([new_ow_df,merged_data_ow_out], axis = 0, ignore_index = True)\n",
    "all_combine_df['Hitpath_Offer_ID'] = all_combine_df['Hitpath_Offer_ID'].astype(str).str.split(\".\",expand = True)[0]\n",
    "all_combine_df['Affiliate_Id'] = all_combine_df['Affiliate_Id'].astype(str).str.split(\".\",expand = True)[0]\n",
    "all_combine_df.rename(columns = {'Hitpath_Offer_ID':'Hitpath Offer ID'}, inplace = True)\n",
    "all_combine_df['Date'] = pd.to_datetime(all_combine_df['Date'])\n",
    "all_combine_df['Revenue'] = all_combine_df['Revenue'].fillna(0)\n",
    "all_combine_df.to_csv(localfolder + \"offerwall_merged_all.csv\")\n",
    "#new_ow_df.to_csv(localfolder + \"offerwall_merged_all.csv\")\n",
    "\n",
    "#new_ow_df[(new_ow_df['Affiliate_Id']=='461842') & (new_ow_df['Date']=='2024-03-12') & (new_ow_df['Send Strategy']=='P')]\n",
    "\"\"\"\n",
    "# Group by 'Item' and calculate subtotal to subtract for each item\n",
    "subtotals_to_subtract = new_ow_df[new_ow_df[ 'Child Offer ID'] != 'Total'].groupby(['Date','Hitpath_Offer_ID','Send Strategy','Affiliate_Id'])[['Revenue','Jump Page Clicks']].sum().reset_index()\n",
    "\n",
    "# Merge back on the main DataFrame to get the subtraction value per 'Item'\n",
    "df = pd.merge(new_ow_df, subtotals_to_subtract, on=['Date','Hitpath_Offer_ID','Send Strategy','Affiliate_Id'], how='left', suffixes=('', '_to_subtract'))\n",
    "\n",
    "# Subtract the subtotals from the Totals\n",
    "new_ow_df.loc[new_ow_df[ 'Child Offer ID'] == 'Total', 'Revenue'] -= df['Revenue_to_subtract']\n",
    "#new_ow_df.loc[new_ow_df[ 'Child Offer ID'] == 'Total', 'Conversions'] -= df['Conversions_to_subtract']\n",
    "new_ow_df.loc[new_ow_df[ 'Child Offer ID'] == 'Total', 'Jump Page Clicks'] -= df['Jump Page Clicks_to_subtract']\n",
    "# Drop the temporary columns\n",
    "df.drop(columns=['Revenue_to_subtract','Conversions_to_subtract','Jump Page Clicks_to_subtract'], inplace=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "240a1398",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "connect: to ('smtp.gmail.com', 587) None\n",
      "reply: b'220 smtp.gmail.com ESMTP d2e1a72fcca58-70673ca69efsm7255092b3a.168 - gsmtp\\r\\n'\n",
      "reply: retcode (220); Msg: b'smtp.gmail.com ESMTP d2e1a72fcca58-70673ca69efsm7255092b3a.168 - gsmtp'\n",
      "connect: b'smtp.gmail.com ESMTP d2e1a72fcca58-70673ca69efsm7255092b3a.168 - gsmtp'\n",
      "send: 'ehlo [100.96.2.115]\\r\\n'\n",
      "reply: b'250-smtp.gmail.com at your service, [52.38.54.186]\\r\\n'\n",
      "reply: b'250-SIZE 35882577\\r\\n'\n",
      "reply: b'250-8BITMIME\\r\\n'\n",
      "reply: b'250-STARTTLS\\r\\n'\n",
      "reply: b'250-ENHANCEDSTATUSCODES\\r\\n'\n",
      "reply: b'250-PIPELINING\\r\\n'\n",
      "reply: b'250-CHUNKING\\r\\n'\n",
      "reply: b'250 SMTPUTF8\\r\\n'\n",
      "reply: retcode (250); Msg: b'smtp.gmail.com at your service, [52.38.54.186]\\nSIZE 35882577\\n8BITMIME\\nSTARTTLS\\nENHANCEDSTATUSCODES\\nPIPELINING\\nCHUNKING\\nSMTPUTF8'\n",
      "send: 'STARTTLS\\r\\n'\n",
      "reply: b'220 2.0.0 Ready to start TLS\\r\\n'\n",
      "reply: retcode (220); Msg: b'2.0.0 Ready to start TLS'\n",
      "send: 'ehlo [100.96.2.115]\\r\\n'\n",
      "reply: b'250-smtp.gmail.com at your service, [52.38.54.186]\\r\\n'\n",
      "reply: b'250-SIZE 35882577\\r\\n'\n",
      "reply: b'250-8BITMIME\\r\\n'\n",
      "reply: b'250-AUTH LOGIN PLAIN XOAUTH2 PLAIN-CLIENTTOKEN OAUTHBEARER XOAUTH\\r\\n'\n",
      "reply: b'250-ENHANCEDSTATUSCODES\\r\\n'\n",
      "reply: b'250-PIPELINING\\r\\n'\n",
      "reply: b'250-CHUNKING\\r\\n'\n",
      "reply: b'250 SMTPUTF8\\r\\n'\n",
      "reply: retcode (250); Msg: b'smtp.gmail.com at your service, [52.38.54.186]\\nSIZE 35882577\\n8BITMIME\\nAUTH LOGIN PLAIN XOAUTH2 PLAIN-CLIENTTOKEN OAUTHBEARER XOAUTH\\nENHANCEDSTATUSCODES\\nPIPELINING\\nCHUNKING\\nSMTPUTF8'\n",
      "send: 'AUTH PLAIN AHNjaGVkdWxlYWxhcnRzcnhtZ0BnbWFpbC5jb20AeXlqZXRlanRqaGpoenh4aw==\\r\\n'\n",
      "reply: b'235 2.7.0 Accepted\\r\\n'\n",
      "reply: retcode (235); Msg: b'2.7.0 Accepted'\n",
      "send: 'mail FROM:<schedulealartsrxmg@gmail.com> size=25263462\\r\\n'\n",
      "reply: b'250 2.1.0 OK d2e1a72fcca58-70673ca69efsm7255092b3a.168 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.1.0 OK d2e1a72fcca58-70673ca69efsm7255092b3a.168 - gsmtp'\n",
      "send: 'rcpt TO:<nathan@rxmg.com>\\r\\n'\n",
      "reply: b'250 2.1.5 OK d2e1a72fcca58-70673ca69efsm7255092b3a.168 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.1.5 OK d2e1a72fcca58-70673ca69efsm7255092b3a.168 - gsmtp'\n",
      "send: 'data\\r\\n'\n",
      "reply: b'354  Go ahead d2e1a72fcca58-70673ca69efsm7255092b3a.168 - gsmtp\\r\\n'\n",
      "reply: retcode (354); Msg: b'Go ahead d2e1a72fcca58-70673ca69efsm7255092b3a.168 - gsmtp'\n",
      "data: (354, b'Go ahead d2e1a72fcca58-70673ca69efsm7255092b3a.168 - gsmtp')\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "reply: b'250 2.0.0 OK  1719423098 d2e1a72fcca58-70673ca69efsm7255092b3a.168 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.0.0 OK  1719423098 d2e1a72fcca58-70673ca69efsm7255092b3a.168 - gsmtp'\n",
      "data: (250, b'2.0.0 OK  1719423098 d2e1a72fcca58-70673ca69efsm7255092b3a.168 - gsmtp')\n",
      "send: 'quit\\r\\n'\n",
      "reply: b'221 2.0.0 closing connection d2e1a72fcca58-70673ca69efsm7255092b3a.168 - gsmtp\\r\\n'\n",
      "reply: retcode (221); Msg: b'2.0.0 closing connection d2e1a72fcca58-70673ca69efsm7255092b3a.168 - gsmtp'\n",
      "connect: to ('smtp.gmail.com', 587) None\n",
      "reply: b'220 smtp.gmail.com ESMTP 41be03b00d2f7-716b4a7161asm9011260a12.46 - gsmtp\\r\\n'\n",
      "reply: retcode (220); Msg: b'smtp.gmail.com ESMTP 41be03b00d2f7-716b4a7161asm9011260a12.46 - gsmtp'\n",
      "connect: b'smtp.gmail.com ESMTP 41be03b00d2f7-716b4a7161asm9011260a12.46 - gsmtp'\n",
      "send: 'ehlo [100.96.2.115]\\r\\n'\n",
      "reply: b'250-smtp.gmail.com at your service, [52.38.54.186]\\r\\n'\n",
      "reply: b'250-SIZE 35882577\\r\\n'\n",
      "reply: b'250-8BITMIME\\r\\n'\n",
      "reply: b'250-STARTTLS\\r\\n'\n",
      "reply: b'250-ENHANCEDSTATUSCODES\\r\\n'\n",
      "reply: b'250-PIPELINING\\r\\n'\n",
      "reply: b'250-CHUNKING\\r\\n'\n",
      "reply: b'250 SMTPUTF8\\r\\n'\n",
      "reply: retcode (250); Msg: b'smtp.gmail.com at your service, [52.38.54.186]\\nSIZE 35882577\\n8BITMIME\\nSTARTTLS\\nENHANCEDSTATUSCODES\\nPIPELINING\\nCHUNKING\\nSMTPUTF8'\n",
      "send: 'STARTTLS\\r\\n'\n",
      "reply: b'220 2.0.0 Ready to start TLS\\r\\n'\n",
      "reply: retcode (220); Msg: b'2.0.0 Ready to start TLS'\n",
      "send: 'ehlo [100.96.2.115]\\r\\n'\n",
      "reply: b'250-smtp.gmail.com at your service, [52.38.54.186]\\r\\n'\n",
      "reply: b'250-SIZE 35882577\\r\\n'\n",
      "reply: b'250-8BITMIME\\r\\n'\n",
      "reply: b'250-AUTH LOGIN PLAIN XOAUTH2 PLAIN-CLIENTTOKEN OAUTHBEARER XOAUTH\\r\\n'\n",
      "reply: b'250-ENHANCEDSTATUSCODES\\r\\n'\n",
      "reply: b'250-PIPELINING\\r\\n'\n",
      "reply: b'250-CHUNKING\\r\\n'\n",
      "reply: b'250 SMTPUTF8\\r\\n'\n",
      "reply: retcode (250); Msg: b'smtp.gmail.com at your service, [52.38.54.186]\\nSIZE 35882577\\n8BITMIME\\nAUTH LOGIN PLAIN XOAUTH2 PLAIN-CLIENTTOKEN OAUTHBEARER XOAUTH\\nENHANCEDSTATUSCODES\\nPIPELINING\\nCHUNKING\\nSMTPUTF8'\n",
      "send: 'AUTH PLAIN AHNjaGVkdWxlYWxhcnRzcnhtZ0BnbWFpbC5jb20AeXlqZXRlanRqaGpoenh4aw==\\r\\n'\n",
      "reply: b'235 2.7.0 Accepted\\r\\n'\n",
      "reply: retcode (235); Msg: b'2.7.0 Accepted'\n",
      "send: 'mail FROM:<schedulealartsrxmg@gmail.com> size=25263460\\r\\n'\n",
      "reply: b'250 2.1.0 OK 41be03b00d2f7-716b4a7161asm9011260a12.46 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.1.0 OK 41be03b00d2f7-716b4a7161asm9011260a12.46 - gsmtp'\n",
      "send: 'rcpt TO:<lili@rxmg.com>\\r\\n'\n",
      "reply: b'250 2.1.5 OK 41be03b00d2f7-716b4a7161asm9011260a12.46 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.1.5 OK 41be03b00d2f7-716b4a7161asm9011260a12.46 - gsmtp'\n",
      "send: 'data\\r\\n'\n",
      "reply: b'354  Go ahead 41be03b00d2f7-716b4a7161asm9011260a12.46 - gsmtp\\r\\n'\n",
      "reply: retcode (354); Msg: b'Go ahead 41be03b00d2f7-716b4a7161asm9011260a12.46 - gsmtp'\n",
      "data: (354, b'Go ahead 41be03b00d2f7-716b4a7161asm9011260a12.46 - gsmtp')\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "reply: b'250 2.0.0 OK  1719423126 41be03b00d2f7-716b4a7161asm9011260a12.46 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.0.0 OK  1719423126 41be03b00d2f7-716b4a7161asm9011260a12.46 - gsmtp'\n",
      "data: (250, b'2.0.0 OK  1719423126 41be03b00d2f7-716b4a7161asm9011260a12.46 - gsmtp')\n",
      "send: 'quit\\r\\n'\n",
      "reply: b'221 2.0.0 closing connection 41be03b00d2f7-716b4a7161asm9011260a12.46 - gsmtp\\r\\n'\n",
      "reply: retcode (221); Msg: b'2.0.0 closing connection 41be03b00d2f7-716b4a7161asm9011260a12.46 - gsmtp'\n",
      "connect: to ('smtp.gmail.com', 587) None\n",
      "reply: b'220 smtp.gmail.com ESMTP 98e67ed59e1d1-2c8d7f5cc20sm1986158a91.17 - gsmtp\\r\\n'\n",
      "reply: retcode (220); Msg: b'smtp.gmail.com ESMTP 98e67ed59e1d1-2c8d7f5cc20sm1986158a91.17 - gsmtp'\n",
      "connect: b'smtp.gmail.com ESMTP 98e67ed59e1d1-2c8d7f5cc20sm1986158a91.17 - gsmtp'\n",
      "send: 'ehlo [100.96.2.115]\\r\\n'\n",
      "reply: b'250-smtp.gmail.com at your service, [52.38.54.186]\\r\\n'\n",
      "reply: b'250-SIZE 35882577\\r\\n'\n",
      "reply: b'250-8BITMIME\\r\\n'\n",
      "reply: b'250-STARTTLS\\r\\n'\n",
      "reply: b'250-ENHANCEDSTATUSCODES\\r\\n'\n",
      "reply: b'250-PIPELINING\\r\\n'\n",
      "reply: b'250-CHUNKING\\r\\n'\n",
      "reply: b'250 SMTPUTF8\\r\\n'\n",
      "reply: retcode (250); Msg: b'smtp.gmail.com at your service, [52.38.54.186]\\nSIZE 35882577\\n8BITMIME\\nSTARTTLS\\nENHANCEDSTATUSCODES\\nPIPELINING\\nCHUNKING\\nSMTPUTF8'\n",
      "send: 'STARTTLS\\r\\n'\n",
      "reply: b'220 2.0.0 Ready to start TLS\\r\\n'\n",
      "reply: retcode (220); Msg: b'2.0.0 Ready to start TLS'\n",
      "send: 'ehlo [100.96.2.115]\\r\\n'\n",
      "reply: b'250-smtp.gmail.com at your service, [52.38.54.186]\\r\\n'\n",
      "reply: b'250-SIZE 35882577\\r\\n'\n",
      "reply: b'250-8BITMIME\\r\\n'\n",
      "reply: b'250-AUTH LOGIN PLAIN XOAUTH2 PLAIN-CLIENTTOKEN OAUTHBEARER XOAUTH\\r\\n'\n",
      "reply: b'250-ENHANCEDSTATUSCODES\\r\\n'\n",
      "reply: b'250-PIPELINING\\r\\n'\n",
      "reply: b'250-CHUNKING\\r\\n'\n",
      "reply: b'250 SMTPUTF8\\r\\n'\n",
      "reply: retcode (250); Msg: b'smtp.gmail.com at your service, [52.38.54.186]\\nSIZE 35882577\\n8BITMIME\\nAUTH LOGIN PLAIN XOAUTH2 PLAIN-CLIENTTOKEN OAUTHBEARER XOAUTH\\nENHANCEDSTATUSCODES\\nPIPELINING\\nCHUNKING\\nSMTPUTF8'\n",
      "send: 'AUTH PLAIN AHNjaGVkdWxlYWxhcnRzcnhtZ0BnbWFpbC5jb20AeXlqZXRlanRqaGpoenh4aw==\\r\\n'\n",
      "reply: b'235 2.7.0 Accepted\\r\\n'\n",
      "reply: retcode (235); Msg: b'2.7.0 Accepted'\n",
      "send: 'mail FROM:<schedulealartsrxmg@gmail.com> size=25263460\\r\\n'\n",
      "reply: b'250 2.1.0 OK 98e67ed59e1d1-2c8d7f5cc20sm1986158a91.17 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.1.0 OK 98e67ed59e1d1-2c8d7f5cc20sm1986158a91.17 - gsmtp'\n",
      "send: 'rcpt TO:<nina@rxmg.com>\\r\\n'\n",
      "reply: b'250 2.1.5 OK 98e67ed59e1d1-2c8d7f5cc20sm1986158a91.17 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.1.5 OK 98e67ed59e1d1-2c8d7f5cc20sm1986158a91.17 - gsmtp'\n",
      "send: 'data\\r\\n'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reply: b'354  Go ahead 98e67ed59e1d1-2c8d7f5cc20sm1986158a91.17 - gsmtp\\r\\n'\n",
      "reply: retcode (354); Msg: b'Go ahead 98e67ed59e1d1-2c8d7f5cc20sm1986158a91.17 - gsmtp'\n",
      "data: (354, b'Go ahead 98e67ed59e1d1-2c8d7f5cc20sm1986158a91.17 - gsmtp')\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "reply: b'250 2.0.0 OK  1719423155 98e67ed59e1d1-2c8d7f5cc20sm1986158a91.17 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.0.0 OK  1719423155 98e67ed59e1d1-2c8d7f5cc20sm1986158a91.17 - gsmtp'\n",
      "data: (250, b'2.0.0 OK  1719423155 98e67ed59e1d1-2c8d7f5cc20sm1986158a91.17 - gsmtp')\n",
      "send: 'quit\\r\\n'\n",
      "reply: b'221 2.0.0 closing connection 98e67ed59e1d1-2c8d7f5cc20sm1986158a91.17 - gsmtp\\r\\n'\n",
      "reply: retcode (221); Msg: b'2.0.0 closing connection 98e67ed59e1d1-2c8d7f5cc20sm1986158a91.17 - gsmtp'\n",
      "connect: to ('smtp.gmail.com', 587) None\n",
      "reply: b'220 smtp.gmail.com ESMTP d2e1a72fcca58-70689f26039sm5450924b3a.200 - gsmtp\\r\\n'\n",
      "reply: retcode (220); Msg: b'smtp.gmail.com ESMTP d2e1a72fcca58-70689f26039sm5450924b3a.200 - gsmtp'\n",
      "connect: b'smtp.gmail.com ESMTP d2e1a72fcca58-70689f26039sm5450924b3a.200 - gsmtp'\n",
      "send: 'ehlo [100.96.2.115]\\r\\n'\n",
      "reply: b'250-smtp.gmail.com at your service, [52.38.54.186]\\r\\n'\n",
      "reply: b'250-SIZE 35882577\\r\\n'\n",
      "reply: b'250-8BITMIME\\r\\n'\n",
      "reply: b'250-STARTTLS\\r\\n'\n",
      "reply: b'250-ENHANCEDSTATUSCODES\\r\\n'\n",
      "reply: b'250-PIPELINING\\r\\n'\n",
      "reply: b'250-CHUNKING\\r\\n'\n",
      "reply: b'250 SMTPUTF8\\r\\n'\n",
      "reply: retcode (250); Msg: b'smtp.gmail.com at your service, [52.38.54.186]\\nSIZE 35882577\\n8BITMIME\\nSTARTTLS\\nENHANCEDSTATUSCODES\\nPIPELINING\\nCHUNKING\\nSMTPUTF8'\n",
      "send: 'STARTTLS\\r\\n'\n",
      "reply: b'220 2.0.0 Ready to start TLS\\r\\n'\n",
      "reply: retcode (220); Msg: b'2.0.0 Ready to start TLS'\n",
      "send: 'ehlo [100.96.2.115]\\r\\n'\n",
      "reply: b'250-smtp.gmail.com at your service, [52.38.54.186]\\r\\n'\n",
      "reply: b'250-SIZE 35882577\\r\\n'\n",
      "reply: b'250-8BITMIME\\r\\n'\n",
      "reply: b'250-AUTH LOGIN PLAIN XOAUTH2 PLAIN-CLIENTTOKEN OAUTHBEARER XOAUTH\\r\\n'\n",
      "reply: b'250-ENHANCEDSTATUSCODES\\r\\n'\n",
      "reply: b'250-PIPELINING\\r\\n'\n",
      "reply: b'250-CHUNKING\\r\\n'\n",
      "reply: b'250 SMTPUTF8\\r\\n'\n",
      "reply: retcode (250); Msg: b'smtp.gmail.com at your service, [52.38.54.186]\\nSIZE 35882577\\n8BITMIME\\nAUTH LOGIN PLAIN XOAUTH2 PLAIN-CLIENTTOKEN OAUTHBEARER XOAUTH\\nENHANCEDSTATUSCODES\\nPIPELINING\\nCHUNKING\\nSMTPUTF8'\n",
      "send: 'AUTH PLAIN AHNjaGVkdWxlYWxhcnRzcnhtZ0BnbWFpbC5jb20AeXlqZXRlanRqaGpoenh4aw==\\r\\n'\n",
      "reply: b'235 2.7.0 Accepted\\r\\n'\n",
      "reply: retcode (235); Msg: b'2.7.0 Accepted'\n",
      "send: 'mail FROM:<schedulealartsrxmg@gmail.com> size=25263466\\r\\n'\n",
      "reply: b'250 2.1.0 OK d2e1a72fcca58-70689f26039sm5450924b3a.200 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.1.0 OK d2e1a72fcca58-70689f26039sm5450924b3a.200 - gsmtp'\n",
      "send: 'rcpt TO:<b.ratzlaff@rxmg.com>\\r\\n'\n",
      "reply: b'250 2.1.5 OK d2e1a72fcca58-70689f26039sm5450924b3a.200 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.1.5 OK d2e1a72fcca58-70689f26039sm5450924b3a.200 - gsmtp'\n",
      "send: 'data\\r\\n'\n",
      "reply: b'354  Go ahead d2e1a72fcca58-70689f26039sm5450924b3a.200 - gsmtp\\r\\n'\n",
      "reply: retcode (354); Msg: b'Go ahead d2e1a72fcca58-70689f26039sm5450924b3a.200 - gsmtp'\n",
      "data: (354, b'Go ahead d2e1a72fcca58-70689f26039sm5450924b3a.200 - gsmtp')\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "reply: b'250 2.0.0 OK  1719423182 d2e1a72fcca58-70689f26039sm5450924b3a.200 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.0.0 OK  1719423182 d2e1a72fcca58-70689f26039sm5450924b3a.200 - gsmtp'\n",
      "data: (250, b'2.0.0 OK  1719423182 d2e1a72fcca58-70689f26039sm5450924b3a.200 - gsmtp')\n",
      "send: 'quit\\r\\n'\n",
      "reply: b'221 2.0.0 closing connection d2e1a72fcca58-70689f26039sm5450924b3a.200 - gsmtp\\r\\n'\n",
      "reply: retcode (221); Msg: b'2.0.0 closing connection d2e1a72fcca58-70689f26039sm5450924b3a.200 - gsmtp'\n",
      "connect: to ('smtp.gmail.com', 587) None\n",
      "reply: b'220 smtp.gmail.com ESMTP d9443c01a7336-1f9eb3d8bd8sm102293745ad.209 - gsmtp\\r\\n'\n",
      "reply: retcode (220); Msg: b'smtp.gmail.com ESMTP d9443c01a7336-1f9eb3d8bd8sm102293745ad.209 - gsmtp'\n",
      "connect: b'smtp.gmail.com ESMTP d9443c01a7336-1f9eb3d8bd8sm102293745ad.209 - gsmtp'\n",
      "send: 'ehlo [100.96.2.115]\\r\\n'\n",
      "reply: b'250-smtp.gmail.com at your service, [52.38.54.186]\\r\\n'\n",
      "reply: b'250-SIZE 35882577\\r\\n'\n",
      "reply: b'250-8BITMIME\\r\\n'\n",
      "reply: b'250-STARTTLS\\r\\n'\n",
      "reply: b'250-ENHANCEDSTATUSCODES\\r\\n'\n",
      "reply: b'250-PIPELINING\\r\\n'\n",
      "reply: b'250-CHUNKING\\r\\n'\n",
      "reply: b'250 SMTPUTF8\\r\\n'\n",
      "reply: retcode (250); Msg: b'smtp.gmail.com at your service, [52.38.54.186]\\nSIZE 35882577\\n8BITMIME\\nSTARTTLS\\nENHANCEDSTATUSCODES\\nPIPELINING\\nCHUNKING\\nSMTPUTF8'\n",
      "send: 'STARTTLS\\r\\n'\n",
      "reply: b'220 2.0.0 Ready to start TLS\\r\\n'\n",
      "reply: retcode (220); Msg: b'2.0.0 Ready to start TLS'\n",
      "send: 'ehlo [100.96.2.115]\\r\\n'\n",
      "reply: b'250-smtp.gmail.com at your service, [52.38.54.186]\\r\\n'\n",
      "reply: b'250-SIZE 35882577\\r\\n'\n",
      "reply: b'250-8BITMIME\\r\\n'\n",
      "reply: b'250-AUTH LOGIN PLAIN XOAUTH2 PLAIN-CLIENTTOKEN OAUTHBEARER XOAUTH\\r\\n'\n",
      "reply: b'250-ENHANCEDSTATUSCODES\\r\\n'\n",
      "reply: b'250-PIPELINING\\r\\n'\n",
      "reply: b'250-CHUNKING\\r\\n'\n",
      "reply: b'250 SMTPUTF8\\r\\n'\n",
      "reply: retcode (250); Msg: b'smtp.gmail.com at your service, [52.38.54.186]\\nSIZE 35882577\\n8BITMIME\\nAUTH LOGIN PLAIN XOAUTH2 PLAIN-CLIENTTOKEN OAUTHBEARER XOAUTH\\nENHANCEDSTATUSCODES\\nPIPELINING\\nCHUNKING\\nSMTPUTF8'\n",
      "send: 'AUTH PLAIN AHNjaGVkdWxlYWxhcnRzcnhtZ0BnbWFpbC5jb20AeXlqZXRlanRqaGpoenh4aw==\\r\\n'\n",
      "reply: b'235 2.7.0 Accepted\\r\\n'\n",
      "reply: retcode (235); Msg: b'2.7.0 Accepted'\n",
      "send: 'mail FROM:<schedulealartsrxmg@gmail.com> size=25263464\\r\\n'\n",
      "reply: b'250 2.1.0 OK d9443c01a7336-1f9eb3d8bd8sm102293745ad.209 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.1.0 OK d9443c01a7336-1f9eb3d8bd8sm102293745ad.209 - gsmtp'\n",
      "send: 'rcpt TO:<n.ohashi@rxmg.com>\\r\\n'\n",
      "reply: b'250 2.1.5 OK d9443c01a7336-1f9eb3d8bd8sm102293745ad.209 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.1.5 OK d9443c01a7336-1f9eb3d8bd8sm102293745ad.209 - gsmtp'\n",
      "send: 'data\\r\\n'\n",
      "reply: b'354  Go ahead d9443c01a7336-1f9eb3d8bd8sm102293745ad.209 - gsmtp\\r\\n'\n",
      "reply: retcode (354); Msg: b'Go ahead d9443c01a7336-1f9eb3d8bd8sm102293745ad.209 - gsmtp'\n",
      "data: (354, b'Go ahead d9443c01a7336-1f9eb3d8bd8sm102293745ad.209 - gsmtp')\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "reply: b'250 2.0.0 OK  1719423207 d9443c01a7336-1f9eb3d8bd8sm102293745ad.209 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.0.0 OK  1719423207 d9443c01a7336-1f9eb3d8bd8sm102293745ad.209 - gsmtp'\n",
      "data: (250, b'2.0.0 OK  1719423207 d9443c01a7336-1f9eb3d8bd8sm102293745ad.209 - gsmtp')\n",
      "send: 'quit\\r\\n'\n",
      "reply: b'221 2.0.0 closing connection d9443c01a7336-1f9eb3d8bd8sm102293745ad.209 - gsmtp\\r\\n'\n",
      "reply: retcode (221); Msg: b'2.0.0 closing connection d9443c01a7336-1f9eb3d8bd8sm102293745ad.209 - gsmtp'\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "os.chdir(localfolder) \n",
    "filename = 'daily_data_update.zip'\n",
    "email_body = ''\n",
    "with zipfile.ZipFile('daily_data_update.zip', 'w',zipfile.ZIP_DEFLATED) as zipf:\n",
    "    zipf.write('SS_LC_merged_data.csv')\n",
    "toaddr = ['nathan@rxmg.com','lili@rxmg.com','nina@rxmg.com','b.ratzlaff@rxmg.com','n.ohashi@rxmg.com']\n",
    "#toaddr = ['n.ohashi@rxmg.com','nina@rxmg.com']\n",
    "#toaddr = ['lili@rxmg.com']\n",
    "subject_line = 'SMS Daily Data Update' \n",
    "email_body \n",
    "for i in toaddr:\n",
    "    send_email.send_email([filename],subject_line,email_body,i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c79da4",
   "metadata": {},
   "source": [
    "## Verification Data Quality "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "167f8cae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Shortcode Name</th>\n",
       "      <th>Delivered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DSS</td>\n",
       "      <td>183102.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FLC</td>\n",
       "      <td>2881011.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HZB</td>\n",
       "      <td>1658231.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MBC</td>\n",
       "      <td>3672159.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVT</td>\n",
       "      <td>897460.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>UAA</td>\n",
       "      <td>711621.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>UAATF</td>\n",
       "      <td>33008.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>W1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Shortcode Name  Delivered\n",
       "0            DSS   183102.0\n",
       "1            FLC  2881011.0\n",
       "2            HZB  1658231.0\n",
       "3            MBC  3672159.0\n",
       "4            SVT   897460.0\n",
       "5            UAA   711621.0\n",
       "6          UAATF    33008.0\n",
       "7             W1        0.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check delivered volumn by shortcode name \n",
    "merged_data['Date'] = pd.to_datetime(merged_data['Date'])\n",
    "merged_data['Month'] = merged_data['Date'].dt.strftime('%Y-%m')\n",
    "merged_data[ merged_data['Month'] == '2023-12'].groupby(['Shortcode Name'])[['Delivered']].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c40e46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The revenue discrepency after November between raw file and Tableau data is $ -1120.35\n",
      "The total revenue after November is $ 373751.86\n",
      "The revenue discrepency is -0.0 % of the total revenue after November\n"
     ]
    }
   ],
   "source": [
    "# Check & Examination: \n",
    "# Revenue files sum up after November \n",
    "rev_file = combined_csv[combined_csv['date']>='2022-11-01'].reset_index(drop=True)['amount'].sum()\n",
    "# Revenue we made after November\n",
    "#after_merge = combined_csv_ss_creative_na['Revenue'].sum() + combined_csv_ss_creative_notna['Revenue'].sum() + combined_csv_ss_exclude_11['Revenue'].sum() +flows_clean2['Revenue'].sum() + combined_csv_ss_last_11['amount'].sum() + lc_df_full_11['amount'].sum() + combined_csv_push_11['Revenue'].sum() + combined_csv_w1_11['Revenue'].sum()\n",
    "after_merge = combined_csv_ss_creative_na['Revenue'].sum() + combined_csv_ss_creative_notna['Revenue'].sum() + combined_csv_ss_exclude_11['Revenue'].sum() +flows_clean2['Revenue'].sum() + combined_csv_ss_last_11['amount'].sum() + combined_csv_push_11['Revenue'].sum() + combined_csv_w1_11['Revenue'].sum()\n",
    "\n",
    "email_body = \"The revenue discrepency after November between raw file and Tableau data is $\"+str(round((rev_file - after_merge),2)) + \"\\n\" \n",
    "email_body += \"The total revenue after November is $\" + str(round(rev_file,2)) + \"\\n\" \n",
    "email_body += \"The revenue discrepency is \"+ str( round((rev_file - after_merge) /rev_file,2)*100 ) + \"% of the total revenue after November\" + \"\\n\" + \"The revenue we made after November is $\" + str(round(after_merge,2))\n",
    "print(\"The revenue discrepency after November between raw file and Tableau data is $\",round((rev_file - after_merge),2))\n",
    "print(\"The total revenue after November is $\", round(rev_file,2))\n",
    "print(\"The revenue discrepency is\", round((rev_file - after_merge) /rev_file,2)*100, \"% of the total revenue after November\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "485b968a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Month\n",
       "1899-12         0.000000\n",
       "1999-12       652.800000\n",
       "2019-12         0.000000\n",
       "2020-01         0.000000\n",
       "2020-02       163.100000\n",
       "2020-03      1116.300000\n",
       "2020-04      5060.200000\n",
       "2020-05      2218.760000\n",
       "2020-06      3551.800000\n",
       "2020-07      6578.100000\n",
       "2020-08     41870.600000\n",
       "2020-09     52336.370000\n",
       "2020-10     50779.900000\n",
       "2020-11     37785.730000\n",
       "2020-12     32358.100000\n",
       "2021-01     23948.040000\n",
       "2021-02     20137.840000\n",
       "2021-03     26584.280000\n",
       "2021-04     50262.800000\n",
       "2021-05     74756.520000\n",
       "2021-06     91146.100000\n",
       "2021-07    144141.810000\n",
       "2021-08    158429.500000\n",
       "2021-09    154983.178800\n",
       "2021-10    201033.211100\n",
       "2021-11    225191.850000\n",
       "2021-12    266562.230000\n",
       "2022-01    276934.304528\n",
       "2022-02    268022.813635\n",
       "2022-03    249501.265543\n",
       "2022-04    296194.537417\n",
       "2022-05    277401.288745\n",
       "2022-06    404793.833177\n",
       "2022-07    247403.937505\n",
       "2022-08    155207.055008\n",
       "2022-09    161623.944255\n",
       "2022-10     39895.442484\n",
       "2022-11     87340.753168\n",
       "2022-12    112803.439432\n",
       "2023-01    126387.636178\n",
       "2023-02    106185.909126\n",
       "2023-03    132726.926130\n",
       "2023-04    138092.207699\n",
       "2023-05    138327.489997\n",
       "2023-06    133089.996674\n",
       "2023-07    157398.963897\n",
       "2023-08    176794.764455\n",
       "2023-09    174226.860641\n",
       "2023-10    204304.325832\n",
       "2023-11    194982.323519\n",
       "2023-12    203068.883677\n",
       "2024-01    194690.306698\n",
       "2024-02    165550.460576\n",
       "2024-03    170519.217965\n",
       "2024-04    166596.294373\n",
       "2024-05    185397.136319\n",
       "2024-06    169737.264578\n",
       "NaT          5482.642000\n",
       "Name: Revenue, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#monthly revenue\n",
    "merged_data['Month'] = merged_data['Date'].astype(str).str[:7]\n",
    "temp = merged_data.groupby('Month')['Revenue'].sum()\n",
    "email_body +=  \"\\n\\nThe revenue by month is\\n\" \n",
    "email_body += str(temp)  + '\\n'\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2c51e893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "month\n",
       "2024-04     10.32\n",
       "2024-05    301.96\n",
       "2024-06    130.79\n",
       "Name: amount, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_csv_ss_last_11['month'] = combined_csv_ss_last_11['date'].astype(str).str[:7]\n",
    "\n",
    "email_body += \"\\n\" + \"The revenue by month for unclaimed revenue is \" + str(combined_csv_ss_last_11.groupby('month')['amount'].sum()) +\"\\n\"\n",
    "combined_csv_ss_last_11.groupby('month')['amount'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "995d5f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date\n",
       "1899-12-31       0.00000\n",
       "1999-12-31     652.80000\n",
       "2019-12-25       0.00000\n",
       "2019-12-26       0.00000\n",
       "2020-01-24       0.00000\n",
       "                 ...    \n",
       "2024-06-22    5839.34889\n",
       "2024-06-23    6438.76424\n",
       "2024-06-24    8648.00650\n",
       "2024-06-25    6213.65700\n",
       "2024-06-26     267.84000\n",
       "Name: Revenue, Length: 1590, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = merged_data.groupby('Date')['Revenue'].sum()\n",
    "email_body +=  \"\\n\\nThe revenue by Date is\\n\" \n",
    "email_body += str(temp)  + '\\n' \n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662f4b58",
   "metadata": {},
   "source": [
    "# Send out email to the Team "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eac9a6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "connect: to ('smtp.gmail.com', 587) None\n",
      "reply: b'220 smtp.gmail.com ESMTP 41be03b00d2f7-716ba6aa524sm8902730a12.63 - gsmtp\\r\\n'\n",
      "reply: retcode (220); Msg: b'smtp.gmail.com ESMTP 41be03b00d2f7-716ba6aa524sm8902730a12.63 - gsmtp'\n",
      "connect: b'smtp.gmail.com ESMTP 41be03b00d2f7-716ba6aa524sm8902730a12.63 - gsmtp'\n",
      "send: 'ehlo [100.96.2.115]\\r\\n'\n",
      "reply: b'250-smtp.gmail.com at your service, [52.38.54.186]\\r\\n'\n",
      "reply: b'250-SIZE 35882577\\r\\n'\n",
      "reply: b'250-8BITMIME\\r\\n'\n",
      "reply: b'250-STARTTLS\\r\\n'\n",
      "reply: b'250-ENHANCEDSTATUSCODES\\r\\n'\n",
      "reply: b'250-PIPELINING\\r\\n'\n",
      "reply: b'250-CHUNKING\\r\\n'\n",
      "reply: b'250 SMTPUTF8\\r\\n'\n",
      "reply: retcode (250); Msg: b'smtp.gmail.com at your service, [52.38.54.186]\\nSIZE 35882577\\n8BITMIME\\nSTARTTLS\\nENHANCEDSTATUSCODES\\nPIPELINING\\nCHUNKING\\nSMTPUTF8'\n",
      "send: 'STARTTLS\\r\\n'\n",
      "reply: b'220 2.0.0 Ready to start TLS\\r\\n'\n",
      "reply: retcode (220); Msg: b'2.0.0 Ready to start TLS'\n",
      "send: 'ehlo [100.96.2.115]\\r\\n'\n",
      "reply: b'250-smtp.gmail.com at your service, [52.38.54.186]\\r\\n'\n",
      "reply: b'250-SIZE 35882577\\r\\n'\n",
      "reply: b'250-8BITMIME\\r\\n'\n",
      "reply: b'250-AUTH LOGIN PLAIN XOAUTH2 PLAIN-CLIENTTOKEN OAUTHBEARER XOAUTH\\r\\n'\n",
      "reply: b'250-ENHANCEDSTATUSCODES\\r\\n'\n",
      "reply: b'250-PIPELINING\\r\\n'\n",
      "reply: b'250-CHUNKING\\r\\n'\n",
      "reply: b'250 SMTPUTF8\\r\\n'\n",
      "reply: retcode (250); Msg: b'smtp.gmail.com at your service, [52.38.54.186]\\nSIZE 35882577\\n8BITMIME\\nAUTH LOGIN PLAIN XOAUTH2 PLAIN-CLIENTTOKEN OAUTHBEARER XOAUTH\\nENHANCEDSTATUSCODES\\nPIPELINING\\nCHUNKING\\nSMTPUTF8'\n",
      "send: 'AUTH PLAIN AHNjaGVkdWxlYWxhcnRzcnhtZ0BnbWFpbC5jb20AeXlqZXRlanRqaGpoenh4aw==\\r\\n'\n",
      "reply: b'235 2.7.0 Accepted\\r\\n'\n",
      "reply: retcode (235); Msg: b'2.7.0 Accepted'\n",
      "send: 'mail FROM:<schedulealartsrxmg@gmail.com> size=2709\\r\\n'\n",
      "reply: b'250 2.1.0 OK 41be03b00d2f7-716ba6aa524sm8902730a12.63 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.1.0 OK 41be03b00d2f7-716ba6aa524sm8902730a12.63 - gsmtp'\n",
      "send: 'rcpt TO:<lili@rxmg.com>\\r\\n'\n",
      "reply: b'250 2.1.5 OK 41be03b00d2f7-716ba6aa524sm8902730a12.63 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.1.5 OK 41be03b00d2f7-716ba6aa524sm8902730a12.63 - gsmtp'\n",
      "send: 'data\\r\\n'\n",
      "reply: b'354  Go ahead 41be03b00d2f7-716ba6aa524sm8902730a12.63 - gsmtp\\r\\n'\n",
      "reply: retcode (354); Msg: b'Go ahead 41be03b00d2f7-716ba6aa524sm8902730a12.63 - gsmtp'\n",
      "data: (354, b'Go ahead 41be03b00d2f7-716ba6aa524sm8902730a12.63 - gsmtp')\n",
      "send: b'Content-Type: multipart/mixed; boundary=\"===============9120684871317511992==\"\\r\\nMIME-Version: 1.0\\r\\nFrom: schedulealartsrxmg@gmail.com\\r\\nTo: lili@rxmg.com\\r\\nSubject: SMS Daily Update Report\\r\\n\\r\\n--===============9120684871317511992==\\r\\nContent-Type: text/plain; charset=\"us-ascii\"\\r\\nMIME-Version: 1.0\\r\\nContent-Transfer-Encoding: 7bit\\r\\n\\r\\nThe revenue discrepency after November between raw file and Tableau data is $-1120.35\\r\\nThe total revenue after November is $373751.86\\r\\nThe revenue discrepency is -0.0% of the total revenue after November\\r\\nThe revenue we made after November is $374872.22\\r\\n\\r\\nThe revenue by month is\\r\\nMonth\\r\\n1899-12         0.000000\\r\\n1999-12       652.800000\\r\\n2019-12         0.000000\\r\\n2020-01         0.000000\\r\\n2020-02       163.100000\\r\\n2020-03      1116.300000\\r\\n2020-04      5060.200000\\r\\n2020-05      2218.760000\\r\\n2020-06      3551.800000\\r\\n2020-07      6578.100000\\r\\n2020-08     41870.600000\\r\\n2020-09     52336.370000\\r\\n2020-10     50779.900000\\r\\n2020-11     37785.730000\\r\\n2020-12     32358.100000\\r\\n2021-01     23948.040000\\r\\n2021-02     20137.840000\\r\\n2021-03     26584.280000\\r\\n2021-04     50262.800000\\r\\n2021-05     74756.520000\\r\\n2021-06     91146.100000\\r\\n2021-07    144141.810000\\r\\n2021-08    158429.500000\\r\\n2021-09    154983.178800\\r\\n2021-10    201033.211100\\r\\n2021-11    225191.850000\\r\\n2021-12    266562.230000\\r\\n2022-01    276934.304528\\r\\n2022-02    268022.813635\\r\\n2022-03    249501.265543\\r\\n2022-04    296194.537417\\r\\n2022-05    277401.288745\\r\\n2022-06    404793.833177\\r\\n2022-07    247403.937505\\r\\n2022-08    155207.055008\\r\\n2022-09    161623.944255\\r\\n2022-10     39895.442484\\r\\n2022-11     87340.753168\\r\\n2022-12    112803.439432\\r\\n2023-01    126387.636178\\r\\n2023-02    106185.909126\\r\\n2023-03    132726.926130\\r\\n2023-04    138092.207699\\r\\n2023-05    138327.489997\\r\\n2023-06    133089.996674\\r\\n2023-07    157398.963897\\r\\n2023-08    176794.764455\\r\\n2023-09    174226.860641\\r\\n2023-10    204304.325832\\r\\n2023-11    194982.323519\\r\\n2023-12    203068.883677\\r\\n2024-01    194690.306698\\r\\n2024-02    165550.460576\\r\\n2024-03    170519.217965\\r\\n2024-04    166596.294373\\r\\n2024-05    185397.136319\\r\\n2024-06    169737.264578\\r\\nNaT          5482.642000\\r\\nName: Revenue, dtype: float64\\r\\n\\r\\nThe revenue by month for unclaimed revenue is month\\r\\n2024-04     10.32\\r\\n2024-05    301.96\\r\\n2024-06    130.79\\r\\nName: amount, dtype: float64\\r\\n\\r\\n\\r\\nThe revenue by Date is\\r\\nDate\\r\\n1899-12-31       0.00000\\r\\n1999-12-31     652.80000\\r\\n2019-12-25       0.00000\\r\\n2019-12-26       0.00000\\r\\n2020-01-24       0.00000\\r\\n                 ...    \\r\\n2024-06-22    5839.34889\\r\\n2024-06-23    6438.76424\\r\\n2024-06-24    8648.00650\\r\\n2024-06-25    6213.65700\\r\\n2024-06-26     267.84000\\r\\nName: Revenue, Length: 1590, dtype: float64\\r\\n\\r\\n--===============9120684871317511992==--\\r\\n.\\r\\n'\n",
      "reply: b'250 2.0.0 OK  1719423265 41be03b00d2f7-716ba6aa524sm8902730a12.63 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.0.0 OK  1719423265 41be03b00d2f7-716ba6aa524sm8902730a12.63 - gsmtp'\n",
      "data: (250, b'2.0.0 OK  1719423265 41be03b00d2f7-716ba6aa524sm8902730a12.63 - gsmtp')\n",
      "send: 'quit\\r\\n'\n",
      "reply: b'221 2.0.0 closing connection 41be03b00d2f7-716ba6aa524sm8902730a12.63 - gsmtp\\r\\n'\n",
      "reply: retcode (221); Msg: b'2.0.0 closing connection 41be03b00d2f7-716ba6aa524sm8902730a12.63 - gsmtp'\n",
      "connect: to ('smtp.gmail.com', 587) None\n",
      "reply: b'220 smtp.gmail.com ESMTP 98e67ed59e1d1-2c8d7f5a6aesm1937675a91.22 - gsmtp\\r\\n'\n",
      "reply: retcode (220); Msg: b'smtp.gmail.com ESMTP 98e67ed59e1d1-2c8d7f5a6aesm1937675a91.22 - gsmtp'\n",
      "connect: b'smtp.gmail.com ESMTP 98e67ed59e1d1-2c8d7f5a6aesm1937675a91.22 - gsmtp'\n",
      "send: 'ehlo [100.96.2.115]\\r\\n'\n",
      "reply: b'250-smtp.gmail.com at your service, [52.38.54.186]\\r\\n'\n",
      "reply: b'250-SIZE 35882577\\r\\n'\n",
      "reply: b'250-8BITMIME\\r\\n'\n",
      "reply: b'250-STARTTLS\\r\\n'\n",
      "reply: b'250-ENHANCEDSTATUSCODES\\r\\n'\n",
      "reply: b'250-PIPELINING\\r\\n'\n",
      "reply: b'250-CHUNKING\\r\\n'\n",
      "reply: b'250 SMTPUTF8\\r\\n'\n",
      "reply: retcode (250); Msg: b'smtp.gmail.com at your service, [52.38.54.186]\\nSIZE 35882577\\n8BITMIME\\nSTARTTLS\\nENHANCEDSTATUSCODES\\nPIPELINING\\nCHUNKING\\nSMTPUTF8'\n",
      "send: 'STARTTLS\\r\\n'\n",
      "reply: b'220 2.0.0 Ready to start TLS\\r\\n'\n",
      "reply: retcode (220); Msg: b'2.0.0 Ready to start TLS'\n",
      "send: 'ehlo [100.96.2.115]\\r\\n'\n",
      "reply: b'250-smtp.gmail.com at your service, [52.38.54.186]\\r\\n'\n",
      "reply: b'250-SIZE 35882577\\r\\n'\n",
      "reply: b'250-8BITMIME\\r\\n'\n",
      "reply: b'250-AUTH LOGIN PLAIN XOAUTH2 PLAIN-CLIENTTOKEN OAUTHBEARER XOAUTH\\r\\n'\n",
      "reply: b'250-ENHANCEDSTATUSCODES\\r\\n'\n",
      "reply: b'250-PIPELINING\\r\\n'\n",
      "reply: b'250-CHUNKING\\r\\n'\n",
      "reply: b'250 SMTPUTF8\\r\\n'\n",
      "reply: retcode (250); Msg: b'smtp.gmail.com at your service, [52.38.54.186]\\nSIZE 35882577\\n8BITMIME\\nAUTH LOGIN PLAIN XOAUTH2 PLAIN-CLIENTTOKEN OAUTHBEARER XOAUTH\\nENHANCEDSTATUSCODES\\nPIPELINING\\nCHUNKING\\nSMTPUTF8'\n",
      "send: 'AUTH PLAIN AHNjaGVkdWxlYWxhcnRzcnhtZ0BnbWFpbC5jb20AeXlqZXRlanRqaGpoenh4aw==\\r\\n'\n",
      "reply: b'235 2.7.0 Accepted\\r\\n'\n",
      "reply: retcode (235); Msg: b'2.7.0 Accepted'\n",
      "send: 'mail FROM:<schedulealartsrxmg@gmail.com> size=2711\\r\\n'\n",
      "reply: b'250 2.1.0 OK 98e67ed59e1d1-2c8d7f5a6aesm1937675a91.22 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.1.0 OK 98e67ed59e1d1-2c8d7f5a6aesm1937675a91.22 - gsmtp'\n",
      "send: 'rcpt TO:<nathan@rxmg.com>\\r\\n'\n",
      "reply: b'250 2.1.5 OK 98e67ed59e1d1-2c8d7f5a6aesm1937675a91.22 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.1.5 OK 98e67ed59e1d1-2c8d7f5a6aesm1937675a91.22 - gsmtp'\n",
      "send: 'data\\r\\n'\n",
      "reply: b'354  Go ahead 98e67ed59e1d1-2c8d7f5a6aesm1937675a91.22 - gsmtp\\r\\n'\n",
      "reply: retcode (354); Msg: b'Go ahead 98e67ed59e1d1-2c8d7f5a6aesm1937675a91.22 - gsmtp'\n",
      "data: (354, b'Go ahead 98e67ed59e1d1-2c8d7f5a6aesm1937675a91.22 - gsmtp')\n",
      "send: b'Content-Type: multipart/mixed; boundary=\"===============5318555429639306785==\"\\r\\nMIME-Version: 1.0\\r\\nFrom: schedulealartsrxmg@gmail.com\\r\\nTo: nathan@rxmg.com\\r\\nSubject: SMS Daily Update Report\\r\\n\\r\\n--===============5318555429639306785==\\r\\nContent-Type: text/plain; charset=\"us-ascii\"\\r\\nMIME-Version: 1.0\\r\\nContent-Transfer-Encoding: 7bit\\r\\n\\r\\nThe revenue discrepency after November between raw file and Tableau data is $-1120.35\\r\\nThe total revenue after November is $373751.86\\r\\nThe revenue discrepency is -0.0% of the total revenue after November\\r\\nThe revenue we made after November is $374872.22\\r\\n\\r\\nThe revenue by month is\\r\\nMonth\\r\\n1899-12         0.000000\\r\\n1999-12       652.800000\\r\\n2019-12         0.000000\\r\\n2020-01         0.000000\\r\\n2020-02       163.100000\\r\\n2020-03      1116.300000\\r\\n2020-04      5060.200000\\r\\n2020-05      2218.760000\\r\\n2020-06      3551.800000\\r\\n2020-07      6578.100000\\r\\n2020-08     41870.600000\\r\\n2020-09     52336.370000\\r\\n2020-10     50779.900000\\r\\n2020-11     37785.730000\\r\\n2020-12     32358.100000\\r\\n2021-01     23948.040000\\r\\n2021-02     20137.840000\\r\\n2021-03     26584.280000\\r\\n2021-04     50262.800000\\r\\n2021-05     74756.520000\\r\\n2021-06     91146.100000\\r\\n2021-07    144141.810000\\r\\n2021-08    158429.500000\\r\\n2021-09    154983.178800\\r\\n2021-10    201033.211100\\r\\n2021-11    225191.850000\\r\\n2021-12    266562.230000\\r\\n2022-01    276934.304528\\r\\n2022-02    268022.813635\\r\\n2022-03    249501.265543\\r\\n2022-04    296194.537417\\r\\n2022-05    277401.288745\\r\\n2022-06    404793.833177\\r\\n2022-07    247403.937505\\r\\n2022-08    155207.055008\\r\\n2022-09    161623.944255\\r\\n2022-10     39895.442484\\r\\n2022-11     87340.753168\\r\\n2022-12    112803.439432\\r\\n2023-01    126387.636178\\r\\n2023-02    106185.909126\\r\\n2023-03    132726.926130\\r\\n2023-04    138092.207699\\r\\n2023-05    138327.489997\\r\\n2023-06    133089.996674\\r\\n2023-07    157398.963897\\r\\n2023-08    176794.764455\\r\\n2023-09    174226.860641\\r\\n2023-10    204304.325832\\r\\n2023-11    194982.323519\\r\\n2023-12    203068.883677\\r\\n2024-01    194690.306698\\r\\n2024-02    165550.460576\\r\\n2024-03    170519.217965\\r\\n2024-04    166596.294373\\r\\n2024-05    185397.136319\\r\\n2024-06    169737.264578\\r\\nNaT          5482.642000\\r\\nName: Revenue, dtype: float64\\r\\n\\r\\nThe revenue by month for unclaimed revenue is month\\r\\n2024-04     10.32\\r\\n2024-05    301.96\\r\\n2024-06    130.79\\r\\nName: amount, dtype: float64\\r\\n\\r\\n\\r\\nThe revenue by Date is\\r\\nDate\\r\\n1899-12-31       0.00000\\r\\n1999-12-31     652.80000\\r\\n2019-12-25       0.00000\\r\\n2019-12-26       0.00000\\r\\n2020-01-24       0.00000\\r\\n                 ...    \\r\\n2024-06-22    5839.34889\\r\\n2024-06-23    6438.76424\\r\\n2024-06-24    8648.00650\\r\\n2024-06-25    6213.65700\\r\\n2024-06-26     267.84000\\r\\nName: Revenue, Length: 1590, dtype: float64\\r\\n\\r\\n--===============5318555429639306785==--\\r\\n.\\r\\n'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reply: b'250 2.0.0 OK  1719423266 98e67ed59e1d1-2c8d7f5a6aesm1937675a91.22 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.0.0 OK  1719423266 98e67ed59e1d1-2c8d7f5a6aesm1937675a91.22 - gsmtp'\n",
      "data: (250, b'2.0.0 OK  1719423266 98e67ed59e1d1-2c8d7f5a6aesm1937675a91.22 - gsmtp')\n",
      "send: 'quit\\r\\n'\n",
      "reply: b'221 2.0.0 closing connection 98e67ed59e1d1-2c8d7f5a6aesm1937675a91.22 - gsmtp\\r\\n'\n",
      "reply: retcode (221); Msg: b'2.0.0 closing connection 98e67ed59e1d1-2c8d7f5a6aesm1937675a91.22 - gsmtp'\n",
      "connect: to ('smtp.gmail.com', 587) None\n",
      "reply: b'220 smtp.gmail.com ESMTP d2e1a72fcca58-706863876ebsm5951814b3a.142 - gsmtp\\r\\n'\n",
      "reply: retcode (220); Msg: b'smtp.gmail.com ESMTP d2e1a72fcca58-706863876ebsm5951814b3a.142 - gsmtp'\n",
      "connect: b'smtp.gmail.com ESMTP d2e1a72fcca58-706863876ebsm5951814b3a.142 - gsmtp'\n",
      "send: 'ehlo [100.96.2.115]\\r\\n'\n",
      "reply: b'250-smtp.gmail.com at your service, [52.38.54.186]\\r\\n'\n",
      "reply: b'250-SIZE 35882577\\r\\n'\n",
      "reply: b'250-8BITMIME\\r\\n'\n",
      "reply: b'250-STARTTLS\\r\\n'\n",
      "reply: b'250-ENHANCEDSTATUSCODES\\r\\n'\n",
      "reply: b'250-PIPELINING\\r\\n'\n",
      "reply: b'250-CHUNKING\\r\\n'\n",
      "reply: b'250 SMTPUTF8\\r\\n'\n",
      "reply: retcode (250); Msg: b'smtp.gmail.com at your service, [52.38.54.186]\\nSIZE 35882577\\n8BITMIME\\nSTARTTLS\\nENHANCEDSTATUSCODES\\nPIPELINING\\nCHUNKING\\nSMTPUTF8'\n",
      "send: 'STARTTLS\\r\\n'\n",
      "reply: b'220 2.0.0 Ready to start TLS\\r\\n'\n",
      "reply: retcode (220); Msg: b'2.0.0 Ready to start TLS'\n",
      "send: 'ehlo [100.96.2.115]\\r\\n'\n",
      "reply: b'250-smtp.gmail.com at your service, [52.38.54.186]\\r\\n'\n",
      "reply: b'250-SIZE 35882577\\r\\n'\n",
      "reply: b'250-8BITMIME\\r\\n'\n",
      "reply: b'250-AUTH LOGIN PLAIN XOAUTH2 PLAIN-CLIENTTOKEN OAUTHBEARER XOAUTH\\r\\n'\n",
      "reply: b'250-ENHANCEDSTATUSCODES\\r\\n'\n",
      "reply: b'250-PIPELINING\\r\\n'\n",
      "reply: b'250-CHUNKING\\r\\n'\n",
      "reply: b'250 SMTPUTF8\\r\\n'\n",
      "reply: retcode (250); Msg: b'smtp.gmail.com at your service, [52.38.54.186]\\nSIZE 35882577\\n8BITMIME\\nAUTH LOGIN PLAIN XOAUTH2 PLAIN-CLIENTTOKEN OAUTHBEARER XOAUTH\\nENHANCEDSTATUSCODES\\nPIPELINING\\nCHUNKING\\nSMTPUTF8'\n",
      "send: 'AUTH PLAIN AHNjaGVkdWxlYWxhcnRzcnhtZ0BnbWFpbC5jb20AeXlqZXRlanRqaGpoenh4aw==\\r\\n'\n",
      "reply: b'235 2.7.0 Accepted\\r\\n'\n",
      "reply: retcode (235); Msg: b'2.7.0 Accepted'\n",
      "send: 'mail FROM:<schedulealartsrxmg@gmail.com> size=2709\\r\\n'\n",
      "reply: b'250 2.1.0 OK d2e1a72fcca58-706863876ebsm5951814b3a.142 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.1.0 OK d2e1a72fcca58-706863876ebsm5951814b3a.142 - gsmtp'\n",
      "send: 'rcpt TO:<nina@rxmg.com>\\r\\n'\n",
      "reply: b'250 2.1.5 OK d2e1a72fcca58-706863876ebsm5951814b3a.142 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.1.5 OK d2e1a72fcca58-706863876ebsm5951814b3a.142 - gsmtp'\n",
      "send: 'data\\r\\n'\n",
      "reply: b'354  Go ahead d2e1a72fcca58-706863876ebsm5951814b3a.142 - gsmtp\\r\\n'\n",
      "reply: retcode (354); Msg: b'Go ahead d2e1a72fcca58-706863876ebsm5951814b3a.142 - gsmtp'\n",
      "data: (354, b'Go ahead d2e1a72fcca58-706863876ebsm5951814b3a.142 - gsmtp')\n",
      "send: b'Content-Type: multipart/mixed; boundary=\"===============3569679696364965787==\"\\r\\nMIME-Version: 1.0\\r\\nFrom: schedulealartsrxmg@gmail.com\\r\\nTo: nina@rxmg.com\\r\\nSubject: SMS Daily Update Report\\r\\n\\r\\n--===============3569679696364965787==\\r\\nContent-Type: text/plain; charset=\"us-ascii\"\\r\\nMIME-Version: 1.0\\r\\nContent-Transfer-Encoding: 7bit\\r\\n\\r\\nThe revenue discrepency after November between raw file and Tableau data is $-1120.35\\r\\nThe total revenue after November is $373751.86\\r\\nThe revenue discrepency is -0.0% of the total revenue after November\\r\\nThe revenue we made after November is $374872.22\\r\\n\\r\\nThe revenue by month is\\r\\nMonth\\r\\n1899-12         0.000000\\r\\n1999-12       652.800000\\r\\n2019-12         0.000000\\r\\n2020-01         0.000000\\r\\n2020-02       163.100000\\r\\n2020-03      1116.300000\\r\\n2020-04      5060.200000\\r\\n2020-05      2218.760000\\r\\n2020-06      3551.800000\\r\\n2020-07      6578.100000\\r\\n2020-08     41870.600000\\r\\n2020-09     52336.370000\\r\\n2020-10     50779.900000\\r\\n2020-11     37785.730000\\r\\n2020-12     32358.100000\\r\\n2021-01     23948.040000\\r\\n2021-02     20137.840000\\r\\n2021-03     26584.280000\\r\\n2021-04     50262.800000\\r\\n2021-05     74756.520000\\r\\n2021-06     91146.100000\\r\\n2021-07    144141.810000\\r\\n2021-08    158429.500000\\r\\n2021-09    154983.178800\\r\\n2021-10    201033.211100\\r\\n2021-11    225191.850000\\r\\n2021-12    266562.230000\\r\\n2022-01    276934.304528\\r\\n2022-02    268022.813635\\r\\n2022-03    249501.265543\\r\\n2022-04    296194.537417\\r\\n2022-05    277401.288745\\r\\n2022-06    404793.833177\\r\\n2022-07    247403.937505\\r\\n2022-08    155207.055008\\r\\n2022-09    161623.944255\\r\\n2022-10     39895.442484\\r\\n2022-11     87340.753168\\r\\n2022-12    112803.439432\\r\\n2023-01    126387.636178\\r\\n2023-02    106185.909126\\r\\n2023-03    132726.926130\\r\\n2023-04    138092.207699\\r\\n2023-05    138327.489997\\r\\n2023-06    133089.996674\\r\\n2023-07    157398.963897\\r\\n2023-08    176794.764455\\r\\n2023-09    174226.860641\\r\\n2023-10    204304.325832\\r\\n2023-11    194982.323519\\r\\n2023-12    203068.883677\\r\\n2024-01    194690.306698\\r\\n2024-02    165550.460576\\r\\n2024-03    170519.217965\\r\\n2024-04    166596.294373\\r\\n2024-05    185397.136319\\r\\n2024-06    169737.264578\\r\\nNaT          5482.642000\\r\\nName: Revenue, dtype: float64\\r\\n\\r\\nThe revenue by month for unclaimed revenue is month\\r\\n2024-04     10.32\\r\\n2024-05    301.96\\r\\n2024-06    130.79\\r\\nName: amount, dtype: float64\\r\\n\\r\\n\\r\\nThe revenue by Date is\\r\\nDate\\r\\n1899-12-31       0.00000\\r\\n1999-12-31     652.80000\\r\\n2019-12-25       0.00000\\r\\n2019-12-26       0.00000\\r\\n2020-01-24       0.00000\\r\\n                 ...    \\r\\n2024-06-22    5839.34889\\r\\n2024-06-23    6438.76424\\r\\n2024-06-24    8648.00650\\r\\n2024-06-25    6213.65700\\r\\n2024-06-26     267.84000\\r\\nName: Revenue, Length: 1590, dtype: float64\\r\\n\\r\\n--===============3569679696364965787==--\\r\\n.\\r\\n'\n",
      "reply: b'250 2.0.0 OK  1719423268 d2e1a72fcca58-706863876ebsm5951814b3a.142 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.0.0 OK  1719423268 d2e1a72fcca58-706863876ebsm5951814b3a.142 - gsmtp'\n",
      "data: (250, b'2.0.0 OK  1719423268 d2e1a72fcca58-706863876ebsm5951814b3a.142 - gsmtp')\n",
      "send: 'quit\\r\\n'\n",
      "reply: b'221 2.0.0 closing connection d2e1a72fcca58-706863876ebsm5951814b3a.142 - gsmtp\\r\\n'\n",
      "reply: retcode (221); Msg: b'2.0.0 closing connection d2e1a72fcca58-706863876ebsm5951814b3a.142 - gsmtp'\n",
      "connect: to ('smtp.gmail.com', 587) None\n",
      "reply: b'220 smtp.gmail.com ESMTP 41be03b00d2f7-716b4a73051sm9023758a12.47 - gsmtp\\r\\n'\n",
      "reply: retcode (220); Msg: b'smtp.gmail.com ESMTP 41be03b00d2f7-716b4a73051sm9023758a12.47 - gsmtp'\n",
      "connect: b'smtp.gmail.com ESMTP 41be03b00d2f7-716b4a73051sm9023758a12.47 - gsmtp'\n",
      "send: 'ehlo [100.96.2.115]\\r\\n'\n",
      "reply: b'250-smtp.gmail.com at your service, [52.38.54.186]\\r\\n'\n",
      "reply: b'250-SIZE 35882577\\r\\n'\n",
      "reply: b'250-8BITMIME\\r\\n'\n",
      "reply: b'250-STARTTLS\\r\\n'\n",
      "reply: b'250-ENHANCEDSTATUSCODES\\r\\n'\n",
      "reply: b'250-PIPELINING\\r\\n'\n",
      "reply: b'250-CHUNKING\\r\\n'\n",
      "reply: b'250 SMTPUTF8\\r\\n'\n",
      "reply: retcode (250); Msg: b'smtp.gmail.com at your service, [52.38.54.186]\\nSIZE 35882577\\n8BITMIME\\nSTARTTLS\\nENHANCEDSTATUSCODES\\nPIPELINING\\nCHUNKING\\nSMTPUTF8'\n",
      "send: 'STARTTLS\\r\\n'\n",
      "reply: b'220 2.0.0 Ready to start TLS\\r\\n'\n",
      "reply: retcode (220); Msg: b'2.0.0 Ready to start TLS'\n",
      "send: 'ehlo [100.96.2.115]\\r\\n'\n",
      "reply: b'250-smtp.gmail.com at your service, [52.38.54.186]\\r\\n'\n",
      "reply: b'250-SIZE 35882577\\r\\n'\n",
      "reply: b'250-8BITMIME\\r\\n'\n",
      "reply: b'250-AUTH LOGIN PLAIN XOAUTH2 PLAIN-CLIENTTOKEN OAUTHBEARER XOAUTH\\r\\n'\n",
      "reply: b'250-ENHANCEDSTATUSCODES\\r\\n'\n",
      "reply: b'250-PIPELINING\\r\\n'\n",
      "reply: b'250-CHUNKING\\r\\n'\n",
      "reply: b'250 SMTPUTF8\\r\\n'\n",
      "reply: retcode (250); Msg: b'smtp.gmail.com at your service, [52.38.54.186]\\nSIZE 35882577\\n8BITMIME\\nAUTH LOGIN PLAIN XOAUTH2 PLAIN-CLIENTTOKEN OAUTHBEARER XOAUTH\\nENHANCEDSTATUSCODES\\nPIPELINING\\nCHUNKING\\nSMTPUTF8'\n",
      "send: 'AUTH PLAIN AHNjaGVkdWxlYWxhcnRzcnhtZ0BnbWFpbC5jb20AeXlqZXRlanRqaGpoenh4aw==\\r\\n'\n",
      "reply: b'235 2.7.0 Accepted\\r\\n'\n",
      "reply: retcode (235); Msg: b'2.7.0 Accepted'\n",
      "send: 'mail FROM:<schedulealartsrxmg@gmail.com> size=2713\\r\\n'\n",
      "reply: b'250 2.1.0 OK 41be03b00d2f7-716b4a73051sm9023758a12.47 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.1.0 OK 41be03b00d2f7-716b4a73051sm9023758a12.47 - gsmtp'\n",
      "send: 'rcpt TO:<n.ohashi@rxmg.com>\\r\\n'\n",
      "reply: b'250 2.1.5 OK 41be03b00d2f7-716b4a73051sm9023758a12.47 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.1.5 OK 41be03b00d2f7-716b4a73051sm9023758a12.47 - gsmtp'\n",
      "send: 'data\\r\\n'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reply: b'354  Go ahead 41be03b00d2f7-716b4a73051sm9023758a12.47 - gsmtp\\r\\n'\n",
      "reply: retcode (354); Msg: b'Go ahead 41be03b00d2f7-716b4a73051sm9023758a12.47 - gsmtp'\n",
      "data: (354, b'Go ahead 41be03b00d2f7-716b4a73051sm9023758a12.47 - gsmtp')\n",
      "send: b'Content-Type: multipart/mixed; boundary=\"===============2094581291733028325==\"\\r\\nMIME-Version: 1.0\\r\\nFrom: schedulealartsrxmg@gmail.com\\r\\nTo: n.ohashi@rxmg.com\\r\\nSubject: SMS Daily Update Report\\r\\n\\r\\n--===============2094581291733028325==\\r\\nContent-Type: text/plain; charset=\"us-ascii\"\\r\\nMIME-Version: 1.0\\r\\nContent-Transfer-Encoding: 7bit\\r\\n\\r\\nThe revenue discrepency after November between raw file and Tableau data is $-1120.35\\r\\nThe total revenue after November is $373751.86\\r\\nThe revenue discrepency is -0.0% of the total revenue after November\\r\\nThe revenue we made after November is $374872.22\\r\\n\\r\\nThe revenue by month is\\r\\nMonth\\r\\n1899-12         0.000000\\r\\n1999-12       652.800000\\r\\n2019-12         0.000000\\r\\n2020-01         0.000000\\r\\n2020-02       163.100000\\r\\n2020-03      1116.300000\\r\\n2020-04      5060.200000\\r\\n2020-05      2218.760000\\r\\n2020-06      3551.800000\\r\\n2020-07      6578.100000\\r\\n2020-08     41870.600000\\r\\n2020-09     52336.370000\\r\\n2020-10     50779.900000\\r\\n2020-11     37785.730000\\r\\n2020-12     32358.100000\\r\\n2021-01     23948.040000\\r\\n2021-02     20137.840000\\r\\n2021-03     26584.280000\\r\\n2021-04     50262.800000\\r\\n2021-05     74756.520000\\r\\n2021-06     91146.100000\\r\\n2021-07    144141.810000\\r\\n2021-08    158429.500000\\r\\n2021-09    154983.178800\\r\\n2021-10    201033.211100\\r\\n2021-11    225191.850000\\r\\n2021-12    266562.230000\\r\\n2022-01    276934.304528\\r\\n2022-02    268022.813635\\r\\n2022-03    249501.265543\\r\\n2022-04    296194.537417\\r\\n2022-05    277401.288745\\r\\n2022-06    404793.833177\\r\\n2022-07    247403.937505\\r\\n2022-08    155207.055008\\r\\n2022-09    161623.944255\\r\\n2022-10     39895.442484\\r\\n2022-11     87340.753168\\r\\n2022-12    112803.439432\\r\\n2023-01    126387.636178\\r\\n2023-02    106185.909126\\r\\n2023-03    132726.926130\\r\\n2023-04    138092.207699\\r\\n2023-05    138327.489997\\r\\n2023-06    133089.996674\\r\\n2023-07    157398.963897\\r\\n2023-08    176794.764455\\r\\n2023-09    174226.860641\\r\\n2023-10    204304.325832\\r\\n2023-11    194982.323519\\r\\n2023-12    203068.883677\\r\\n2024-01    194690.306698\\r\\n2024-02    165550.460576\\r\\n2024-03    170519.217965\\r\\n2024-04    166596.294373\\r\\n2024-05    185397.136319\\r\\n2024-06    169737.264578\\r\\nNaT          5482.642000\\r\\nName: Revenue, dtype: float64\\r\\n\\r\\nThe revenue by month for unclaimed revenue is month\\r\\n2024-04     10.32\\r\\n2024-05    301.96\\r\\n2024-06    130.79\\r\\nName: amount, dtype: float64\\r\\n\\r\\n\\r\\nThe revenue by Date is\\r\\nDate\\r\\n1899-12-31       0.00000\\r\\n1999-12-31     652.80000\\r\\n2019-12-25       0.00000\\r\\n2019-12-26       0.00000\\r\\n2020-01-24       0.00000\\r\\n                 ...    \\r\\n2024-06-22    5839.34889\\r\\n2024-06-23    6438.76424\\r\\n2024-06-24    8648.00650\\r\\n2024-06-25    6213.65700\\r\\n2024-06-26     267.84000\\r\\nName: Revenue, Length: 1590, dtype: float64\\r\\n\\r\\n--===============2094581291733028325==--\\r\\n.\\r\\n'\n",
      "reply: b'250 2.0.0 OK  1719423269 41be03b00d2f7-716b4a73051sm9023758a12.47 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.0.0 OK  1719423269 41be03b00d2f7-716b4a73051sm9023758a12.47 - gsmtp'\n",
      "data: (250, b'2.0.0 OK  1719423269 41be03b00d2f7-716b4a73051sm9023758a12.47 - gsmtp')\n",
      "send: 'quit\\r\\n'\n",
      "reply: b'221 2.0.0 closing connection 41be03b00d2f7-716b4a73051sm9023758a12.47 - gsmtp\\r\\n'\n",
      "reply: retcode (221); Msg: b'2.0.0 closing connection 41be03b00d2f7-716b4a73051sm9023758a12.47 - gsmtp'\n"
     ]
    }
   ],
   "source": [
    "toaddr = ['lili@rxmg.com','nathan@rxmg.com','nina@rxmg.com','n.ohashi@rxmg.com']\n",
    "#toaddr = ['nina@rxmg.com']\n",
    "#toaddr = ['lili@rxmg.com']\n",
    "subject_line = 'SMS Daily Update Report' \n",
    "email_body \n",
    "for i in toaddr:\n",
    "    send_email.send_email('',subject_line,email_body,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3c99c68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e24ccc6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 1632.72 seconds\n"
     ]
    }
   ],
   "source": [
    "elapsed_time = end_time - start_time\n",
    "print(f\"Execution time: {elapsed_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
