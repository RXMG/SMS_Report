{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec8304fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import filepath\n",
    "import send_email \n",
    "import pygsheets\n",
    "import infrastructure \n",
    "import time\n",
    "\n",
    "import filepath \n",
    "localfolder = filepath.output_folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fa84859",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-10T16:00:55.815503Z",
     "start_time": "2023-08-10T15:57:41.600924Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\lilig\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pygsheets\\worksheet.py:1554: UserWarning: At least one column name in the data frame is an empty string. If this is a concern, please specify include_tailing_empty=False and/or ensure that each column containing data has a name.\n",
      "  warnings.warn('At least one column name in the data frame is an empty string. If this is a concern, please specify include_tailing_empty=False and/or ensure that each column containing data has a name.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166859837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\lilig\\AppData\\Local\\anaconda3\\Lib\\site-packages\\dateutil\\parser\\_parser.py:1207: UnknownTimezoneWarning: tzname EST identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n",
      "D:\\Users\\lilig\\AppData\\Local\\anaconda3\\Lib\\site-packages\\dateutil\\parser\\_parser.py:1207: UnknownTimezoneWarning: tzname EST identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_12408\\264990687.py:134: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  combined_csv = pd.concat([pd.read_csv(f) for f in all_files ])\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_12408\\264990687.py:134: DtypeWarning: Columns (9,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  combined_csv = pd.concat([pd.read_csv(f) for f in all_files ])\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_12408\\264990687.py:134: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  combined_csv = pd.concat([pd.read_csv(f) for f in all_files ])\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_12408\\264990687.py:134: DtypeWarning: Columns (7,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  combined_csv = pd.concat([pd.read_csv(f) for f in all_files ])\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_12408\\264990687.py:134: DtypeWarning: Columns (9,10,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  combined_csv = pd.concat([pd.read_csv(f) for f in all_files ])\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_12408\\264990687.py:134: DtypeWarning: Columns (10,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  combined_csv = pd.concat([pd.read_csv(f) for f in all_files ])\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_12408\\264990687.py:134: DtypeWarning: Columns (10,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  combined_csv = pd.concat([pd.read_csv(f) for f in all_files ])\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_12408\\264990687.py:134: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  combined_csv = pd.concat([pd.read_csv(f) for f in all_files ])\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_12408\\264990687.py:134: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  combined_csv = pd.concat([pd.read_csv(f) for f in all_files ])\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_12408\\264990687.py:134: DtypeWarning: Columns (7,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  combined_csv = pd.concat([pd.read_csv(f) for f in all_files ])\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_12408\\264990687.py:134: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  combined_csv = pd.concat([pd.read_csv(f) for f in all_files ])\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_12408\\264990687.py:134: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  combined_csv = pd.concat([pd.read_csv(f) for f in all_files ])\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_12408\\264990687.py:134: DtypeWarning: Columns (7,9,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  combined_csv = pd.concat([pd.read_csv(f) for f in all_files ])\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_12408\\264990687.py:134: DtypeWarning: Columns (7,9,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  combined_csv = pd.concat([pd.read_csv(f) for f in all_files ])\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_12408\\264990687.py:134: DtypeWarning: Columns (7,9,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  combined_csv = pd.concat([pd.read_csv(f) for f in all_files ])\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_12408\\264990687.py:134: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  combined_csv = pd.concat([pd.read_csv(f) for f in all_files ])\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_12408\\264990687.py:134: DtypeWarning: Columns (9,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  combined_csv = pd.concat([pd.read_csv(f) for f in all_files ])\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_12408\\264990687.py:134: DtypeWarning: Columns (7,9,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  combined_csv = pd.concat([pd.read_csv(f) for f in all_files ])\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_12408\\264990687.py:134: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  combined_csv = pd.concat([pd.read_csv(f) for f in all_files ])\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_12408\\264990687.py:134: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  combined_csv = pd.concat([pd.read_csv(f) for f in all_files ])\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_12408\\264990687.py:134: DtypeWarning: Columns (7,9,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  combined_csv = pd.concat([pd.read_csv(f) for f in all_files ])\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_12408\\264990687.py:139: DtypeWarning: Columns (7,9,10,11,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  combined_csv = pd.read_csv(localfolder + 'SMS_master_revenue.csv', dtype={'advertiser_name':'str','campaign_name':'str'})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6794492.62968901\n",
      "6794492.62968901\n",
      "9335559.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "#import publisher from google sheet \n",
    "gc = pygsheets.authorize(service_account_file=filepath.service_account_location)\n",
    "rxmgref = gc.open_by_url('https://docs.google.com/spreadsheets/d/1Tzda6Djr3zQmOhWu7Ief3GVR9Cjaml8238CeX7chj_U/edit#gid=1620368362') \n",
    "publisher_raw  = rxmgref.worksheet('title','Publisher Configurations').get_as_df()\n",
    "publisher_raw = publisher_raw.drop_duplicates(subset='PUBID', keep='last')\n",
    "publisher_raw.to_csv(localfolder+'smartsheet/Publisher.csv')\n",
    "publisher = publisher_raw[['DP.DS or DP.sV','PUBID']].drop_duplicates()\n",
    "\n",
    "# import SMS OMS\n",
    "offer_sheet = infrastructure.get_smartsheet('offers_sms')\n",
    "\n",
    "## import La Nina. \n",
    "gc = pygsheets.authorize(service_account_file=filepath.service_account_location)\n",
    "lanina = gc.open_by_url('https://docs.google.com/spreadsheets/d/1obszkCQoE0ELOR1O0CrLVETUEmEIWlGuyAmK3FgWSJg/edit#gid=1099746391') \n",
    "lanina_sheet  = lanina.worksheet('title','La Nina (Current)').get_as_df()\n",
    "lanina_sheet.to_csv(localfolder+'smartsheet/La_Nina.csv')\n",
    "\n",
    "## EMIT \n",
    "emit = infrastructure.get_smartsheet('emit')\n",
    "email_pubid = emit['Revenue Pub ID'].astype(str).str.split('.',expand = True)[0].unique().tolist()\n",
    "\n",
    "jobs = pd.read_csv(localfolder + 'SMS_SC_SS_Jobs.csv', dtype={'job_id' :'Int64'})\n",
    "creative_stats = pd.read_csv(localfolder + 'SMS_SC_SS_CreativesStats.csv', dtype={'CreativeId' :'str'})\n",
    "raw_creative_stats = creative_stats\n",
    "offers = pd.read_csv(localfolder + 'SMS_SC_SS_Offers.csv', usecols = ['name','hitpath_offer_id', 'type','status','redirect_type','conversion_event','conversion_payout','currency'] )\n",
    "historic_data = pd.read_csv(localfolder + 'SS_data.csv')#  data before Nov1\n",
    "flows = pd.read_csv(localfolder + 'SMS_SC_SS_Flows.csv')\n",
    "\n",
    "# add some information in flows\n",
    "flows['Revenue Source'] = 'Short Code - SS Flow'\n",
    "flows['Code_Type'] = 'Short Code' \n",
    "flows['Shortcode Name'] = flows['Name'].str.split('-',expand = True)[1]\n",
    "flows['Shortcode1'] = flows['Shortcode'].str.extract('(\\d{5})')\n",
    "flows = flows.drop('Shortcode',axis = 1)\n",
    "flows['Dataset'] = flows['Name'].str.split('-',expand = True)[0]\n",
    "flows['Dataset'] = flows['Dataset'].str.replace('JM.ONP','JET.ONP')\n",
    "flows['Dataset'] = flows['Dataset'].str.replace('JM.NTC','JET.NTC')\n",
    "flows.loc[flows['Name'].str.contains('I.A4F', na = False), 'Dataset'] = 'I.A4F'\n",
    "flows.loc[flows['Name'].str.contains('I.MFA', na = False), 'Dataset'] = 'I.MFA'\n",
    "flows.loc[flows['CountUnsub']==0,'CountUnsub'] = flows['CountAutoresponderStop']\n",
    "#flows.loc[flows['CountDeliver']==0, 'CountDeliver'] = flows['CountSent']\n",
    "flows = flows.rename(columns=({'CountSent':'Delivered', 'CostDeliver':'Cost','CountClick':'Clicks','Shortcode1':'Shortcode', 'CountUnsub':'Optout'}))\n",
    "flows['Date'] = pd.to_datetime(flows['Period'].str[:10])\n",
    "flows['AR Flow ID']=flows['Id'].astype('str')\n",
    "flows['AR Day'] = flows['OfferName'].str.extract('(\\d+)\\s*\\(')\n",
    "flows.loc[flows['AR Day'].isna(),'AR Day'] =  flows['OfferName'].str.extract('AR(\\d)')[0]\n",
    "flows.loc[flows['AR Day'].isna(),'AR Day'] = 'Null'\n",
    "flows['Hitpath ID'] = flows['OfferName'].str.extract(r'(\\d{4,5})')\n",
    "flows.loc[flows['OfferName'].str.contains( 'OW',na = False), 'Hitpath ID'] =flows['OfferName'].str.split(\"_|[| ]\",expand= True)[3]\n",
    "flows['AR Flow'] = flows['AR Flow ID'] + '_Day_' + flows['Shortcode Name']+'_'+  flows['AR Day']\n",
    "flows_clean = flows[['Hitpath ID','Date','Dataset','Shortcode','Shortcode Name','Revenue Source','Code_Type','Delivered','Optout','Cost','Clicks','AR Flow','AR Day','AR Flow ID']]\n",
    "flows_clean = flows_clean.merge(publisher[['DP.DS or DP.sV','PUBID']], left_on ='Dataset', right_on = 'DP.DS or DP.sV', how = 'left' )\n",
    "flows_clean = flows_clean.rename(columns=({'PUBID':'Affiliate_id'}))\n",
    "flows_clean.loc[flows_clean['Shortcode Name'] == 'FLC', 'Shortcode'] = '51797'\n",
    "flows_clean.loc[flows_clean['Shortcode Name'] == 'CSS', 'Shortcode'] = '70610'\n",
    "flows_clean.loc[flows_clean['Shortcode Name'] == 'HZB', 'Shortcode'] = '44345'\n",
    "flows_clean.loc[flows_clean['Shortcode Name'] == 'MBC', 'Shortcode'] = '80837'\n",
    "flows_clean.loc[flows_clean['Shortcode Name'] == 'DSS', 'Shortcode'] = '31232'\n",
    "flows_clean.loc[flows_clean['Shortcode Name'] == 'SVT', 'Shortcode'] = '61659'\n",
    "flows_clean.loc[flows_clean['Shortcode Name'] == 'UAA', 'Shortcode'] = '79743'\n",
    "flows_clean.loc[flows_clean['Shortcode Name'] == 'PRC', 'Shortcode'] = '18333164159'\n",
    "flows_clean.loc[flows_clean['Shortcode Name'] == 'UAATF', 'Shortcode'] = '18333641722'\n",
    "flows_clean.loc[flows_clean['Shortcode Name'].str.contains('A4F'), 'Shortcode'] = '15612023538'\n",
    "flows_clean.loc[flows_clean['Shortcode Name'].str.contains('MFA'), 'Shortcode'] = '18332686782'\n",
    "flows_clean.loc[flows_clean['Shortcode'].str.contains( '51797',na = False), 'Shortcode Name'] = 'FLC'\n",
    "flows_clean.loc[flows_clean['Shortcode'].str.contains( '70610',na = False), 'Shortcode Name'] = 'CSS'\n",
    "flows_clean.loc[flows_clean['Shortcode'].str.contains( '44345',na = False), 'Shortcode Name'] = 'HZB'\n",
    "flows_clean.loc[flows_clean['Shortcode'].str.contains( '80837',na = False), 'Shortcode Name'] = 'MBC'\n",
    "flows_clean.loc[flows_clean['Shortcode'].str.contains( '31232',na = False), 'Shortcode Name'] = 'DSS'\n",
    "flows_clean.loc[flows_clean['Shortcode'].str.contains( '61659',na = False), 'Shortcode Name'] = 'SVT'\n",
    "flows_clean.loc[flows_clean['Shortcode'].str.contains( '79743',na = False), 'Shortcode Name'] = 'UAA'\n",
    "flows_clean.loc[flows_clean['Shortcode'].str.contains( '18333164159',na = False), 'Shortcode Name'] = 'PRC'\n",
    "flows_clean.loc[flows_clean['Shortcode'].str.contains( '18333641722',na = False), 'Shortcode Name'] = 'UAATF'\n",
    "flows_clean.loc[flows_clean['Shortcode'].str.contains( '15612023538',na = False), 'Shortcode Name'] = 'A4FLC'\n",
    "flows_clean.loc[flows_clean['Shortcode'].str.contains( '18332686782',na = False), 'Shortcode Name'] = 'MFATF'\n",
    "# change some segment name \n",
    "creative_stats['Segment'] = creative_stats['Segment'].str.replace('WW.YFA','WWM.YFA')\n",
    "creative_stats['Segment'] = creative_stats['Segment'].str.replace('ARM.CR','AI.CC')\n",
    "creative_stats['Segment'] = creative_stats['Segment'].str.replace('RH.3CS','RHD.CC')\n",
    "creative_stats['Segment'] = creative_stats['Segment'].str.replace('ED.247L','EDM.247L')\n",
    "creative_stats['Segment'] = creative_stats['Segment'].str.replace('PA.SWP','PA.PS')\n",
    "creative_stats['Segment'] = creative_stats['Segment'].str.replace('FM.YS','FSM.YS')\n",
    "creative_stats['Segment'] = creative_stats['Segment'].str.replace('CM_','CM.OSR_')\n",
    "creative_stats['Segment'] = creative_stats['Segment'].str.replace('CM.OSR.OSR','CM.OSR')\n",
    "creative_stats['Segment'] = creative_stats['Segment'].str.replace('CM.OSR.OSR.OSR','CM.OSR')\n",
    "creative_stats = creative_stats.rename(columns=({'DeliverCount':'Delivered', 'TotalCost':'Cost','ClickCount':'Clicks','UnsubCount':'Optout'}))\n",
    "creative_id_na = creative_stats.loc[creative_stats['CreativeId'].isna()]\n",
    "jobs_optout = jobs[['id','optout']]\n",
    "creative_stats =  creative_stats.merge(jobs_optout, left_on = 'JobId' ,right_on='id', how='left')\n",
    "jobid_count = creative_stats['JobId'].value_counts().to_dict()\n",
    "creative_stats['Optout'] = creative_stats.apply(lambda row: row['optout'] / jobid_count[row['JobId']], axis=1)\n",
    "creative_stats = creative_stats.groupby(['JobId', 'Tstamp', 'Offer', 'Segment', 'CreativeId', 'CreativeName','Creative']).sum().reset_index()\n",
    "print(creative_stats['Delivered'].sum())\n",
    "jobs['segments'] = jobs['segments'].str.replace('WW.YFA','WWM.YFA')\n",
    "jobs['segments'] = jobs['segments'].str.replace('ARM.CR','AI.CC')\n",
    "jobs['segments'] = jobs['segments'].str.replace('RH.3CS','RHD.CC')\n",
    "jobs['segments'] = jobs['segments'].str.replace('ED.247L','EDM.247L')\n",
    "jobs['segments'] = jobs['segments'].str.replace('PA.SWP','PA.PS')\n",
    "jobs['segments'] = jobs['segments'].str.replace('FM.YS','FSM.YS')\n",
    "jobs['segments'] = jobs['segments'].str.replace('CM_','CM.OSR_')\n",
    "jobs['segments'] = jobs['segments'].str.replace('CM.OSR.OSR','CM.OSR')\n",
    "jobs['segments'] = jobs['segments'].str.replace('CM.OSR.OSR.OSR','CM.OSR')\n",
    "\n",
    "\n",
    "#clean jobs file\n",
    "#drop columns with nulls\n",
    "jobs.isna().sum()\n",
    "drop_columns_jobs = ['remote_status']\n",
    "jobs.drop(columns=drop_columns_jobs, inplace=True, axis=1)\n",
    "jobs = jobs.rename(columns={ 'id':'job_id', 'name':'job_name'})\n",
    "#convert date fields to correct format\n",
    "jobs['start_tstamp'] = pd.to_datetime(jobs['start_tstamp'])\n",
    "jobs['end_tstamp'] = pd.to_datetime(jobs['end_tstamp'])\n",
    "jobs['scheduled_tstamp'] = pd.to_datetime(jobs['scheduled_tstamp'],format = 'mixed')\n",
    "jobs['Scheduling Time'] = jobs['scheduled_tstamp'].dt.tz_convert('US/Pacific').dt.strftime('%Y-%m-%d %H:%M')\n",
    "creative_stats =  creative_stats.merge(jobs[['job_id','Scheduling Time']], left_on = 'JobId' ,right_on='job_id', how='left')\n",
    "creative_id_na =  creative_id_na.merge(jobs[['job_id','Scheduling Time']], left_on = 'JobId' ,right_on='job_id', how='left')\n",
    "\n",
    "#clean creative_stats\n",
    "creative_stats.isna().sum()\n",
    "creative_stats['Tstamp'] = pd.to_datetime(creative_stats['Tstamp'])\n",
    "\n",
    "#clean offers\n",
    "offers.isna().sum()\n",
    "offers = offers.rename(columns={ 'id':'offer_id', 'name':'offer'})\n",
    "\n",
    "#####engagement stats for all sends#####\n",
    "#merge above df with reveneu file to get all of reveneu information\n",
    "## combine all months revenue CSVs into one master-revenue file.\n",
    "os.chdir(localfolder + \"SMS Rev\")\n",
    "\n",
    "all_files = [i for i in glob.glob('*.{}'.format('csv'))]\n",
    "#combine all files in the list\n",
    "combined_csv = pd.concat([pd.read_csv(f) for f in all_files ])\n",
    "####Exporting master revenue file####\n",
    "combined_csv.to_csv( localfolder + \"SMS_master_revenue.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "\n",
    "combined_csv = pd.read_csv(localfolder + 'SMS_master_revenue.csv', dtype={'advertiser_name':'str','campaign_name':'str'})\n",
    "daily_revenue = combined_csv\n",
    "daily_revenue['date'] = pd.to_datetime(daily_revenue['date'],format = 'mixed').dt.date\n",
    "daily_revenue_gpby = daily_revenue.groupby(['affiliate_id','date'])['amount'].sum().reset_index()\n",
    "jump_page_csv = pd.read_csv( localfolder+\"jumppage_click.csv\")\n",
    "jumppage_clicks = jump_page_csv.rename(columns = {'subid_1':\"subid_2\"})\n",
    "print(combined_csv['amount'].sum())\n",
    "combined_csv = pd.concat([combined_csv,jumppage_clicks])\n",
    "print(combined_csv['amount'].sum())\n",
    "print(combined_csv['Jump Page Clicks'].sum())\n",
    "#check subids\n",
    "subs = [\"subid_1\",\"subid_2\", \"subid_5\"]\n",
    "def sid(d):\n",
    "    sid = \"subid_3\"\n",
    "    for j in subs:\n",
    "        if (d[f\"{j} uc\"]>=5) & (d[f\"{j} ucs\"]==0):\n",
    "            sid = j\n",
    "    return d[sid]\n",
    "for i in subs:\n",
    "    combined_csv[i] = combined_csv[i].astype(str)\n",
    "    combined_csv[f\"{i} uc\"] = combined_csv[i].str.count(\"_\")\n",
    "    combined_csv[f\"{i} ucs\"] = combined_csv[i].str.count(\":\")\n",
    "#merge all subids from 1,2 and 5 in subid\n",
    "combined_csv[\"sub_id\"] = combined_csv.apply(sid,axis=1)\n",
    "\n",
    "########## SS BEGIN #########\n",
    "combined_csv['first_split']=combined_csv['sub_id'].str.split('_').str[0]\n",
    "\"\"\" \n",
    "jumppage_clicks[\"sub_id\"] = jumppage_clicks[\"subid_1\"]\n",
    "subs = [\"sub_id\"]\n",
    "# doing same cleaning with jumpapge files  \n",
    "for i in subs:\n",
    "    jumppage_clicks[i] = jumppage_clicks[i].astype(str)\n",
    "    jumppage_clicks[f\"{i} uc\"] = jumppage_clicks[i].str.count(\"_\")\n",
    "#merge all subids from 1,2 and 5 in subid\n",
    "\n",
    "jumppage_clicks['first_split']=jumppage_clicks['sub_id'].str.split('_').str[0]\n",
    "\"\"\"\n",
    "# get the date based on subid date \n",
    "combined_csv = combined_csv.reset_index(drop=True)\n",
    "combined_csv['Dash_Date_from_subid'] = combined_csv['sub_id'].str.extract(r'(\\d{1,2}[A-Za-z]{3}\\d{2})')\n",
    "combined_csv['Dash_Date_from_subid'] = pd.to_datetime(combined_csv['Dash_Date_from_subid'], format='%d%b%y',errors='coerce')\n",
    "combined_csv['date'] = pd.to_datetime(combined_csv['date'],errors='coerce')\n",
    "combined_csv.loc[combined_csv['Dash_Date_from_subid'].isna(),'Dash_Date_from_subid'] = combined_csv['date']\n",
    "combined_csv['date'] = combined_csv['Dash_Date_from_subid']\n",
    "combined_csv.loc[combined_csv['campaign_id']==8202]\n",
    "combined_csv.loc[(combined_csv['affiliate_id'] == 460001) & (combined_csv['site_id'].isna()== False),'affiliate_id'] = combined_csv['site_id']\n",
    "combined_csv['affiliate_id'] = combined_csv['affiliate_id'].astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6bd0a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3382415.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flows = pd.read_csv(localfolder + 'SMS_SC_SS_Flows.csv')\n",
    "flows[(flows['Period']>='2023-10-01')&(flows['Period']<='2023-10-31')]['CountSent'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db92e61",
   "metadata": {},
   "source": [
    "## Long Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d5efa5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-10T16:00:58.363893Z",
     "start_time": "2023-08-10T16:00:55.862467Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "990.45\n",
      "990.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_12408\\3551923609.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  lc_lanina['Long Code Content ID'] = lc_lanina['Long Code Content ID'].astype(str).str.zfill(5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" \\nmerged_data = merged_data[['Date','Scheduling Time', 'Offer','Hitpath_Offer_ID','DP.SV','Affiliate_Id', 'DP&Pub','Job_Id', 'Job_Name','Creative_Id','Creativename','Creative','Send Strategy', 'Shortcode', 'Start_Tstamp', 'Segments', 'Revenue','Jump Page Clicks', 'Delivered', 'Not_Delivered', 'Optout', 'Clicks',\\n       'Cost', 'Ecpm', 'Time', 'Publisher', 'Campaign', 'Route',  'Carrier', 'Dataset', 'Message',\\n       'Responder Template', 'Keyword', 'Responder', 'Router Domain Name' , 'c1', 'Responded', 'Response Rate', 'CTR',\\n        'Gross Profit' , 'Gross Margin', 'RPU' ,'Provider', 'Code_Type','Revenue Source',\\n     'Ar Day','Sub_Id','Campaign_Id','Roi','Shortcode Name','Total']]\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Long Code data from MD\n",
    "\n",
    "lc = gc.open_by_url('https://docs.google.com/spreadsheets/d/1De9UzMw5POUuS90s7dMKsYNTQVnxTj1dvAvwMhOCC5U/edit#gid=0') \n",
    "lc_wk_df  = lc.worksheet('title','Long Code').get_as_df()\n",
    "lc_wk_df['SMS Cost'] = lc_wk_df['SMS Cost'].str.split('$',expand = True)[1].astype('float')\n",
    "# import MD_\n",
    "md_optout = pd.read_csv(localfolder + 'MD_optout.csv', index_col= False)\n",
    "md_optout[['campaign','Date']]=md_optout['campaign'].str.split('\\nStarted At: ',expand = True)\n",
    "md_optout['Date'] = pd.to_datetime(md_optout['Date'], format='%Y-%m-%d %H:%M:%S',errors='coerce')\n",
    "md_optout['campaign'] = md_optout['campaign'].str.extract('([a-zA-Z].*)', expand=True)\n",
    "lc_wk_df['Date'] = pd.to_datetime(lc_wk_df['Date'], errors='coerce')\n",
    "lc_wk_df = lc_wk_df.merge(md_optout[['Date','campaign','optout']], left_on = ['Offer','Date'], right_on = ['campaign','Date'], how = 'left')\n",
    "# rename to Optout\n",
    "lc_wk_df = lc_wk_df.rename(columns = {'Offer':'Sub_Id','optout':'Optout'})\n",
    "lc_rev = combined_csv.loc[(combined_csv['first_split']=='MD') | (combined_csv['first_split']=='TB') ]\n",
    "#lc_rev = combined_csv.loc[(combined_csv['first_split']=='MD') ]\n",
    "lc_rev.loc[lc_rev['Dash_Date_from_subid'].isna(), 'Dash_Date_from_subid'] = lc_rev['date']\n",
    "lc_rev =lc_rev.rename(columns = {\"Dash_Date_from_subid\":'Date','sub_id':'Sub_Id'})\n",
    "#  define the pattern\n",
    "#pattern = r'(.+\\d{1,2}[A-Za-z]{3}\\d{2})'\n",
    "# extract the pattern\n",
    "#lc_rev['Sub_Id'] = lc_rev['sub_id'].str.extract(pattern)\n",
    "lc_rev_summary = lc_rev.groupby(['Sub_Id','Date'])['amount'].sum().reset_index()\n",
    "lc_df = pd.concat([lc_wk_df,lc_rev_summary],axis = 0, ignore_index=True)\n",
    "# remove Date from lc_df\n",
    "lc_df['Date']= pd.to_datetime(lc_df['Date'],errors='coerce')\n",
    "lc_df = lc_df.groupby(['Sub_Id','Date']).sum().reset_index()  \n",
    "lc_df = lc_df.fillna(0)\n",
    "                           \n",
    "                           \n",
    "lc_df.loc[lc_df['Sub_Id'] == '01JUN23_8838_W4_EDU_SVT','Sub_Id'] = 'MD_LC_OMG_SVT_460918__8838_OT_01JUN23'\n",
    "lc_df.loc[lc_df['Sub_Id'] == '05Jun23_MD_LC_OMG_SVT_460918_W1_11714_T1','Sub_Id'] = 'MD_LC_OMG_SVT_460918__11714_OT_05Jun23'\n",
    "lc_df.loc[lc_df['Sub_Id'] == '11714-2_OMG_SVT','Sub_Id'] = 'MD_LC_OMG_SVT_460918__11714_OT_'\n",
    "lc_df.loc[lc_df['Sub_Id'] == 'MD_LC_OMG_SVT_460919_00066_9088__29Jul23','Sub_Id'] = 'MD_LC_OMG_SVT_460919_00066_9088_P_29Jul23'\n",
    "lc_df.loc[lc_df['Sub_Id'] == 'MD_LC_OMG_SVT_460920_00068_12076__30Jul23','Sub_Id']  = 'MD_LC_OMG_SVT_460920_00068_12076_P_30Jul23'\n",
    "lc_df['split_column'] = lc_df['Sub_Id'].str.split('_')\n",
    "lc_df['subid_uc'] = lc_df['Sub_Id'].str.count('_')\n",
    "#lc_df['Date'] = lc_df.loc[lc_df['Date'].isna(), 'Date'] = pd.to_datetime(lc_df['split_column'].str[8], format='%d%b%y',errors='coerce')\n",
    "lc_df['Send Strategy'] = lc_df['split_column'].str[7]\n",
    "lc_df['Hitpath_Offer_ID'] = lc_df['split_column'].str[6]\n",
    "# detect a 6digit number that start with \"46\" from Sub_Id\n",
    "lc_df['Affiliate_Id'] = lc_df['Sub_Id'].str.extract(r'(46\\d{4})')\n",
    "#lc_df['Affiliate_Id'] = lc_df['split_column'].str[4]\n",
    "lc_df['Long Code Content ID'] = lc_df['split_column'].str[5]\n",
    "lc_df['Shortcode Name'] = ''\n",
    "lc_df.loc[lc_df['split_column'].str[3] == 'SVT','Shortcode Name' ] = 'SVTLC'\n",
    "lc_df.loc[lc_df['split_column'].str[3] == 'UAA','Shortcode Name' ] = 'UAALC'\n",
    "lc_df['Send Strategy'] = lc_df['Send Strategy'].str.replace('T0','OT')\n",
    "lc_df['Send Strategy'] = lc_df['Send Strategy'].str.replace('T1','OT') \n",
    "lc_df.loc[lc_df['split_column'].str[0] == 'TB', 'Revenue Source'] = 'Long Code - Textback'\n",
    "lc_df.loc[lc_df['split_column'].str[0] == 'MD', 'Revenue Source'] = 'Long Code - Mobile Drips'\n",
    "lc_df.loc[lc_df['split_column'].str[0] == 'TB', 'Send Strategy'] = 'P'\n",
    "lc_df.loc[lc_df['split_column'].str[0] == 'TB', 'Long Code Content ID'] = np.nan\n",
    "\n",
    "# currently we don't consider split test content option \n",
    "lc_df['amount'] = lc_df['amount'].fillna(0)\n",
    "print(lc_df.loc[lc_df['split_column'].str[0] == 'MD',]['amount'].sum())\n",
    "lc_lanina = lanina_sheet[(lanina_sheet['Long Code Content ID'].isna() == False) & ((lanina_sheet['Long Code Content ID'] !=\"\"))]\n",
    "lc_lanina['Long Code Content ID'] = lc_lanina['Long Code Content ID'].astype(str).str.zfill(5)\n",
    "lc_df_full = lc_df.merge(lc_lanina[['Long Code Content ID','Reporting Content ID','Content']],copy = False, how= 'left', on = 'Long Code Content ID')\n",
    "print(lc_df_full.loc[lc_df_full['split_column'].str[0] == 'MD',]['amount'].sum())\n",
    "lc_df_full['Code_Type'] = 'Long Code'\n",
    "lc_df_full_11 = lc_df_full[lc_df_full['Date'] >= '2022-11-01']\n",
    "\n",
    "\n",
    "lc_df_full = lc_df_full.rename(columns = {'Qty':'Sent','Daily Success Qty':'Delivered','Fail Qty': 'Undelivered',\\\n",
    "                            'Clicks Qty':'Clicks','SMS Cost':'Cost',\\\n",
    "                            'amount':'Revenue','Long Code Content ID':'Creative_Id', 'Reporting Content ID':'Creativename','Content':'Creative'})\n",
    "\n",
    "lc_df_full = lc_df_full[['Date','Affiliate_Id', 'Hitpath_Offer_ID','Sent','Delivered','Undelivered','Clicks','Optout',\\\n",
    "       'Cost','Revenue', 'Send Strategy', 'Shortcode Name', 'Revenue Source', 'Code_Type','Sub_Id','Creative_Id','Creativename','Creative']]\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "merged_data = merged_data[['Date','Scheduling Time', 'Offer','Hitpath_Offer_ID','DP.SV','Affiliate_Id', 'DP&Pub','Job_Id', 'Job_Name','Creative_Id','Creativename','Creative','Send Strategy', 'Shortcode', 'Start_Tstamp', 'Segments', 'Revenue','Jump Page Clicks', 'Delivered', 'Not_Delivered', 'Optout', 'Clicks',\n",
    "       'Cost', 'Ecpm', 'Time', 'Publisher', 'Campaign', 'Route',  'Carrier', 'Dataset', 'Message',\n",
    "       'Responder Template', 'Keyword', 'Responder', 'Router Domain Name' , 'c1', 'Responded', 'Response Rate', 'CTR',\n",
    "        'Gross Profit' , 'Gross Margin', 'RPU' ,'Provider', 'Code_Type','Revenue Source',\n",
    "     'Ar Day','Sub_Id','Campaign_Id','Roi','Shortcode Name','Total']]\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6ab16a",
   "metadata": {},
   "source": [
    "## Short Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebd487cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-10T16:01:35.522346Z",
     "start_time": "2023-08-10T16:00:57.519352Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2235779.3910549995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_12408\\2964799913.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  combined_csv_ss_only.loc[combined_csv_ss_only['offer_id'].str.contains(\"^(\\d{4,5}).*|OW\", regex = True, na = False) == False , 'offer_id'] =  combined_csv_ss_only['split_column'].str[5]\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_12408\\2964799913.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  combined_csv_ss_only.loc[combined_csv_ss_only['offer_id'].str.contains(\"^(\\d{4,5}).*|OW\", regex = True, na = False) == False, 'offer_id'] =  combined_csv_ss_only['split_column'].str[4]\n"
     ]
    }
   ],
   "source": [
    "combined_csv['date'] = pd.to_datetime(combined_csv['date'])\n",
    "combined_csv_ss_only = combined_csv[(combined_csv['first_split']=='SS')  |  (combined_csv['first_split']=='SMS')] \n",
    "combined_csv_ss_only = combined_csv_ss_only[combined_csv_ss_only['date']>='2022-11-01'].reset_index(drop=True)\n",
    "print(combined_csv_ss_only['amount'].sum())\n",
    "#combined_csv_ss_only= combined_csv_ss_only.drop_duplicates()\n",
    "#count number of underscores in subid\n",
    "combined_csv_ss_only['subid_uc'] = combined_csv_ss_only.sub_id.str.count('_')\n",
    "#combined_csv_ss_only['subid_uc']=combined_csv.sub_id.str.count('_')\n",
    "\n",
    "#ignoring the subids that  are not formatted correctly ex: SS_HZB_{{datasource_id}}_{{job_id}}_ALL.SMS_10434_SM_44345_{{today_d}}{{today_mon}}{{today_yy}}_1_{{member_id}}\n",
    "#they have very less revenue under them\n",
    "#these are with sub id uc <12\n",
    "combined_csv_ss_only = combined_csv_ss_only[combined_csv_ss_only['subid_uc'] < 12]\n",
    "\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['advertiser_name'] == 'NIC','campaign_id'] = 10267\n",
    "Offer_name = combined_csv_ss_only.groupby(['campaign_id','campaign_name']).count().reset_index()[['campaign_id','campaign_name']]\n",
    "\n",
    "# get jump page sum and revenue sum \n",
    "combined_csv_ss_only['split_column'] = combined_csv_ss_only['sub_id'].str.split('_')\n",
    "combined_csv_ss_only.loc[(combined_csv_ss_only['first_split'] == 'SS')  , 'offer_id'] = combined_csv_ss_only['split_column'].str[6]\n",
    "combined_csv_ss_only.loc[(combined_csv_ss_only['first_split'] == 'SMS')  , 'offer_id'] = combined_csv_ss_only['split_column'].str[7]\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['offer_id'].str.contains(\"^(\\d{4,5}).*|OW\", regex = True, na = False) == False , 'offer_id'] =  combined_csv_ss_only['split_column'].str[5]\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['offer_id'].str.contains(\"^(\\d{4,5}).*|OW\", regex = True, na = False) == False, 'offer_id'] =  combined_csv_ss_only['split_column'].str[4]\n",
    "\n",
    "combined_csv_ss_only['campaign_id']= combined_csv_ss_only['campaign_id'].astype('str').str.split('.',expand = True)[0]\n",
    "\n",
    "combined_csv_ss_only['Jump Page Version'] = '0'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['offer_id'].str.contains('v',na = False),'Jump Page Version'] = combined_csv_ss_only['offer_id'].str.split('v',expand = True)[1]\n",
    "\n",
    "\n",
    "#combined_csv_ss_only['c1'] = combined_csv_ss_only['sub_id'].str[:-9]\n",
    "\n",
    "# import offer wall database\n",
    "ofwall_df = infrastructure.get_smartsheet('ow_sms')\n",
    "ofwall_df = ofwall_df.rename(columns = {'Hitpath Offer ID':'Offer Wall ID'})\n",
    "offer_wall_id = ofwall_df['Offer Wall ID'].astype(str).unique().tolist()\n",
    "combined_csv_ss_only['Offer Type'] = 'Single Offer'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['offer_id'].isin(offer_wall_id), 'Offer Type'] = 'Offer Wall'\n",
    "offerwall = combined_csv_ss_only.loc[combined_csv_ss_only['Offer Type']=='Offer Wall' ,]\n",
    "\n",
    "combined_csv_ss_only['Offer Wall Id'] = ''\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['Offer Type']=='Offer Wall' , 'Offer Wall Id'] = combined_csv_ss_only['offer_id']\n",
    "#combined_csv_ss_only['offer_id'] = combined_csv_ss_only['offer_id'].astype('str').str.extract(r'\\b(\\d{4,5}).*')\n",
    "#combined_csv_ss_only.loc[combined_csv_ss_only['Offer Type']=='Offer Wall' , 'campaign_id'] = combined_csv_ss_only['offer_id'].str.split('OW',expand = True)[0]\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['Offer Type']=='Offer Wall' , 'campaign_id'] = combined_csv_ss_only['offer_id']\n",
    "combined_csv_ss_only.loc[(combined_csv_ss_only['campaign_id'].isna()) | (combined_csv_ss_only['campaign_id'] == 'nan'),'campaign_id' ] = combined_csv_ss_only['offer_id']\n",
    "combined_csv_ss_only.loc[(combined_csv_ss_only['campaign_id'] != combined_csv_ss_only['offer_id'])  ,'campaign_id'] = combined_csv_ss_only['offer_id']\n",
    "# rename offer id to offer wall id\n",
    "\n",
    "\n",
    "combined_csv_ss_only = combined_csv_ss_only[['date', 'campaign_id', 'Offer Wall Id','Jump Page Version',\n",
    "       'affiliate_id',  'amount', 'Jump Page Clicks',\n",
    "        'sub_id', 'first_split','Dash_Date_from_subid', 'subid_uc','Offer Type']]\n",
    "\n",
    "combined_csv_ss_only['amount'] = combined_csv_ss_only['amount'].fillna(0)\n",
    "combined_csv_ss_only[ 'Jump Page Clicks'] = combined_csv_ss_only[ 'Jump Page Clicks'].fillna(0)\n",
    "combined_csv_ss_only = combined_csv_ss_only.groupby(['date', 'campaign_id', 'affiliate_id', 'Jump Page Version','Offer Wall Id', 'sub_id', 'first_split','Dash_Date_from_subid', 'subid_uc','Offer Type'])[[ 'amount', 'Jump Page Clicks']].sum().reset_index()\n",
    "# use for verification \n",
    "jumppageclicks1 = combined_csv_ss_only['Jump Page Clicks'].sum()\n",
    "revenue1 = combined_csv_ss_only['amount'].sum()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d38681df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-10T16:30:44.688910Z",
     "start_time": "2023-08-10T16:01:36.810599Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_12408\\1843339257.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  combined_csv_ss_only_reformatjobs['Job_id'] = combined_csv_ss_only_reformatjobs['sub_id'].str.extract(f'({\"|\".join(job_ids)})')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9261323.0\n"
     ]
    }
   ],
   "source": [
    "#*** Extracting creative information. ****#\n",
    "# find creative id from list of creative ids from creativestats file.\n",
    "combined_csv_ss_only['split_column'] = combined_csv_ss_only['sub_id'].str.split('_')\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only[combined_csv_ss_only['first_split'] == 'SS'].index , 'creative_id'] = combined_csv_ss_only['split_column'].str[7]\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only[combined_csv_ss_only['first_split'] == 'SMS'].index , 'creative_id'] = combined_csv_ss_only['split_column'].str[8]\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only[combined_csv_ss_only['first_split'] == 'SS'].index , 'Job_id'] = combined_csv_ss_only['split_column'].str[5]\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only[combined_csv_ss_only['first_split'] == 'SMS'].index , 'Job_id'] = combined_csv_ss_only['split_column'].str[6]\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only[combined_csv_ss_only['first_split'] == 'SS'].index , 'Code_Type'] = combined_csv_ss_only['split_column'].str[1]\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only[combined_csv_ss_only['first_split'] == 'SMS'].index , 'Code_Type'] = combined_csv_ss_only['split_column'].str[2] \n",
    "\n",
    "# \n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['Code_Type'] == 'TF', 'Code_Type' ] ='Toll Free' \n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['Code_Type'] == 'SC', 'Code_Type' ] ='Short Code' \n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['Code_Type'] == 'FLC', 'Code_Type' ] ='Short Code'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['Code_Type'] == 'LC', 'Code_Type' ] ='Long Code'\n",
    "# some AR subids in december formatted incorrectly, some residuals formatted incorrectly.\n",
    "#creative ids are length>6, creative ids len =1 are ARs, ignoring both the cases and repalcing creative ids with nans for rest of the length values\n",
    "combined_csv_ss_only['creative_idlen'] = combined_csv_ss_only['creative_id'].str.len()\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only[~combined_csv_ss_only['creative_idlen'].isin([1,6])].index,'creative_id']=np.nan \n",
    "\n",
    "# identify shortcode \n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['sub_id'].str.contains('FLC', na=False), 'Shortcode Name'] = 'FLC'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['sub_id'].str.contains('CSS', na=False), 'Shortcode Name'] = 'CSS'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['sub_id'].str.contains('HZB', na=False), 'Shortcode Name'] = 'HZB'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['sub_id'].str.contains('MBC', na=False), 'Shortcode Name'] = 'MBC'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['sub_id'].str.contains('DSS', na=False), 'Shortcode Name'] = 'DSS'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['sub_id'].str.contains('SVT', na=False), 'Shortcode Name'] = 'SVT'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['sub_id'].str.contains('UAA', na=False), 'Shortcode Name'] = 'UAA'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['sub_id'].str.contains('PRC', na=False), 'Shortcode Name'] = 'PRC'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['sub_id'].str.contains('UAATF', na=False), 'Shortcode Name'] = 'UAATF'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['sub_id'].str.contains('MFA', na=False), 'Shortcode Name'] = 'MFATF'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['sub_id'].str.contains('A4F', na=False), 'Shortcode Name'] = 'A4FLC'\n",
    "combined_csv_ss_only.loc[(combined_csv_ss_only['Shortcode Name'] == 'SVT') & (combined_csv_ss_only['Code_Type'] == 'LC' ) , 'Shortcode Name'] = 'SVTLC'\n",
    "combined_csv_ss_only.loc[(combined_csv_ss_only['Shortcode Name'] == 'UAA') & (combined_csv_ss_only['Code_Type'] == 'LC' ) , 'Shortcode Name'] = 'UAALC'\n",
    "\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['Shortcode Name'] == 'FLC', 'Shortcode'] = '51797'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['Shortcode Name'] == 'CSS', 'Shortcode'] = '70610'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['Shortcode Name'] == 'HZB', 'Shortcode'] = '44345'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['Shortcode Name'] == 'MBC', 'Shortcode'] = '80837'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['Shortcode Name'] == 'DSS', 'Shortcode'] = '31232'\n",
    "combined_csv_ss_only.loc[(combined_csv_ss_only['Shortcode Name'] == 'SVT') , 'Shortcode'] = '61659'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['Shortcode Name'] == 'UAA', 'Shortcode'] = '79743'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['Shortcode Name'] == 'PRC', 'Shortcode'] = '18333164159'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['Shortcode Name'] == 'UAATF', 'Shortcode'] = '18333641722'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['Shortcode Name'] == 'MFATF', 'Shortcode'] = '18332686782'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['Shortcode Name'] == 'A4FLC', 'Shortcode'] = '15612023538'\n",
    "#reformat jobs from residuals and AR\n",
    "combined_csv_ss_only['job_idlen'] = combined_csv_ss_only['Job_id'].str.len()\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only[~combined_csv_ss_only['job_idlen'].isin([1,6])].index,'Job_id']=np.nan \n",
    "combined_csv_ss_only_reformatjobs = combined_csv_ss_only[combined_csv_ss_only['Job_id'].isna() ]\n",
    "jobs['job_id'] =jobs['job_id'].astype('str')\n",
    "\"\"\"\n",
    "for i in jobs['job_id']:\n",
    "    check_len = combined_csv_ss_only_reformatjobs[combined_csv_ss_only_reformatjobs['sub_id'].str.contains(i)]\n",
    "    if len(check_len) > 0:\n",
    "        combined_csv_ss_only_reformatjobs.loc[check_len.index,'Job_id'] = i\n",
    "\"\"\"\n",
    "# Extract the job IDs to a set for faster membership testing\n",
    "creative_stats['JobId'] = creative_stats['JobId'].astype('str')\n",
    "job_ids = set(jobs['job_id']) | set(creative_stats['JobId']) - set(['0'])\n",
    "#creativeid = set(creative_stats['Creative_Id'])\n",
    "#combined_csv_ss_only['creative_id'] = combined_csv_ss_only_reformatjobs['sub_id'].str.extract(f'({\"|\".join(creativeid)})')\n",
    "\n",
    "# Use str.extract to extract the job ID from the sub_id column\n",
    "combined_csv_ss_only_reformatjobs['Job_id'] = combined_csv_ss_only_reformatjobs['sub_id'].str.extract(f'({\"|\".join(job_ids)})')\n",
    "\n",
    "combined_csv_ss_only =   pd.concat([combined_csv_ss_only_reformatjobs,(combined_csv_ss_only[~combined_csv_ss_only['Job_id'].isna()])] )\n",
    "\n",
    "combined_csv_ss_only = combined_csv_ss_only[combined_csv_ss_only['creative_idlen']!= 8]\n",
    "combined_csv_ss_only = combined_csv_ss_only[~combined_csv_ss_only['creative_idlen'].isna()]\n",
    "#get all creatives information.\n",
    "combined_csv_ss_only['Job_id'] = combined_csv_ss_only['Job_id'].astype('float')\n",
    "combined_csv_ss_only['creative_id'] = combined_csv_ss_only['creative_id'].replace('6Oct22', np.nan)\n",
    "combined_csv_ss_only['creative_id'] = combined_csv_ss_only['creative_id'].astype('float')\n",
    "\n",
    "creative_stats['CreativeId'] = creative_stats['CreativeId'].astype('float')\n",
    "# make sure we don't want to merge jobid = 0 \n",
    "#combined_csv_ss_only = combined_csv_ss_only.groupby(['Job_id', 'creative_id'])['amount'].sum().reset_index()\n",
    "#combined_csv_ss_creative = combined_csv_ss_only.merge(creative_stats, left_on = ['Job_id', 'creative_id'], right_on = ['JobId', 'CreativeId'], how = 'left')\n",
    "combined_csv_ss_only['date']  = pd.to_datetime(combined_csv_ss_only['date'] )\n",
    "combined_csv_ss_only['date'] = combined_csv_ss_only['date'].dt.date\n",
    "\n",
    "print(combined_csv_ss_only['Jump Page Clicks'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3e1c76",
   "metadata": {},
   "source": [
    "### SS flow, SS jobs and SS data without jobid(such as optin) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36485303",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-10T16:30:55.238446Z",
     "start_time": "2023-08-10T16:30:44.738392Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3214917.0\n",
      "3214917.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_12408\\352514485.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  flows_clean_no_delivered['Affiliate_Id']= flows_clean_no_delivered['Affiliate_Id1']\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_12408\\352514485.py:124: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  creative_id_na11['Campaign_Id'] = creative_id_na11['Offer'].str.split(' ',expand = True)[0]\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_12408\\352514485.py:126: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  creative_id_na11['Campaign_Id']  = creative_id_na11['Campaign_Id'].astype(float)\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_12408\\352514485.py:127: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  creative_id_na11['Shortcode Name'] = ''\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "combined_csv_ss_only['Dash_Date_from_subid'] = combined_csv_ss_only['sub_id'].str.extract(r'(\\d{1,2}[A-Za-z]{3}\\d{2})')\n",
    "# convert the dateinfo to datetime format using pd.to_datetime()\n",
    "\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['first_split'] == 'SS' , 'Dash_Date_from_subid'] = combined_csv_ss_only['split_column'].str[8]\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['first_split'] == 'SMS' , 'Dash_Date_from_subid'] = combined_csv_ss_only['split_column'].str[9] \n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['Dash_Date_from_subid'] == '1','Dash_Date_from_subid' ] = combined_csv_ss_only['split_column'].str[7] \n",
    "combined_csv_ss_only['Dash_Date_from_subid'] = pd.to_datetime(combined_csv_ss_only['Dash_Date_from_subid'], format=\"%d%b%y\", errors='coerce')\n",
    "\"\"\"\n",
    "combined_csv_ss_only['Hitpath ID'] = combined_csv_ss_only['campaign_id']\n",
    "#combined_csv_ss_only['Dash_Date_from_subid'] = pd.to_datetime(combined_csv_ss_only['Dash_Date_from_subid'], format=\"%d%b%y\")\n",
    "# flow data (51797 & 80837 & jobid = 0)\n",
    "#  by 2023/4/11, we don't include creative id. Subid has that info\n",
    "\n",
    "flows_clean1 = flows_clean.groupby(['Hitpath ID', 'Date', 'Dataset', 'Shortcode', 'Shortcode Name',\\\n",
    "       'Revenue Source', 'Code_Type', 'AR Flow', 'AR Day', 'AR Flow ID', 'DP.DS or DP.sV','Affiliate_id']).sum().reset_index()\n",
    "flows_clean1.loc[flows_clean1['AR Day']=='Null', 'AR Day'] = 0 \n",
    "flows_clean1['AR Day'] = flows_clean1['AR Day'].astype(int)\n",
    "flows_clean1 = flows_clean1.groupby(['Hitpath ID', 'Date', 'Dataset', 'Shortcode', 'Shortcode Name',\n",
    "       'Revenue Source', 'Code_Type',  'DP.DS or DP.sV',\n",
    "       'Affiliate_id']).sum().reset_index()\n",
    "flows_clean1['AR Day'] = flows_clean1['AR Day'].astype(str)\n",
    "\n",
    "combined_csv_ss_only.loc[(combined_csv_ss_only['split_column'].str[5].str.contains('AR',na = False)) & (combined_csv_ss_only['Hitpath ID'].isna()) ,'Hitpath ID'] =combined_csv_ss_only['split_column'].str[6].str.extract(r'\\b(\\d{4,5}).*')\n",
    "combined_csv_ss_ar_flow = combined_csv_ss_only[(combined_csv_ss_only['sub_id'].str.contains('AR')) |  (combined_csv_ss_only['Job_id']==0)]\n",
    "combined_csv_ss_ar_flow1 = combined_csv_ss_ar_flow.groupby(['affiliate_id','Hitpath ID','Shortcode','Dash_Date_from_subid','Offer Type','Jump Page Version','Offer Wall Id'],dropna = False)[['amount','Jump Page Clicks']].sum().reset_index()\n",
    "combined_csv_ss_ar_flow1 = combined_csv_ss_ar_flow1.rename(columns=({'amount': 'Revenue','affiliate_id':'affiliate_id1'}))\n",
    "flows_clean1 = flows_clean1.groupby(['Hitpath ID', 'Date', 'Dataset', 'Shortcode', 'Shortcode Name',\\\n",
    "       'Revenue Source', 'Code_Type', 'AR Day', \\\n",
    "       'DP.DS or DP.sV', 'Affiliate_id']).sum().reset_index()\n",
    "print(combined_csv_ss_ar_flow1['Jump Page Clicks'].sum())\n",
    "unique_id_count = combined_csv_ss_ar_flow1[['Dash_Date_from_subid','Hitpath ID','Shortcode','affiliate_id1']].value_counts().reset_index(name = 'count')\n",
    "combined_csv_ss_ar_flow1 = combined_csv_ss_ar_flow1.merge(unique_id_count, on = ['Dash_Date_from_subid','Hitpath ID','Shortcode','affiliate_id1'],how = 'left')\n",
    "flows_clean2 = flows_clean1.merge(combined_csv_ss_ar_flow1, left_on = ['Date','Hitpath ID','Shortcode','Affiliate_id'],right_on = ['Dash_Date_from_subid','Hitpath ID','Shortcode','affiliate_id1'],how = 'outer')\n",
    "flows_clean2['count'] = flows_clean2['count'].fillna(1)\n",
    "flows_clean2['Delivered'] = flows_clean2['Delivered'] /flows_clean2['count']\n",
    "flows_clean2['Optout'] = flows_clean2['Optout'] /flows_clean2['count']\n",
    "flows_clean2['Cost'] = flows_clean2['Cost'] /flows_clean2['count']\n",
    "flows_clean2['Clicks'] = flows_clean2['Clicks'] /flows_clean2['count']\n",
    "print(flows_clean2['Jump Page Clicks'].sum())\n",
    "flows_clean2['Send Strategy'] = 'AR'\n",
    "flows_clean2['Revenue Source'] = 'Short Code - SS Flow'\n",
    "flows_clean2['Code_Type'] = 'Short Code'\n",
    "flows_clean2.loc[flows_clean2['Date'].isna(), 'Date'] = flows_clean2['Dash_Date_from_subid']\n",
    "flows_clean2.loc[(flows_clean2['Shortcode'] == '51797') & (flows_clean2['Shortcode Name'].isna()), 'Shortcode Name'] = 'FLC'\n",
    "flows_clean2.loc[(flows_clean2['Shortcode'] == '80837') & (flows_clean2['Shortcode Name'].isna()), 'Shortcode Name'] = 'MBC'\n",
    "flows_clean2['Revenue'] =  flows_clean2['Revenue'].fillna(0)\n",
    "flows_clean2.columns = flows_clean2.columns.str.title()\n",
    "#flows_clean2.loc[flows_clean2['Affiliate_Id'].isna(),'Affiliate_Id' ]  = flows_clean2['Affiliate_Id1']\n",
    "flows_clean_no_delivered = flows_clean2.loc[flows_clean2['Affiliate_Id'].isna(), ] \n",
    "flows_clean_no_delivered['Affiliate_Id']= flows_clean_no_delivered['Affiliate_Id1']\n",
    "flows_clean_no_delivered = flows_clean_no_delivered.dropna(axis =1 , how ='all') \n",
    "flows_clean2 = flows_clean2.loc[flows_clean2['Affiliate_Id'].isna() == False, ] \n",
    "\n",
    "check_no_match = combined_csv_ss_ar_flow.merge(flows_clean1, how = 'left', right_on = ['Date','Hitpath ID','Shortcode','Affiliate_id'],left_on = ['Dash_Date_from_subid','Hitpath ID','Shortcode','affiliate_id'])\n",
    "check_no_match1 = check_no_match[check_no_match['Delivered'].isna()]\n",
    "check_no_match_with_flow = check_no_match1.dropna(axis =1 , how ='all') \n",
    "\n",
    "\n",
    "#jobs without creative or job \n",
    "combined_csv_ss_only1 = combined_csv_ss_only[~((combined_csv_ss_only['sub_id'].str.contains('AR')) |  (combined_csv_ss_only['Job_id']==0))]\n",
    "combined_csv_ss_only1 = pd.concat([combined_csv_ss_only1,check_no_match_with_flow]).reset_index(drop = True)\n",
    "combined_csv_ss_creative_na = combined_csv_ss_only1[(combined_csv_ss_only1['creative_id'].isna())| (combined_csv_ss_only1['creative_id']<100000)| ( (combined_csv_ss_only1['Job_id'].isna()))]\n",
    "\n",
    "#jobs with creative and job \n",
    "combined_csv_ss_creative_notna = combined_csv_ss_only1[~((combined_csv_ss_only1['creative_id'].isna()) | (combined_csv_ss_only1['creative_id']<100000)| ( (combined_csv_ss_only1['Job_id'].isna())))]\n",
    "\n",
    "# get revenue and delivery stats for jobs with creatives\n",
    "\"\"\" \n",
    "combined_csv_ss_creative_notna = combined_csv_ss_creative_notna.groupby(['Job_id', 'creative_id'])['amount'].sum().reset_index()\n",
    "merge_frame = creative_stats[['JobId', 'CreativeId']].append(combined_csv_ss_creative_notna[['Job_id', 'creative_id']]).drop_duplicates()\n",
    "creative_stats = creative_stats[creative_stats['Tstamp']>= '2022-11-01']\n",
    "print(combined_csv_ss_creative_notna['amount'].sum())\n",
    "combined_csv_ss_creative_notna = combined_csv_ss_creative_notna.merge(creative_stats, left_on = ['Job_id', 'creative_id'], right_on = ['JobId', 'CreativeId'], how = 'left')\n",
    "print(combined_csv_ss_creative_notna['amount'].sum())\n",
    "\"\"\" \n",
    "# get revenue and delivery stats for jobs with creatives\n",
    "combined_csv_ss_creative_notna = combined_csv_ss_creative_notna.groupby(['Job_id', 'creative_id','Jump Page Version','Offer Wall Id','Offer Type'])[['amount','Jump Page Clicks']].sum().reset_index()\n",
    "creative_stats11 = creative_stats[creative_stats['Tstamp']>= '2022-11-01']\n",
    "creative_stats11 = creative_stats11.rename(columns=({'JobId': 'Job_id', 'CreativeId':'creative_id'}))\n",
    "creative_stats11['Job_id'] = creative_stats11['Job_id'].astype('int')\n",
    "\n",
    "merge_frame = pd.concat([creative_stats11[['Job_id', 'creative_id']],combined_csv_ss_creative_notna[['Job_id', 'creative_id']]]).drop_duplicates()\n",
    "merge_frame = merge_frame.merge(combined_csv_ss_creative_notna, how = 'left')\n",
    "merge_frame = merge_frame.merge(creative_stats11, how = 'left')\n",
    "\n",
    "combined_csv_ss_creative_notna = merge_frame.fillna(0)\n",
    "combined_csv_ss_creative_notna = combined_csv_ss_creative_notna[['Offer','Job_id', 'creative_id','Jump Page Version','Offer Wall Id','Offer Type', 'CreativeName',\\\n",
    "       'Creative', 'Delivered', 'Cost', 'Optout', 'Clicks', 'amount','Jump Page Clicks']]\n",
    "combined_csv_ss_creative_notna = combined_csv_ss_creative_notna.rename(columns=({'amount': 'Revenue'}))\n",
    "#combined_csv_ss_creative_notna = combined_csv_ss_creative_notna.drop_duplicates()\n",
    "#combined_csv_ss_creative_notna = combined_csv_ss_creative_notna.groupby(['date','Offer','Job_id','CreativeId', 'CreativeName', 'Creative', 'Delivered', 'Cost', 'Unsubcount', 'Clicks']).sum('Revenue').reset_index()\n",
    "\n",
    "#get jobs information from jobs file\n",
    "jobs['job_id'] =jobs['job_id'].astype('int')\n",
    "combined_csv_ss_creative_notna = combined_csv_ss_creative_notna.merge(jobs[['job_id','job_name', 'shortcode', 'start_tstamp',\n",
    "       'end_tstamp', 'scheduled_tstamp', 'status_text', 'segments','Scheduling Time']], left_on = 'Job_id', right_on = 'job_id', how = 'left')\n",
    "combined_csv_ss_creative_notna['date'] = pd.to_datetime(combined_csv_ss_creative_notna['scheduled_tstamp']).dt.strftime(\"%Y-%m-%d\")\n",
    "combined_csv_ss_creative_notna = combined_csv_ss_creative_notna.drop(columns ='job_id')\n",
    "combined_csv_ss_creative_notna.columns = combined_csv_ss_creative_notna.columns.str.title()\n",
    "# create send strategy \n",
    "combined_csv_ss_creative_notna['Send Strategy'] = np.nan \n",
    "combined_csv_ss_creative_notna.loc[ combined_csv_ss_creative_notna['Job_Name'].str.split('_|[|]| ').str[-2:].astype('str').apply(lambda l: 'P' in l),'Send Strategy'] = 'P'\n",
    "combined_csv_ss_creative_notna.loc[ combined_csv_ss_creative_notna['Job_Name'].str.split('_|[|]| ').str[-2:].astype('str').apply(lambda l: 'T' in l),'Send Strategy'] = 'OT'\n",
    "combined_csv_ss_creative_notna.loc[ combined_csv_ss_creative_notna['Job_Name'].str.split('_|[|]| ').str[-2:].astype('str').apply(lambda l: 'OT' in l),'Send Strategy'] = 'OT'\n",
    "combined_csv_ss_creative_notna.loc[ combined_csv_ss_creative_notna['Job_Name'].str.split('_|[|]| ').str[-2:].astype('str').apply(lambda l: 'PT' in l),'Send Strategy'] = 'PT'\n",
    "combined_csv_ss_creative_notna.loc[ combined_csv_ss_creative_notna['Job_Name'].str.split('_|[|]| ').str[-2:].astype('str').apply(lambda l: 'AR' in l),'Send Strategy'] = 'AR'\n",
    "combined_csv_ss_creative_notna.loc[ combined_csv_ss_creative_notna['Job_Name'].str.split('_|[|]| ').str[-2:].astype('str').apply(lambda l: 'CT' in l),'Send Strategy'] = 'CT'\n",
    "combined_csv_ss_creative_notna.loc[ combined_csv_ss_creative_notna['Job_Name'].str.split('_|[|]| ').str[-2:].astype('str').apply(lambda l: 'MI' in l),'Send Strategy'] = 'MI'\n",
    "combined_csv_ss_creative_notna.loc[ combined_csv_ss_creative_notna['Job_Name'].str.split('_|[|]| ').str[-2:].astype('str').apply(lambda l: 'JT' in l),'Send Strategy'] = 'JT'\n",
    "# we didn't define the right send strategy for the following jobs in mamba\n",
    "combined_csv_ss_creative_notna.loc[combined_csv_ss_creative_notna['Job_Name'].str.contains('SS_FLC_PN-FC-21DC-VZN_12305_P_01Nov23', na = False),'Send Strategy'] ='JT'\n",
    "combined_csv_ss_creative_notna.loc[combined_csv_ss_creative_notna['Job_Name'].str.contains('SS_FLC_PN-FC-21DC-VZN_12305v1_CT_01Nov23', na = False),'Send Strategy'] ='JT'\n",
    "\n",
    "combined_csv_ss_creative_notna = combined_csv_ss_creative_notna.reset_index(drop=True)\n",
    "\n",
    "#get revenue stats for jobs without creatives.\n",
    "combined_csv_ss_creative_na = combined_csv_ss_creative_na[['date','Job_id', 'campaign_id','amount','sub_id','Jump Page Clicks','Jump Page Version','Offer Wall Id','Offer Type']]\n",
    "combined_csv_ss_creative_na = combined_csv_ss_creative_na.rename(columns=({'amount': 'Revenue'}))\n",
    "#get delivery and click stats for jobs without creatives from job file\n",
    " \n",
    "creative_id_na['Date'] = pd.to_datetime(creative_id_na['Tstamp']).dt.strftime(\"%Y-%m-%d\")\n",
    "creative_id_na11=creative_id_na[creative_id_na['Date'] >= '2022-11-01']\n",
    "creative_id_na11['Campaign_Id'] = creative_id_na11['Offer'].str.split(' ',expand = True)[0]\n",
    "creative_id_na11.loc[creative_id_na11['Campaign_Id'].str.isdigit() == False,'Campaign_Id' ]  = np.nan\n",
    "creative_id_na11['Campaign_Id']  = creative_id_na11['Campaign_Id'].astype(float)\n",
    "creative_id_na11['Shortcode Name'] = ''\n",
    "creative_id_na11.loc[creative_id_na11['Offer'].str.contains('CSS', na = False),'Shortcode Name'] = 'CSS'\n",
    "creative_id_na11.loc[creative_id_na11['Offer'].str.contains('HZB', na = False),'Shortcode Name'] = 'HZB'\n",
    "# the jobid didn't use in the combined_csv_ss_creative_notna\n",
    "#unjoined_creative_stats11 = creative_stats11[~creative_stats11['Job_id'].isin(combined_csv_ss_creative_notna['Job_Id'])]\n",
    "creative_stats11['Date'] = pd.to_datetime(creative_stats11['Tstamp']).dt.strftime(\"%Y-%m-%d\")\n",
    "creative_stats11['Campaign_Id'] = creative_stats11['Offer'].str.split(' ',expand = True)[0]\n",
    "creative_stats11.loc[creative_stats11['Campaign_Id'].str.isdigit() == False,'Campaign_Id' ]  = np.nan\n",
    "creative_stats11['Campaign_Id']  = creative_stats11['Campaign_Id'].astype(float)\n",
    "creative_stats11['Shortcode Name'] = ''\n",
    "creative_stats11.loc[creative_stats11['Offer'].str.contains('CSS', na = False),'Shortcode Name'] = 'CSS'\n",
    "creative_stats11.loc[creative_stats11['Offer'].str.contains('HZB', na = False),'Shortcode Name'] = 'HZB'\n",
    "\n",
    "#\n",
    "combined_csv_ss_creative_na.loc[combined_csv_ss_creative_na['sub_id'].str.contains('70610' ,na=False ),'shortcode']= '70610'\n",
    "combined_csv_ss_creative_na.loc[combined_csv_ss_creative_na['sub_id'].str.contains('44345',na=False),'shortcode']= '44345'\n",
    "combined_csv_ss_creative_na.loc[combined_csv_ss_creative_na['sub_id'].str.contains('70610' ,na=False ),'Shortcode Name']= 'CSS'\n",
    "combined_csv_ss_creative_na.loc[combined_csv_ss_creative_na['sub_id'].str.contains('44345',na=False),'Shortcode Name']= 'HZB'\n",
    "combined_csv_ss_creative_na['campaign_id'] = combined_csv_ss_creative_na['sub_id'].str.split('_',expand = True)[6]\n",
    "combined_csv_ss_creative_na.loc[combined_csv_ss_creative_na['campaign_id'].str.isdigit()== False, 'campaign_id'] = combined_csv_ss_creative_na['sub_id'].str.split('_',expand = True)[5]\n",
    "combined_csv_ss_creative_na.loc[combined_csv_ss_creative_na['campaign_id'].str.isdigit()== False, 'campaign_id'] = combined_csv_ss_creative_na['sub_id'].str.split('_',expand = True)[4]\n",
    "combined_csv_ss_creative_na['affiliate_id'] = combined_csv_ss_creative_na['sub_id'].str.extract(r'(46\\d{4})')\n",
    "combined_csv_ss_creative_na.loc[combined_csv_ss_creative_na['campaign_id'].str.isdigit() == False,'campaign_id' ]  = np.nan\n",
    "combined_csv_ss_creative_na['campaign_id'] = combined_csv_ss_creative_na['campaign_id'].astype(float)\n",
    "#combined_csv_ss_creative_na['date'] = pd.to_datetime(combined_csv_ss_creative_na['scheduled_tstamp']).dt.strftime(\"%Y-%m-%d\")\n",
    "job_11 = jobs[jobs['scheduled_tstamp'] >= '2022-11-01']\n",
    "#creative_stats_limit = creative_stats11[creative_stats11['Job_id'].isin(combined_csv_ss_creative_na['Job_id'].unique().tolist())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "442ce515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3502821.0\n",
      "3502821.0\n",
      "3502821.0\n"
     ]
    }
   ],
   "source": [
    "print(flows_clean[(flows_clean['Date']>= '2023-10-01') &(flows_clean['Date']< '2023-11-01')]['Delivered'].sum())\n",
    "print(flows_clean1[(flows_clean1['Date']>= '2023-10-01') &(flows_clean1['Date']< '2023-11-01')]['Delivered'].sum())\n",
    "print(flows_clean2[(flows_clean2['Date']>= '2023-10-01') & (flows_clean2['Date']< '2023-11-01')]['Delivered'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f98e6fad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-10T16:31:58.443111Z",
     "start_time": "2023-08-10T16:30:55.238240Z"
    }
   },
   "outputs": [],
   "source": [
    "combined_csv_ss_creative_na = combined_csv_ss_creative_na.merge(creative_id_na11[['Date','Scheduling Time','JobId','Offer','Campaign_Id', 'Segment', 'CreativeId', 'CreativeName','Shortcode Name',\n",
    "       'Creative','Delivered',  'Optout', 'Clicks', 'Cost']], left_on = ['Job_id','campaign_id','date','Shortcode Name'], right_on = ['JobId','Campaign_Id','Date','Shortcode Name'], how = 'outer', copy = False)\n",
    "combined_csv_ss_creative_na = combined_csv_ss_creative_na.merge(job_11[['job_id','job_name', 'offer', 'shortcode', 'scheduled_tstamp', 'status_text', 'segments', \n",
    "        'delivered', 'optout', 'clicks', 'cost']], left_on = ['Job_id'], right_on =['job_id'] , how = 'left' , copy = False)\n",
    "combined_csv_ss_creative_na.loc[combined_csv_ss_creative_na['date'].isna(), 'date'] = combined_csv_ss_creative_na['Date']\n",
    "combined_csv_ss_creative_na.loc[combined_csv_ss_creative_na['campaign_id'].isna(), 'campaign_id'] = combined_csv_ss_creative_na['Campaign_Id']\n",
    "combined_csv_ss_creative_na.loc[combined_csv_ss_creative_na['Offer'].isna(), 'Offer'] = combined_csv_ss_creative_na['offer']\n",
    "combined_csv_ss_creative_na.loc[combined_csv_ss_creative_na['Delivered'].isna(), 'Delivered'] = combined_csv_ss_creative_na['delivered']\n",
    "combined_csv_ss_creative_na.loc[combined_csv_ss_creative_na['Optout'].isna(), 'Optout'] = combined_csv_ss_creative_na['optout']\n",
    "combined_csv_ss_creative_na.loc[combined_csv_ss_creative_na['Clicks'].isna(), 'Clicks'] = combined_csv_ss_creative_na['clicks']\n",
    "combined_csv_ss_creative_na.loc[combined_csv_ss_creative_na['Cost'].isna(), 'Cost'] = combined_csv_ss_creative_na['cost']\n",
    "combined_csv_ss_creative_na.loc[combined_csv_ss_creative_na['Job_id'].isna(), 'Job_id'] = combined_csv_ss_creative_na['JobId']\n",
    "combined_csv_ss_creative_na = combined_csv_ss_creative_na.drop(columns = ['job_id','Campaign_Id','Date','offer', 'delivered', 'optout', 'clicks', 'cost','JobId'])\n",
    "#combined_csv_ss_creative_na = combined_csv_ss_creative_na.reset_index()\n",
    "#combined_csv_ss_creative_na['date'] = pd.to_datetime(combined_csv_ss_creative_na['scheduled_tstamp']).dt.strftime(\"%Y-%m-%d\")\n",
    "combined_csv_ss_creative_na.columns = combined_csv_ss_creative_na.columns.str.title()\n",
    "\n",
    "# create send strategy \n",
    "combined_csv_ss_creative_na['Send Strategy'] = np.nan \n",
    "combined_csv_ss_creative_na.loc[ combined_csv_ss_creative_na['Job_Name'].str.split('_|[|]| ').str[-2:].astype('str').apply(lambda l: 'P' in l),'Send Strategy'] = 'P'\n",
    "combined_csv_ss_creative_na.loc[ combined_csv_ss_creative_na['Job_Name'].str.split('_|[|]| ').str[-2:].astype('str').apply(lambda l: 'T' in l),'Send Strategy'] = 'OT'\n",
    "combined_csv_ss_creative_na.loc[ combined_csv_ss_creative_na['Job_Name'].str.split('_|[|]| ').str[-2:].astype('str').apply(lambda l: 'OT' in l),'Send Strategy'] = 'OT'\n",
    "combined_csv_ss_creative_na.loc[ combined_csv_ss_creative_na['Job_Name'].str.split('_|[|]| ').str[-2:].astype('str').apply(lambda l: 'PT' in l),'Send Strategy'] = 'PT'\n",
    "combined_csv_ss_creative_na.loc[ combined_csv_ss_creative_na['Job_Name'].str.split('_|[|]| ').str[-2:].astype('str').apply(lambda l: 'AR' in l),'Send Strategy'] = 'AR'\n",
    "combined_csv_ss_creative_na.loc[ combined_csv_ss_creative_na['Job_Name'].str.split('_|[|]| ').str[-2:].astype('str').apply(lambda l: 'CT' in l),'Send Strategy'] = 'CT'\n",
    "combined_csv_ss_creative_na.loc[ combined_csv_ss_creative_na['Job_Name'].str.split('_|[|]| ').str[-2:].astype('str').apply(lambda l: 'MI' in l),'Send Strategy'] = 'MI'\n",
    "combined_csv_ss_creative_na.loc[ combined_csv_ss_creative_na['Job_Name'].str.split('_|[|]| ').str[-2:].astype('str').apply(lambda l: 'JT' in l),'Send Strategy'] = 'JT'\n",
    "combined_csv_ss_creative_na.loc[combined_csv_ss_creative_na['Job_Id'] == 0, 'Send Strategy'] = 'AR'\n",
    "combined_csv_ss_creative_na.loc[combined_csv_ss_creative_na['Job_Id'] == 0, 'Revenue Source'] =  'Short Code - SS Jobs'\n",
    "#combined_csv_ss_creative_na = combined_csv_ss_creative_na.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a414a442",
   "metadata": {},
   "source": [
    "### Other Revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79f3c129",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_12408\\2575836291.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  combined_csv_ss_exclude['date'] = pd.to_datetime(combined_csv_ss_exclude['date'])\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_12408\\2575836291.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  combined_csv_push['date'] = pd.to_datetime(combined_csv_push['date'])\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_12408\\2575836291.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  combined_csv_w1['Send Strategy'] = 'AR'\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_12408\\2575836291.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  combined_csv_w1['Revenue Source'] = 'Short Code - SS Jobs'\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_12408\\2575836291.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  combined_csv_w1['date'] = pd.to_datetime(combined_csv_w1['date'])\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_12408\\2575836291.py:39: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  combined_csv_ss_last =  combined_csv_ss_rest[ ((combined_csv['affiliate_name'].str.lower().str.contains(\"push\", na = False) == False)) & (combined_csv_ss_rest['subid_2'].str.lower().str.contains(\"w1\", na = False) == False)]\n"
     ]
    }
   ],
   "source": [
    "#case #3: revenue for email \n",
    "combined_csv_ss_exclude = combined_csv\n",
    "combined_csv_ss_exclude['affiliate_id'] = combined_csv_ss_exclude['affiliate_id'].astype(str).str.split(\".\",expand = True)[0]\n",
    "combined_csv_ss_exclude['site_id'] = combined_csv_ss_exclude['site_id'].astype(str).str.split(\".\",expand = True)[0]\n",
    "\n",
    "combined_csv_ss_exclude =  combined_csv_ss_exclude.loc[(combined_csv_ss_exclude['affiliate_id'].isin(email_pubid)) | (combined_csv_ss_exclude['site_id'].isin(email_pubid)),] \n",
    "combined_csv_ss_exclude['date'] = pd.to_datetime(combined_csv_ss_exclude['date'])\n",
    "combined_csv_ss_exclude = combined_csv_ss_exclude[[ 'amount', 'date']]\n",
    "combined_csv_ss_exclude = combined_csv_ss_exclude.rename(columns=({'amount': 'Revenue','date': 'Date'}))\n",
    "combined_csv_ss_exclude_11 = combined_csv_ss_exclude[combined_csv_ss_exclude['Date']>='2022-11-01'].reset_index(drop=True)\n",
    "# case #4: push revenue \n",
    "email_pubid_int = emit['Revenue Pub ID'].unique().tolist()\n",
    "combined_csv_ss_rest =  combined_csv[(combined_csv['first_split']!='SS') & (combined_csv['first_split']!='SMS') & (combined_csv['affiliate_id'].astype(float).isin(email_pubid_int)== False) & (combined_csv['site_id'].astype(float).isin(email_pubid_int)== False) & (combined_csv['first_split']!='MD') &  (combined_csv['first_split']!='TB')  ] \n",
    "#combined_csv_ss_rest =  combined_csv[(combined_csv['first_split']!='SS') & (combined_csv['first_split']!='SMS') & (combined_csv['subid_2'].str.contains('FLC|MBC',na = False)== False)& (combined_csv['affiliate_id'].astype(float).isin(email_pubid_int)== False) & (combined_csv['site_id'].astype(float).isin(email_pubid_int)== False) & (combined_csv['first_split']!='MD')   ] \n",
    "\n",
    "combined_csv_push = combined_csv_ss_rest.loc[combined_csv_ss_rest['affiliate_name'].str.lower().str.contains(\"push\", na = False),]\n",
    "combined_csv_push['date'] = pd.to_datetime(combined_csv_push['date'])\n",
    "combined_csv_push = combined_csv_push[[ 'amount', 'date']]\n",
    "combined_csv_push = combined_csv_push.rename(columns=({'amount': 'Revenue','date': 'Date'}))\n",
    "combined_csv_push_11 =  combined_csv_push[combined_csv_push['Date']>='2022-11-01'].reset_index(drop=True)\n",
    "\n",
    "# case #6: previos AR job\n",
    "combined_csv_w1= combined_csv_ss_rest.loc[combined_csv_ss_rest['subid_2'].str.lower().str.contains(\"_w1\", na = False),]\n",
    "combined_csv_w1['Send Strategy'] = 'AR'\n",
    "combined_csv_w1['Revenue Source'] = 'Short Code - SS Jobs'\n",
    "combined_csv_w1['date'] = pd.to_datetime(combined_csv_w1['date'])\n",
    "combined_csv_w1 = combined_csv_w1.rename(columns=({'amount': 'Revenue','date': 'Date','affiliate_id':'Affiliate_Id', 'subid_2':'Sub_Id'}))\n",
    "combined_csv_w1['Hitpath Id'] = combined_csv_w1['campaign_id'].astype('str').str.split('.',expand = True)[0]\n",
    "# split subid_2 with \"_\"\n",
    "combined_csv_w1['Shortcode Name'] = np.nan\n",
    "combined_csv_w1.loc[combined_csv_w1['Date']>= '2022-11-01','Shortcode Name'] = combined_csv_w1['Sub_Id'].str.split(\"_\",expand = True)[2]\n",
    "## when shortcode name is 460654\n",
    "combined_csv_w1.loc[combined_csv_w1['Shortcode Name'] == '460654', 'Shortcode Name' ] = np.nan\n",
    "## when shortcode name is PLV, it's long code \n",
    "combined_csv_w1 = combined_csv_w1[['Revenue', 'Date', 'Send Strategy', 'Revenue Source', 'Hitpath Id', 'Affiliate_Id', 'Sub_Id','Shortcode Name']]\n",
    "combined_csv_w1_11 = combined_csv_w1[combined_csv_w1['Date']>='2022-11-01']\n",
    "# case #5: revenue the rest of the revenue. \n",
    "\n",
    "combined_csv_ss_last =  combined_csv_ss_rest[ ((combined_csv['affiliate_name'].str.lower().str.contains(\"push\", na = False) == False)) & (combined_csv_ss_rest['subid_2'].str.lower().str.contains(\"w1\", na = False) == False)] \n",
    "combined_csv_ss_last_11 = combined_csv_ss_last[combined_csv_ss_last['date']>='2022-11-01'].reset_index(drop=True)\n",
    "\n",
    "# identify Revenue Sourc\n",
    "combined_csv_ss_creative_na['Revenue Source'] = 'Short Code - SS Jobs'\n",
    "combined_csv_ss_creative_notna['Revenue Source'] = 'Short Code - SS Jobs'\n",
    "combined_csv_ss_exclude['Revenue Source'] = 'Email'\n",
    "combined_csv_push['Revenue Source'] = 'Push'\n",
    "historic_data['Revenue Source'] = 'Short Code - SS Jobs'\n",
    "historic_data['Send Strategy'] = np.nan\n",
    "\n",
    "#combined_csv_ss_creative_na = combined_csv_ss_creative_na.drop_duplicates()\n",
    "SS_New_data = pd.concat([combined_csv_ss_creative_notna,combined_csv_ss_creative_na,flows_clean2,combined_csv_ss_exclude, combined_csv_push,combined_csv_w1], axis=0)\n",
    "SS_New_data['Ecpm'] = SS_New_data['Revenue'] * 1000/SS_New_data['Delivered']\n",
    "SS_New_data['Roi'] = SS_New_data['Cost'] - SS_New_data['Revenue']\n",
    "#SS_New_data['Sent'] = SS_New_data['Total']\n",
    "\n",
    "#combining historic data(before Nov 1) with new data to get SS full data.\n",
    "#SS_New_data = SS_New_data[historic_data.columns]\n",
    "\n",
    "\n",
    "SS_Full_data = pd.concat([SS_New_data,historic_data], axis = 0)\n",
    "SS_Full_data['Date'] = pd.to_datetime(SS_Full_data['Date'])\n",
    "SS_Full_data = SS_Full_data.sort_values('Date', ascending=False)\n",
    "#SS_Full_data.loc[(SS_Full_data['Job_Id'].isna()) & (SS_Full_data['Revenue Source']==  'Short Code - SS Jobs')  ,'Revenue Source'] = 'Short Code - Opt In'\n",
    "SS_Full_data.loc[ (SS_Full_data['Job_Id'].isna()) & (SS_Full_data['Sub_Id'].str.contains('AR', na = False)) ,'Revenue Source'] =  'Short Code - SS Flow'\n",
    "SS_Full_data.loc[ (SS_Full_data['Job_Id'].isna())&  (SS_Full_data['Sub_Id'].str.contains('AR', na = False)) ,'Send Strategy'] = 'AR'\n",
    "SS_Full_data.loc[(SS_Full_data['Job_Id'].isna()) & (SS_Full_data['Revenue Source']==  'Short Code - Opt In') ,'Send Strategy'] = 'Opt In'\n",
    "SS_Full_data.loc[(SS_Full_data['Job_Id']==0) & (SS_Full_data['Revenue Source']==  'Short Code - SS Jobs') & (SS_Full_data['Send Strategy'].isna()) ,'Send Strategy'] = 'AR'\n",
    "SS_Full_data.to_csv(localfolder + 'SS_Fulldata.csv', index =False)  \n",
    "\n",
    "######## SS END ########\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a389806",
   "metadata": {},
   "source": [
    "## long code MP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54acff91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-10T16:31:59.037237Z",
     "start_time": "2023-08-10T16:31:58.487041Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_12408\\3977380537.py:7: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  MP_campaigns['ACTDATE']= pd.to_datetime(MP_campaigns['ACTDATE'])\n"
     ]
    }
   ],
   "source": [
    "######## MP BEGIN ########\n",
    "#Read MP file and clean. All stats present in MP_Campaigns file.\n",
    "\n",
    "MP_campaigns = pd.read_csv(localfolder + 'SMS_SC_MP_Campaigns.csv')\n",
    "MP_campaigns = MP_campaigns[~MP_campaigns['ACTDATE'].isna()]\n",
    "MP_campaigns = MP_campaigns[~MP_campaigns['ACTDATE'].isin(['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])]\n",
    "MP_campaigns['ACTDATE']= pd.to_datetime(MP_campaigns['ACTDATE'])\n",
    "#a = MP_campaigns.isna().sum()\n",
    "drop_columns = ['Daily Opt Out', 'Unnamed: 28', 'Unnamed: 53', 'Reference', 'c1NEW' , 'Unnamed: 61', 'Unnamed: 62', 'Unnamed: 64', 'Unnamed: 72', 'Unnamed: 73']\n",
    "MP_campaigns.drop(columns=drop_columns, inplace=True, axis=1)\n",
    "MP_campaigns= MP_campaigns[~MP_campaigns['Done.'].isna()]\n",
    "MP_campaigns.columns = MP_campaigns.columns.str.strip()\n",
    "\n",
    "# remove % and $ symbold\n",
    "MP_campaigns['REV'] = MP_campaigns['REV'].str.strip()\n",
    "MP_campaigns['REV'] = MP_campaigns['REV'].replace('$ -', np.nan)\n",
    "MP_campaigns['REV'] = MP_campaigns['REV'].str.replace('$','')\n",
    "MP_campaigns['REV'] = MP_campaigns['REV'].str.replace(',','')\n",
    "MP_campaigns['REV'] = MP_campaigns['REV'].astype('float')\n",
    "\n",
    "MP_campaigns['COST'] = MP_campaigns['COST'].str.strip()\n",
    "MP_campaigns['COST'] = MP_campaigns['COST'].replace('$ -', np.nan)\n",
    "MP_campaigns['COST'] = MP_campaigns['COST'].str.replace('$','')\n",
    "MP_campaigns['COST'] = MP_campaigns['COST'].astype('float')\n",
    "\n",
    "MP_campaigns['eCPM'] = MP_campaigns['eCPM'].str.strip()\n",
    "MP_campaigns['eCPM'] = MP_campaigns['eCPM'].replace('$ -', np.nan)\n",
    "MP_campaigns['eCPM'] = MP_campaigns['eCPM'].str.replace('$','')\n",
    "MP_campaigns['eCPM'] = MP_campaigns['eCPM'].astype('float')\n",
    "\n",
    "MP_campaigns['gPROFIT'] = MP_campaigns['gPROFIT'].str.strip()\n",
    "MP_campaigns['gPROFIT'] = MP_campaigns['gPROFIT'].str.replace('$','')\n",
    "MP_campaigns['gPROFIT'] = MP_campaigns['gPROFIT'].replace('-',np.nan)\n",
    "MP_campaigns['gPROFIT'] = MP_campaigns['gPROFIT'].replace(' -',np.nan)\n",
    "MP_campaigns['gPROFIT'] = MP_campaigns['gPROFIT'].astype(str).str.replace('\\((.*)\\)', '-\\\\1')\n",
    "MP_campaigns['gPROFIT'] = MP_campaigns['gPROFIT'].str.replace(',','')\n",
    "MP_campaigns['gPROFIT'] = MP_campaigns['gPROFIT'].astype('float')\n",
    "\n",
    "MP_campaigns['CLICK %'] = MP_campaigns['CLICK %'].str.strip()\n",
    "MP_campaigns['CLICK %'] = MP_campaigns['CLICK %'].str.replace('%','')\n",
    "MP_campaigns['CLICK %'] = MP_campaigns['CLICK %'].astype('float')\n",
    "\n",
    "MP_campaigns['gMARGIN'] = MP_campaigns['gMARGIN'].str.strip()\n",
    "MP_campaigns['gMARGIN'] = MP_campaigns['gMARGIN'].str.replace('%','')\n",
    "MP_campaigns['gMARGIN'] = MP_campaigns['gMARGIN'].astype('float')\n",
    "\n",
    "MP_campaigns.to_csv(localfolder + 'MP_data.csv', index =False)  \n",
    "\n",
    "###### MP ENDS #######\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb66254",
   "metadata": {},
   "source": [
    "## Combine all data from all sources "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a37a9e56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-10T16:34:25.685817Z",
     "start_time": "2023-08-10T16:31:59.036893Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_12408\\3001138842.py:3: DtypeWarning: Columns (0,4,5,6,7,14,16,17,18,19,20,21,23,26,28,33,34,35,37,38,41) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  SS_data = pd.read_csv(localfolder + 'SS_Fulldata.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6562419.730989001\n",
      "6562419.730989001\n"
     ]
    }
   ],
   "source": [
    "#4 merge SS and MP: not possible as there are columns in MP not in SS.\n",
    "LC_data = pd.read_csv(localfolder + 'SMS_LC_Campaigns_clean.csv')\n",
    "SS_data = pd.read_csv(localfolder + 'SS_Fulldata.csv')\n",
    "\n",
    "lc_df_full = lc_df_full.reset_index(drop=True)\n",
    "LC_data = LC_data.reset_index(drop=True)\n",
    "LC_data = pd.concat([lc_df_full,LC_data], axis=0, ignore_index=True)\n",
    "LC_data.loc[LC_data['Affiliate_Id'].isna(),'Affiliate_Id'] = LC_data['pubID']\n",
    "LC_data.loc[LC_data['Hitpath_Offer_ID'].isna(),'Hitpath_Offer_ID' ] = LC_data['sid']\n",
    "LC_data['Affiliate_Id'] = LC_data['Affiliate_Id'].astype(str).str.split('.',expand = True)[0]\n",
    "LC_data['Hitpath_Offer_ID'] = LC_data['Hitpath_Offer_ID'].astype(str).str.split('.',expand = True)[0]\n",
    "SS_data['Code_Type'] = 'Short Code'\n",
    "LC_data['Code_Type'] = 'Long Code'\n",
    "SS_data.loc[SS_data['Revenue Source'] == 'Email', 'Code_Type'] = 'Email'\n",
    "SS_data.loc[SS_data['Revenue Source'] == 'Push', 'Code_Type'] = 'Push'\n",
    "SS_data.loc[SS_data['Shortcode'].astype(str).str.replace('.0','').str.len() ==11, 'Code_Type'] = 'Toll Free'\n",
    "SS_data.loc[SS_data['Shortcode'].astype(str).str.replace('.0','').str.len() ==11, 'Revenue Source'] = 'Toll Free'\n",
    "SS_data['Hitpath_Offer_ID'] = SS_data['Offer'].astype('str').str.extract(r'\\b(\\d{4,5}).*')\n",
    "SS_data.loc[SS_data['Hitpath_Offer_ID'].isna(),'Hitpath_Offer_ID'] = SS_data['Hitpath Id'].astype(str).str.split('.',expand = True)[0]\n",
    "SS_data['Affiliate_Id'] = SS_data['Affiliate_Id'].astype(str).str.split('.',expand = True)[0]\n",
    "#LC_data= LC_data.rename(columns=({'eCPM' : 'Ecpm', 'Clicked': 'Clicks'}))\n",
    "\n",
    "\n",
    "#merge short code and longcode\n",
    "merged_data = pd.concat([SS_data,LC_data], axis=0, ignore_index=True)\n",
    "\n",
    "# merge offer Wall \n",
    "\n",
    "merged_data['Hitpath_Offer_ID'] = merged_data['Hitpath_Offer_ID'].astype(str).str.split('.',expand = True)[0]\n",
    "#merged_data['Hitpath_Offer_ID'] = merged_data['Offer'].astype('str').str.extract(r'\\b(\\d{4,5})\\b')\n",
    "merged_data.loc[merged_data['Revenue Source']=='Short Code - SS Flow','Hitpath_Offer_ID'] =merged_data['Hitpath Id'].astype('str').str.split('.',expand = True)[0]\n",
    "merged_data['Offer Wall Id'] = merged_data['Offer Wall Id'].astype('str').str.split('.',expand = True)[0]\n",
    "merged_data.loc[ (merged_data['Offer Wall Id'].isin(offer_wall_id)),'Hitpath_Offer_ID'] = merged_data['Offer Wall Id']\n",
    "merged_data['Offer Type'] =  'Single Offer'\n",
    "merged_data.loc[ (merged_data['Offer Wall Id'].isin(offer_wall_id)),'Offer Type'] = 'Offer Wall'\n",
    "merged_data['DP.SV'] = merged_data['Dp.Ds Or Dp.Sv']\n",
    "merged_data.loc[merged_data['DP.SV'].isnull(), 'DP.SV'] =  merged_data['Segments'].str.split('_',expand = True)[1]\n",
    "dict_publisher = {'DP.DS or DP.sV':['WWM.YFA','ZM.PL'], 'PUBID': ['461680','461681']}\n",
    "dict_publisher = pd.DataFrame(dict_publisher)\n",
    "new_publisher = pd.concat([publisher,dict_publisher])\n",
    "new_publisher = new_publisher.reset_index(drop=True)\n",
    "# publisher drop na rows\n",
    "new_publisher = new_publisher[~new_publisher['DP.DS or DP.sV'].isna()]\n",
    "new_publisher = new_publisher[['DP.DS or DP.sV','PUBID']]\n",
    "new_publisher['PUBID'] = new_publisher['PUBID'].astype(str).str.split('.',expand = True)[0]\n",
    "merged_data = merged_data.merge(new_publisher[['DP.DS or DP.sV','PUBID']], left_on ='DP.SV', right_on = 'DP.DS or DP.sV', how = 'left' )\n",
    "merged_data.loc[(merged_data['Affiliate_Id'].isna()) |  (merged_data['Affiliate_Id']=='nan'), 'Affiliate_Id'] = merged_data['PUBID']\n",
    "print(merged_data['Revenue'].sum())\n",
    "publisher_copy = publisher\n",
    "publisher_copy['DP.DS or DP.sV'] = publisher_copy['DP.DS or DP.sV'].str.replace('WWM.YFA','WWM.YFA.2')\n",
    "publisher_copy['DP.DS or DP.sV'] = publisher_copy['DP.DS or DP.sV'].str.replace('ZM.PL','ZM.PL.2')\n",
    "publisher_copy = publisher_copy[['DP.DS or DP.sV','PUBID']].drop_duplicates()\n",
    "merged_data = merged_data.merge(publisher_copy, left_on ='Affiliate_Id', right_on = 'PUBID', how = 'left')\n",
    "#merged_data = merged_data.merge(new_publisher[['DP.DS or DP.sV','PUBID']], left_on ='Affiliate_Id', right_on = 'PUBID', how = 'left' )\n",
    "merged_data.loc[(merged_data['DP.SV'].isna()) |  (merged_data['DP.SV']=='nan'),'DP.SV'] = merged_data['DP.DS or DP.sV_y']\n",
    "print(merged_data['Revenue'].sum())\n",
    "merged_data['Affiliate_Id'] = merged_data['Affiliate_Id'].astype(str).str.split('.',expand = True)[0]\n",
    "merged_data['DP&Pub'] = merged_data['DP.SV']+'_'+ merged_data['Affiliate_Id']\n",
    "\n",
    "\n",
    "merged_data = merged_data[['Date','Scheduling Time', 'Offer','Hitpath_Offer_ID','DP.SV','Affiliate_Id', 'DP&Pub','Job_Id', 'Job_Name','Creative_Id','Creativename','Creative','Send Strategy', 'Shortcode', 'Start_Tstamp','Sent', 'Segments', 'Revenue','Jump Page Clicks', 'Delivered', 'Not_Delivered', 'Optout', 'Clicks',\n",
    "       'Cost', 'Ecpm', 'Time', 'Publisher', 'Campaign', 'Route',  'Carrier', 'Dataset', 'Message',\n",
    "       'Responder Template', 'Keyword', 'Responder', 'Router Domain Name' , 'c1', 'Responded', 'Response Rate', 'CTR',\n",
    "        'Gross Profit' , 'Gross Margin', 'RPU' ,'Provider', 'Code_Type','Revenue Source',\n",
    "     'Ar Day','Sub_Id','Campaign_Id','Roi','Shortcode Name','Total','Jump Page Version','Offer Wall Id','Offer Type']]\n",
    "merged_data.loc[merged_data['Clicks'] == '','Clicks'] = np.nan\n",
    "merged_data['Clicks'] = merged_data['Clicks'].astype(float)\n",
    "merged_data.loc[merged_data['Shortcode'] == 51797, 'Shortcode Name'] = 'FLC'\n",
    "merged_data.loc[merged_data['Shortcode'] == 70610, 'Shortcode Name'] = 'CSS'\n",
    "merged_data.loc[merged_data['Shortcode'] == 44345, 'Shortcode Name'] = 'HZB'\n",
    "merged_data.loc[merged_data['Shortcode'] == 80837, 'Shortcode Name'] = 'MBC'\n",
    "merged_data.loc[merged_data['Shortcode'] == 31232, 'Shortcode Name'] = 'DSS'\n",
    "merged_data.loc[merged_data['Shortcode'] == 61659, 'Shortcode Name'] = 'SVT'\n",
    "merged_data.loc[merged_data['Shortcode'] == 79743, 'Shortcode Name'] = 'UAA'\n",
    "merged_data.loc[merged_data['Shortcode'] == 18333164159, 'Shortcode Name'] = 'PRC'\n",
    "merged_data.loc[merged_data['Shortcode'] == 18333641722, 'Shortcode Name'] = 'UAATF'\n",
    "merged_data.loc[merged_data['Shortcode'] == 15612023538, 'Shortcode Name'] = 'A4FLC'\n",
    "merged_data.loc[merged_data['Shortcode'] == 18332686782, 'Shortcode Name'] = 'MFATF'\n",
    "merged_data.loc[merged_data['Shortcode Name'] == 'FLC', 'Shortcode'] = '51797'\n",
    "merged_data.loc[merged_data['Shortcode Name'] == 'CSS', 'Shortcode'] = '70610'\n",
    "merged_data.loc[merged_data['Shortcode Name'] == 'HZB', 'Shortcode'] = '44345'\n",
    "merged_data.loc[merged_data['Shortcode Name'] == 'MBC', 'Shortcode'] = '80837'\n",
    "merged_data.loc[merged_data['Shortcode Name'] == 'DSS', 'Shortcode'] = '31232'\n",
    "merged_data.loc[merged_data['Shortcode Name'] == 'SVT', 'Shortcode'] = '61659'\n",
    "merged_data.loc[merged_data['Shortcode Name'] == 'UAA', 'Shortcode'] = '79743'\n",
    "merged_data.loc[merged_data['Shortcode Name'] == 'PRC', 'Shortcode'] = '18333164159'\n",
    "merged_data.loc[merged_data['Shortcode Name'] == 'UAATF', 'Shortcode'] = '18333641722'\n",
    "merged_data.loc[merged_data['Shortcode Name'] == 'A4FLC', 'Shortcode'] = '15612023538'\n",
    "merged_data.loc[merged_data['Shortcode Name'] == 'MFATF', 'Shortcode'] = '18332686782'\n",
    "merged_data.loc[merged_data['Delivered'] == '','Delivered'] = np.nan\n",
    "merged_data['Delivered'] =  merged_data['Delivered'].astype(float)\n",
    "merged_data.loc[(merged_data['Shortcode Name'] == 'HZB'), 'Cost' ] = 0.007 * merged_data['Delivered']\n",
    "merged_data.loc[(merged_data['Shortcode Name'] == 'MBC') | (merged_data['Shortcode Name'] == 'FLC'),'Cost']  =  0.00563 *  merged_data['Delivered'] +  merged_data['Cost']\n",
    "merged_data.loc[(merged_data['Code_Type'] == 'Short Code')&((merged_data['Shortcode Name'] == 'UAA') | (merged_data['Shortcode Name'] == 'SVT') | (merged_data['Shortcode Name'] == 'DSS')), 'Cost']  =  0.00504 *  merged_data['Delivered'] \n",
    "# we need to add sending cost for MFATF and A4FLC\n",
    "Offer_name[ 'campaign_id'] = Offer_name[ 'campaign_id'].astype('str').str.split('.',expand = True)[0]\n",
    "merged_data = merged_data.merge(Offer_name,left_on = 'Hitpath_Offer_ID',right_on = 'campaign_id',how = 'left')\n",
    "merged_data['Job_Id']  = merged_data['Job_Id'].astype(str).str.split('.',expand = True)[0].str.strip()\n",
    "merged_data['Date'] = pd.to_datetime(merged_data['Date'],format ='mixed').dt.strftime(\"%Y-%m-%d\")\n",
    "merged_data['shortcode_DP.SV'] = merged_data['Shortcode Name'] + \"_\" + merged_data['DP.SV']\n",
    "merged_data.loc[merged_data['shortcode_DP.SV']=='_', 'shortcode_DP.SV'] = np.nan\n",
    "# calculate opportunity cost\n",
    "merged_data['Opportunity Cost Send Strategy'] =  True\n",
    "merged_data.loc[merged_data['Send Strategy'].isin(['Null','Opt In',np.nan]), 'Opportunity Cost Send Strategy'] = False\n",
    "merged_data = merged_data.sort_values('Date')\n",
    "temp1= merged_data.groupby(['shortcode_DP.SV','Opportunity Cost Send Strategy','Date']).agg({'Revenue':'sum','Delivered':'sum'}).reset_index()\n",
    "temp1[['rolling Revenue','rolling Delivered']] = temp1.groupby('shortcode_DP.SV').shift(1).rolling(30, min_periods=5)[['Revenue','Delivered']].sum().reset_index(drop=True)\n",
    "temp1['Dataset_Agg_30D_eCPM'] = temp1['rolling Revenue'] * 1000/ temp1['rolling Delivered']\n",
    "dataset_agg_eCPM =  temp1[['shortcode_DP.SV','Date','Opportunity Cost Send Strategy','Dataset_Agg_30D_eCPM']]\n",
    "merged_data = merged_data.merge(dataset_agg_eCPM, how = 'left')\n",
    "merged_data['Opportunity Cost'] = merged_data['Revenue'] - merged_data['Dataset_Agg_30D_eCPM'] * merged_data['Delivered'] /1000 \n",
    "merged_data = merged_data.sort_values(by = 'Date',ascending = False)\n",
    "merged_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "merged_data.to_csv(localfolder + 'SS_LC_merged_data.csv', index =False)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66abefa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DP.DS or DP.sV</th>\n",
       "      <th>PUBID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I.RC</td>\n",
       "      <td>460654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I.CC</td>\n",
       "      <td>460918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SPK.CR</td>\n",
       "      <td>460921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SPK.SWP</td>\n",
       "      <td>460931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AI.CC</td>\n",
       "      <td>460939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>EDM.247L</td>\n",
       "      <td>461227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>FSM.YS</td>\n",
       "      <td>461242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I.FRG</td>\n",
       "      <td>461412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PA.PS</td>\n",
       "      <td>461247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RHD.CC</td>\n",
       "      <td>461263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NPD.RTO</td>\n",
       "      <td>461313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>JET.ONP</td>\n",
       "      <td>461705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>JET.NTC</td>\n",
       "      <td>461704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>CM.OSR</td>\n",
       "      <td>461452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>PN.SWP</td>\n",
       "      <td>461500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>PN.FC</td>\n",
       "      <td>461653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>WWM.YFA.2</td>\n",
       "      <td>461680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ZM.PL.2</td>\n",
       "      <td>461681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>TLG.PL</td>\n",
       "      <td>461768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>AL.PL</td>\n",
       "      <td>461794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>AL.PL.2</td>\n",
       "      <td>461795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>AL.PL.3</td>\n",
       "      <td>461838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>AMD.PL</td>\n",
       "      <td>461810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>I.RTOD</td>\n",
       "      <td>461800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>I.LD</td>\n",
       "      <td>461801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>I.FD</td>\n",
       "      <td>461802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>I.NFD</td>\n",
       "      <td>461803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>SPK.CR2</td>\n",
       "      <td>461842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>SPK.SWP2</td>\n",
       "      <td>461843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>DMS.SJ</td>\n",
       "      <td>461840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>B2.F</td>\n",
       "      <td>461123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>JET.ZTA</td>\n",
       "      <td>461835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>I.A4F</td>\n",
       "      <td>461870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>I.MFA</td>\n",
       "      <td>461871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>BB.CC</td>\n",
       "      <td>461881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>DOT.PL</td>\n",
       "      <td>461039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>SM.SRV</td>\n",
       "      <td>461896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>WWM.YFA</td>\n",
       "      <td>461680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>ZM.PL</td>\n",
       "      <td>461681</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DP.DS or DP.sV   PUBID\n",
       "0            I.RC  460654\n",
       "1            I.CC  460918\n",
       "2          SPK.CR  460921\n",
       "3         SPK.SWP  460931\n",
       "4           AI.CC  460939\n",
       "5        EDM.247L  461227\n",
       "6          FSM.YS  461242\n",
       "7           I.FRG  461412\n",
       "8           PA.PS  461247\n",
       "9          RHD.CC  461263\n",
       "10        NPD.RTO  461313\n",
       "11        JET.ONP  461705\n",
       "12        JET.NTC  461704\n",
       "13         CM.OSR  461452\n",
       "14         PN.SWP  461500\n",
       "15          PN.FC  461653\n",
       "16      WWM.YFA.2  461680\n",
       "17        ZM.PL.2  461681\n",
       "18         TLG.PL  461768\n",
       "19          AL.PL  461794\n",
       "20        AL.PL.2  461795\n",
       "21        AL.PL.3  461838\n",
       "22         AMD.PL  461810\n",
       "23                       \n",
       "24         I.RTOD  461800\n",
       "25           I.LD  461801\n",
       "26           I.FD  461802\n",
       "27          I.NFD  461803\n",
       "28        SPK.CR2  461842\n",
       "29       SPK.SWP2  461843\n",
       "30         DMS.SJ  461840\n",
       "31           B2.F  461123\n",
       "32        JET.ZTA  461835\n",
       "33          I.A4F  461870\n",
       "34          I.MFA  461871\n",
       "35          BB.CC  461881\n",
       "36         DOT.PL  461039\n",
       "37         SM.SRV  461896\n",
       "38        WWM.YFA  461680\n",
       "39          ZM.PL  461681"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_publisher[['DP.DS or DP.sV','PUBID']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4057e786",
   "metadata": {},
   "source": [
    "# P&L DATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9416546e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-10T16:34:25.691033Z",
     "start_time": "2023-08-10T16:34:22.182771Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_12408\\4062370451.py:1: DtypeWarning: Columns (1,2,3,4,6,8,9,10,11,12,14,16,25,26,27,28,29,30,31,32,33,34,35,36,43,47,50,53,56,57) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  merged_data = pd.read_csv(localfolder + 'SS_LC_merged_data.csv')\n"
     ]
    }
   ],
   "source": [
    "merged_data = pd.read_csv(localfolder + 'SS_LC_merged_data.csv')\n",
    "engagement_daily = merged_data.groupby(['Date','Affiliate_Id'])[['Delivered','Clicks','Jump Page Clicks','Optout','Cost']].sum()\n",
    "engagement_daily = engagement_daily[~np.all(engagement_daily==0,axis = 1)]\n",
    "engagement_daily = engagement_daily.reset_index()\n",
    "daily_revenue_gpby = daily_revenue_gpby.rename(columns = {'amount':'Revenue','affiliate_id': 'Affiliate_Id', 'date': 'Date'})\n",
    "#Pl_data = daily_revenue_gpby.merge(engagement_daily, left_on =['affiliate_id','date'] ,right_on = ['Affiliate_Id','Date'],copy = False, how = 'outer')\n",
    "daily_revenue_gpby['Date'] = pd.to_datetime(daily_revenue_gpby['Date'])\n",
    "engagement_daily['Date'] =  pd.to_datetime(engagement_daily['Date'])\n",
    "Pl_data = pd.concat([daily_revenue_gpby,engagement_daily], axis=0, ignore_index=True)\n",
    "Pl_data['PUBID'] =Pl_data['Affiliate_Id'].astype('str').str.split(\".\",expand = True)[0]\n",
    "Pl_data = Pl_data.fillna(0)\n",
    "Pl_data = Pl_data[[ 'Date', 'PUBID','Revenue', 'Delivered', 'Clicks',\\\n",
    "      'Jump Page Clicks', 'Optout','Cost']]\n",
    "Pl_data = Pl_data.groupby(['PUBID','Date']).sum().reset_index()\n",
    "#Pl_data.to_csv(localfolder+'p&l_data.csv',index =False)\n",
    "\n",
    "api_key = pd.read_csv(localfolder+'SMS_SC_SS_Apikey.csv')\n",
    "#publisher1 = publisher\n",
    "#publisher1['DP.DS or DP.sV'] = publisher1['DP.DS or DP.sV'].str.replace('WWM.YFA.2','WWM.YFA')\n",
    "#publisher1['DP.DS or DP.sV'] = publisher1['DP.DS or DP.sV'].str.replace('ZM.PL.2','ZM.PL')\n",
    "api_key_sms = api_key.merge(new_publisher, left_on = 'DP.SV',right_on ='DP.DS or DP.sV',how = 'inner' )\n",
    "api_key_sms = api_key_sms[['DP.SV','PUBID','Date', 'AcceptedTotal', 'AcceptedNew',\\\n",
    "       'AcceptedDuplicate', 'RejectedTotal', 'RejectedMobile',\\\n",
    "       'RejectedBlacklist', 'RejectedData', 'CostData', 'CostChecks',\\\n",
    "       'CostTotal']]\n",
    "api_key_sms['Date'] = pd.to_datetime(api_key_sms['Date'])\n",
    "Pl_data['Date'] =  pd.to_datetime(Pl_data['Date'] )\n",
    "Pl_data['PUBID']  = Pl_data['PUBID'].astype('str').str.split(\".\",expand = True)[0]\n",
    "api_key_sms['PUBID']  = api_key_sms['PUBID'].astype('str').str.split(\".\",expand = True)[0]\n",
    "rev_accept_df = pd.concat([Pl_data, api_key_sms], axis=0, ignore_index=True)\n",
    "rev_accept_df = rev_accept_df.fillna(0)\n",
    "rev_accept_df['PUBID']  = rev_accept_df['PUBID'].astype('str').str.split(\".\",expand = True)[0]\n",
    "rev_accept_df = rev_accept_df.drop(columns = 'DP.SV')\n",
    "rev_accept_df = rev_accept_df.groupby(['PUBID','Date']).sum().reset_index()\n",
    "\n",
    "\n",
    "sms_post = pd.read_csv(localfolder + 'SMSposts.csv')\n",
    "sms_post['Date'] = pd.to_datetime(sms_post['Dates'])\n",
    "sms_post['PUBID'] = sms_post['Pub Id'].astype('str').str.split(\".\",expand = True)[0]\n",
    "sms_post = sms_post.drop(columns = ['Dates','Pub Id','Pub Name'])\n",
    "post_data = pd.concat([rev_accept_df,sms_post], axis=0, ignore_index=True)\n",
    "post_data = post_data.fillna(0)\n",
    "post_data = post_data.groupby(['PUBID','Date']).sum().reset_index()\n",
    "publisher_raw['PUBID'] = publisher_raw['PUBID'].astype('str').str.split(\".\",expand = True)[0]\n",
    "post_data = post_data.merge(publisher_raw[['PUBID','PUBLISHER' ,'DP.DS or DP.sV','DMA', 'INTERNAL STATUS',\n",
    "       'DATA TEAM STATUS', 'Sub Vertical', 'SCOPE', 'COMPANY (DP)','DP ID']], how = 'left', on = 'PUBID')\n",
    "post_data.to_csv(localfolder+'p&l_data.csv',index =False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a828a65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19738.780000000002\n",
      "19738.780000000002\n",
      "19738.780000000002\n",
      "19738.78\n"
     ]
    }
   ],
   "source": [
    "# offerwall reporting \n",
    "import infrastructure \n",
    "offer_sheet = infrastructure.get_smartsheet('offers_sms')\n",
    "offer_sheet = offer_sheet[offer_sheet['Hitpath Offer ID'].isna() == False]\n",
    "offer_sheet.rename(columns ={'Hitpath Offer ID':'Hitpath ID'}, inplace = True)\n",
    "offer_sheet['Hitpath ID'] = offer_sheet['Hitpath ID'].astype(str).str.split(\".\",expand = True)[0]\n",
    "offerwall = offerwall.rename(columns = {'offer_id':'Offer Wall ID'})\n",
    "print(offerwall[ 'amount'].sum())\n",
    "offerwall_update = offerwall.merge(offer_sheet[['Hitpath ID','Scheduling Name']], copy = False, how = 'left', left_on = 'Offer Wall ID', right_on = 'Hitpath ID')\n",
    "print(offerwall_update[ 'amount'].sum())\n",
    "offerwall_update = offerwall_update.rename(columns = {'Scheduling Name':\"Offer Wall Scheduling Name\" })\n",
    "offerwall_update = offerwall_update.merge(offer_sheet[['Hitpath ID','Scheduling Name']], copy = False, how = 'left', left_on = 'campaign_id', right_on = 'Hitpath ID')\n",
    "offerwall_update.loc[offerwall_update['Scheduling Name'].isna(),'Scheduling Name' ] = offerwall_update['campaign_name']\n",
    "\n",
    "offerwall_update = offerwall_update.rename(columns = {'Scheduling Name':\"Child Offer Campaign Name\" })\n",
    "offerwall_update[['Dash_Date_from_subid','affiliate_id','campaign_id',\"Child Offer Campaign Name\",'sub_id','Offer Wall ID',\"Offer Wall Scheduling Name\" ]] = offerwall_update[['Dash_Date_from_subid','affiliate_id','campaign_id',\"Child Offer Campaign Name\",'sub_id','Offer Wall ID',\"Offer Wall Scheduling Name\" ]].fillna(\"NULL\")\n",
    "offerwall_rev = offerwall_update.groupby(['Dash_Date_from_subid','affiliate_id','campaign_id',\"Child Offer Campaign Name\",'sub_id','Offer Wall ID',\"Offer Wall Scheduling Name\" ])[[ 'amount', 'Jump Page Clicks']].sum().reset_index()\n",
    "print(offerwall_rev[ 'amount'].sum())\n",
    "offerwall_rev['split_column'] = offerwall_rev['sub_id'].str.split('_')\n",
    "offerwall_rev['Send Strategy'] = 'P'\n",
    "offerwall_rev.loc[offerwall_rev['split_column'].str[5].str.contains(\"AR\",na = False),'Send Strategy'] = 'AR'\n",
    "offerwall_rev.loc[offerwall_rev['split_column'].str[5].str.contains(\"CT\",na = False),'Send Strategy'] = 'CT'\n",
    "offerwall_rev = offerwall_rev.rename(columns = {'amount':'Child Offer Revenue','Jump Page Clicks':'Child Offer Jump Page Clicks', 'campaign_id':'Child Offer ID'})\n",
    "merged_data_update = merged_data.add_suffix('- Offer Wall')\n",
    "offerwall_engage_merge = merged_data[merged_data['Offer Type'] == 'Offer Wall']\n",
    "offerwall_engage_subid = merged_data_update.groupby(['Sub_Id- Offer Wall'])[['Delivered- Offer Wall','Clicks- Offer Wall','Optout- Offer Wall','Cost- Offer Wall','Revenue- Offer Wall','Jump Page Clicks- Offer Wall']].sum().reset_index()\n",
    "offerwall_engage =  merged_data_update.groupby(['Date- Offer Wall','Affiliate_Id- Offer Wall','Hitpath_Offer_ID- Offer Wall','Send Strategy- Offer Wall'])[['Delivered- Offer Wall','Clicks- Offer Wall','Optout- Offer Wall','Cost- Offer Wall','Revenue- Offer Wall','Jump Page Clicks- Offer Wall']].sum().reset_index()\n",
    "offerwall_rev['affiliate_id'] = offerwall_rev['affiliate_id'].astype(str).str.split('.',expand = True)[0]\n",
    "offerwall_rev['Offer Wall ID'] = offerwall_rev['Offer Wall ID'].astype(str).str.split('.',expand = True)[0]\n",
    "offerwall_engage['Date- Offer Wall'] = pd.to_datetime(offerwall_engage['Date- Offer Wall'])\n",
    "offerwall_rev['Dash_Date_from_subid'] = pd.to_datetime(offerwall_rev['Dash_Date_from_subid'])\n",
    "offerwall_performance = offerwall_rev.merge(offerwall_engage_subid, left_on = ['sub_id'], right_on =['Sub_Id- Offer Wall'], how = 'left' )\n",
    "offerwall_performance_1 = offerwall_performance.loc[offerwall_performance['Sub_Id- Offer Wall'].isna() == False]\n",
    "offerwall_performance_2 = offerwall_performance.loc[offerwall_performance['Sub_Id- Offer Wall'].isna()== True].drop(columns = ['Sub_Id- Offer Wall','Delivered- Offer Wall','Clicks- Offer Wall','Optout- Offer Wall','Cost- Offer Wall','Revenue- Offer Wall','Jump Page Clicks- Offer Wall'])\n",
    "offerwall_performance_2 = offerwall_performance_2.groupby(['Dash_Date_from_subid','affiliate_id','Offer Wall ID','Send Strategy','Child Offer ID'])[[ 'Child Offer Revenue', 'Child Offer Jump Page Clicks']].sum().reset_index()\n",
    "offerwall_engage['Hitpath_Offer_ID- Offer Wall'] = offerwall_engage['Hitpath_Offer_ID- Offer Wall'].astype(str).str.split(\".\",expand = True)[0]\n",
    "offerwall_engage['Affiliate_Id- Offer Wall'] = offerwall_engage['Affiliate_Id- Offer Wall'].astype(str).str.split(\".\",expand = True)[0]\n",
    "offerwall_performance_2 = offerwall_performance_2.merge(offerwall_engage, left_on = ['Dash_Date_from_subid','affiliate_id','Offer Wall ID','Send Strategy'], right_on = ['Date- Offer Wall','Affiliate_Id- Offer Wall','Hitpath_Offer_ID- Offer Wall','Send Strategy- Offer Wall'], how ='left')\n",
    "offerwall_report = pd.concat([offerwall_performance_1,offerwall_performance_2])\n",
    "offerwall_report = offerwall_report.merge(ofwall_df, left_on = 'Offer Wall ID', right_on = 'Offer Wall ID', how = 'left')\n",
    "print(offerwall_report['Child Offer Revenue'].sum())\n",
    "offerwall_report.to_csv(localfolder + \"offerwall_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5ada817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['8838OW1',\n",
       " 'OW5',\n",
       " '6272',\n",
       " '11928OW2',\n",
       " '9088OW2',\n",
       " '12441OW2',\n",
       " 'OW3',\n",
       " 'OW4',\n",
       " '11928OW4',\n",
       " 'OW6',\n",
       " 'OW7',\n",
       " 'OW8',\n",
       " 'OW9',\n",
       " 'OW10',\n",
       " 'OW12',\n",
       " 'OW13',\n",
       " 'OW11',\n",
       " 'OW14',\n",
       " 'OW15',\n",
       " 'OW16']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offerwall_report['Offer Wall ID'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23dd9381",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "connect: to ('smtp.gmail.com', 587) None\n",
      "reply: b'220 smtp.gmail.com ESMTP vw18-20020a05620a565200b00788663d8a1esm1331796qkn.112 - gsmtp\\r\\n'\n",
      "reply: retcode (220); Msg: b'smtp.gmail.com ESMTP vw18-20020a05620a565200b00788663d8a1esm1331796qkn.112 - gsmtp'\n",
      "connect: b'smtp.gmail.com ESMTP vw18-20020a05620a565200b00788663d8a1esm1331796qkn.112 - gsmtp'\n",
      "send: 'ehlo [100.96.2.115]\\r\\n'\n",
      "reply: b'250-smtp.gmail.com at your service, [52.38.54.186]\\r\\n'\n",
      "reply: b'250-SIZE 35882577\\r\\n'\n",
      "reply: b'250-8BITMIME\\r\\n'\n",
      "reply: b'250-STARTTLS\\r\\n'\n",
      "reply: b'250-ENHANCEDSTATUSCODES\\r\\n'\n",
      "reply: b'250-PIPELINING\\r\\n'\n",
      "reply: b'250-CHUNKING\\r\\n'\n",
      "reply: b'250 SMTPUTF8\\r\\n'\n",
      "reply: retcode (250); Msg: b'smtp.gmail.com at your service, [52.38.54.186]\\nSIZE 35882577\\n8BITMIME\\nSTARTTLS\\nENHANCEDSTATUSCODES\\nPIPELINING\\nCHUNKING\\nSMTPUTF8'\n",
      "send: 'STARTTLS\\r\\n'\n",
      "reply: b'220 2.0.0 Ready to start TLS\\r\\n'\n",
      "reply: retcode (220); Msg: b'2.0.0 Ready to start TLS'\n",
      "send: 'ehlo [100.96.2.115]\\r\\n'\n",
      "reply: b'250-smtp.gmail.com at your service, [52.38.54.186]\\r\\n'\n",
      "reply: b'250-SIZE 35882577\\r\\n'\n",
      "reply: b'250-8BITMIME\\r\\n'\n",
      "reply: b'250-AUTH LOGIN PLAIN XOAUTH2 PLAIN-CLIENTTOKEN OAUTHBEARER XOAUTH\\r\\n'\n",
      "reply: b'250-ENHANCEDSTATUSCODES\\r\\n'\n",
      "reply: b'250-PIPELINING\\r\\n'\n",
      "reply: b'250-CHUNKING\\r\\n'\n",
      "reply: b'250 SMTPUTF8\\r\\n'\n",
      "reply: retcode (250); Msg: b'smtp.gmail.com at your service, [52.38.54.186]\\nSIZE 35882577\\n8BITMIME\\nAUTH LOGIN PLAIN XOAUTH2 PLAIN-CLIENTTOKEN OAUTHBEARER XOAUTH\\nENHANCEDSTATUSCODES\\nPIPELINING\\nCHUNKING\\nSMTPUTF8'\n",
      "send: 'AUTH PLAIN AHNjaGVkdWxlYWxhcnRzcnhtZ0BnbWFpbC5jb20AeXlqZXRlanRqaGpoenh4aw==\\r\\n'\n",
      "reply: b'235 2.7.0 Accepted\\r\\n'\n",
      "reply: retcode (235); Msg: b'2.7.0 Accepted'\n",
      "send: 'mail FROM:<schedulealartsrxmg@gmail.com> size=14670812\\r\\n'\n",
      "reply: b'250 2.1.0 OK vw18-20020a05620a565200b00788663d8a1esm1331796qkn.112 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.1.0 OK vw18-20020a05620a565200b00788663d8a1esm1331796qkn.112 - gsmtp'\n",
      "send: 'rcpt TO:<nathan@rxmg.com>\\r\\n'\n",
      "reply: b'250 2.1.5 OK vw18-20020a05620a565200b00788663d8a1esm1331796qkn.112 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.1.5 OK vw18-20020a05620a565200b00788663d8a1esm1331796qkn.112 - gsmtp'\n",
      "send: 'data\\r\\n'\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "reply: b'250 2.0.0 OK  1710456526 vw18-20020a05620a565200b00788663d8a1esm1331796qkn.112 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.0.0 OK  1710456526 vw18-20020a05620a565200b00788663d8a1esm1331796qkn.112 - gsmtp'\n",
      "data: (250, b'2.0.0 OK  1710456526 vw18-20020a05620a565200b00788663d8a1esm1331796qkn.112 - gsmtp')\n",
      "send: 'quit\\r\\n'\n",
      "reply: b'221 2.0.0 closing connection vw18-20020a05620a565200b00788663d8a1esm1331796qkn.112 - gsmtp\\r\\n'\n",
      "reply: retcode (221); Msg: b'2.0.0 closing connection vw18-20020a05620a565200b00788663d8a1esm1331796qkn.112 - gsmtp'\n",
      "connect: to ('smtp.gmail.com', 587) None\n",
      "reply: b'220 smtp.gmail.com ESMTP q4-20020ad45ca4000000b00690c77505bdsm1066865qvh.37 - gsmtp\\r\\n'\n",
      "reply: retcode (220); Msg: b'smtp.gmail.com ESMTP q4-20020ad45ca4000000b00690c77505bdsm1066865qvh.37 - gsmtp'\n",
      "connect: b'smtp.gmail.com ESMTP q4-20020ad45ca4000000b00690c77505bdsm1066865qvh.37 - gsmtp'\n",
      "send: 'ehlo [100.96.2.115]\\r\\n'\n",
      "reply: b'250-smtp.gmail.com at your service, [52.38.54.186]\\r\\n'\n",
      "reply: b'250-SIZE 35882577\\r\\n'\n",
      "reply: b'250-8BITMIME\\r\\n'\n",
      "reply: b'250-STARTTLS\\r\\n'\n",
      "reply: b'250-ENHANCEDSTATUSCODES\\r\\n'\n",
      "reply: b'250-PIPELINING\\r\\n'\n",
      "reply: b'250-CHUNKING\\r\\n'\n",
      "reply: b'250 SMTPUTF8\\r\\n'\n",
      "reply: retcode (250); Msg: b'smtp.gmail.com at your service, [52.38.54.186]\\nSIZE 35882577\\n8BITMIME\\nSTARTTLS\\nENHANCEDSTATUSCODES\\nPIPELINING\\nCHUNKING\\nSMTPUTF8'\n",
      "send: 'STARTTLS\\r\\n'\n",
      "reply: b'220 2.0.0 Ready to start TLS\\r\\n'\n",
      "reply: retcode (220); Msg: b'2.0.0 Ready to start TLS'\n",
      "send: 'ehlo [100.96.2.115]\\r\\n'\n",
      "reply: b'250-smtp.gmail.com at your service, [52.38.54.186]\\r\\n'\n",
      "reply: b'250-SIZE 35882577\\r\\n'\n",
      "reply: b'250-8BITMIME\\r\\n'\n",
      "reply: b'250-AUTH LOGIN PLAIN XOAUTH2 PLAIN-CLIENTTOKEN OAUTHBEARER XOAUTH\\r\\n'\n",
      "reply: b'250-ENHANCEDSTATUSCODES\\r\\n'\n",
      "reply: b'250-PIPELINING\\r\\n'\n",
      "reply: b'250-CHUNKING\\r\\n'\n",
      "reply: b'250 SMTPUTF8\\r\\n'\n",
      "reply: retcode (250); Msg: b'smtp.gmail.com at your service, [52.38.54.186]\\nSIZE 35882577\\n8BITMIME\\nAUTH LOGIN PLAIN XOAUTH2 PLAIN-CLIENTTOKEN OAUTHBEARER XOAUTH\\nENHANCEDSTATUSCODES\\nPIPELINING\\nCHUNKING\\nSMTPUTF8'\n",
      "send: 'AUTH PLAIN AHNjaGVkdWxlYWxhcnRzcnhtZ0BnbWFpbC5jb20AeXlqZXRlanRqaGpoenh4aw==\\r\\n'\n",
      "reply: b'235 2.7.0 Accepted\\r\\n'\n",
      "reply: retcode (235); Msg: b'2.7.0 Accepted'\n",
      "send: 'mail FROM:<schedulealartsrxmg@gmail.com> size=14670812\\r\\n'\n",
      "reply: b'250 2.1.0 OK q4-20020ad45ca4000000b00690c77505bdsm1066865qvh.37 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.1.0 OK q4-20020ad45ca4000000b00690c77505bdsm1066865qvh.37 - gsmtp'\n",
      "send: 'rcpt TO:<g.chao@rxmg.com>\\r\\n'\n",
      "reply: b'250 2.1.5 OK q4-20020ad45ca4000000b00690c77505bdsm1066865qvh.37 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.1.5 OK q4-20020ad45ca4000000b00690c77505bdsm1066865qvh.37 - gsmtp'\n",
      "send: 'data\\r\\n'\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "reply: b'250 2.0.0 OK  1710456539 q4-20020ad45ca4000000b00690c77505bdsm1066865qvh.37 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.0.0 OK  1710456539 q4-20020ad45ca4000000b00690c77505bdsm1066865qvh.37 - gsmtp'\n",
      "data: (250, b'2.0.0 OK  1710456539 q4-20020ad45ca4000000b00690c77505bdsm1066865qvh.37 - gsmtp')\n",
      "send: 'quit\\r\\n'\n",
      "reply: b'221 2.0.0 closing connection q4-20020ad45ca4000000b00690c77505bdsm1066865qvh.37 - gsmtp\\r\\n'\n",
      "reply: retcode (221); Msg: b'2.0.0 closing connection q4-20020ad45ca4000000b00690c77505bdsm1066865qvh.37 - gsmtp'\n",
      "connect: to ('smtp.gmail.com', 587) None\n",
      "reply: b'220 smtp.gmail.com ESMTP o23-20020ac872d7000000b00430a67b3437sm729977qtp.17 - gsmtp\\r\\n'\n",
      "reply: retcode (220); Msg: b'smtp.gmail.com ESMTP o23-20020ac872d7000000b00430a67b3437sm729977qtp.17 - gsmtp'\n",
      "connect: b'smtp.gmail.com ESMTP o23-20020ac872d7000000b00430a67b3437sm729977qtp.17 - gsmtp'\n",
      "send: 'ehlo [100.96.2.115]\\r\\n'\n",
      "reply: b'250-smtp.gmail.com at your service, [52.38.54.186]\\r\\n'\n",
      "reply: b'250-SIZE 35882577\\r\\n'\n",
      "reply: b'250-8BITMIME\\r\\n'\n",
      "reply: b'250-STARTTLS\\r\\n'\n",
      "reply: b'250-ENHANCEDSTATUSCODES\\r\\n'\n",
      "reply: b'250-PIPELINING\\r\\n'\n",
      "reply: b'250-CHUNKING\\r\\n'\n",
      "reply: b'250 SMTPUTF8\\r\\n'\n",
      "reply: retcode (250); Msg: b'smtp.gmail.com at your service, [52.38.54.186]\\nSIZE 35882577\\n8BITMIME\\nSTARTTLS\\nENHANCEDSTATUSCODES\\nPIPELINING\\nCHUNKING\\nSMTPUTF8'\n",
      "send: 'STARTTLS\\r\\n'\n",
      "reply: b'220 2.0.0 Ready to start TLS\\r\\n'\n",
      "reply: retcode (220); Msg: b'2.0.0 Ready to start TLS'\n",
      "send: 'ehlo [100.96.2.115]\\r\\n'\n",
      "reply: b'250-smtp.gmail.com at your service, [52.38.54.186]\\r\\n'\n",
      "reply: b'250-SIZE 35882577\\r\\n'\n",
      "reply: b'250-8BITMIME\\r\\n'\n",
      "reply: b'250-AUTH LOGIN PLAIN XOAUTH2 PLAIN-CLIENTTOKEN OAUTHBEARER XOAUTH\\r\\n'\n",
      "reply: b'250-ENHANCEDSTATUSCODES\\r\\n'\n",
      "reply: b'250-PIPELINING\\r\\n'\n",
      "reply: b'250-CHUNKING\\r\\n'\n",
      "reply: b'250 SMTPUTF8\\r\\n'\n",
      "reply: retcode (250); Msg: b'smtp.gmail.com at your service, [52.38.54.186]\\nSIZE 35882577\\n8BITMIME\\nAUTH LOGIN PLAIN XOAUTH2 PLAIN-CLIENTTOKEN OAUTHBEARER XOAUTH\\nENHANCEDSTATUSCODES\\nPIPELINING\\nCHUNKING\\nSMTPUTF8'\n",
      "send: 'AUTH PLAIN AHNjaGVkdWxlYWxhcnRzcnhtZ0BnbWFpbC5jb20AeXlqZXRlanRqaGpoenh4aw==\\r\\n'\n",
      "reply: b'235 2.7.0 Accepted\\r\\n'\n",
      "reply: retcode (235); Msg: b'2.7.0 Accepted'\n",
      "send: 'mail FROM:<schedulealartsrxmg@gmail.com> size=14670810\\r\\n'\n",
      "reply: b'250 2.1.0 OK o23-20020ac872d7000000b00430a67b3437sm729977qtp.17 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.1.0 OK o23-20020ac872d7000000b00430a67b3437sm729977qtp.17 - gsmtp'\n",
      "send: 'rcpt TO:<lili@rxmg.com>\\r\\n'\n",
      "reply: b'250 2.1.5 OK o23-20020ac872d7000000b00430a67b3437sm729977qtp.17 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.1.5 OK o23-20020ac872d7000000b00430a67b3437sm729977qtp.17 - gsmtp'\n",
      "send: 'data\\r\\n'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "reply: b'250 2.0.0 OK  1710456549 o23-20020ac872d7000000b00430a67b3437sm729977qtp.17 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.0.0 OK  1710456549 o23-20020ac872d7000000b00430a67b3437sm729977qtp.17 - gsmtp'\n",
      "data: (250, b'2.0.0 OK  1710456549 o23-20020ac872d7000000b00430a67b3437sm729977qtp.17 - gsmtp')\n",
      "send: 'quit\\r\\n'\n",
      "reply: b'221 2.0.0 closing connection o23-20020ac872d7000000b00430a67b3437sm729977qtp.17 - gsmtp\\r\\n'\n",
      "reply: retcode (221); Msg: b'2.0.0 closing connection o23-20020ac872d7000000b00430a67b3437sm729977qtp.17 - gsmtp'\n",
      "connect: to ('smtp.gmail.com', 587) None\n",
      "reply: b'220 smtp.gmail.com ESMTP h13-20020a05620a13ed00b007882bcc6c13sm1342975qkl.6 - gsmtp\\r\\n'\n",
      "reply: retcode (220); Msg: b'smtp.gmail.com ESMTP h13-20020a05620a13ed00b007882bcc6c13sm1342975qkl.6 - gsmtp'\n",
      "connect: b'smtp.gmail.com ESMTP h13-20020a05620a13ed00b007882bcc6c13sm1342975qkl.6 - gsmtp'\n",
      "send: 'ehlo [100.96.2.115]\\r\\n'\n",
      "reply: b'250-smtp.gmail.com at your service, [52.38.54.186]\\r\\n'\n",
      "reply: b'250-SIZE 35882577\\r\\n'\n",
      "reply: b'250-8BITMIME\\r\\n'\n",
      "reply: b'250-STARTTLS\\r\\n'\n",
      "reply: b'250-ENHANCEDSTATUSCODES\\r\\n'\n",
      "reply: b'250-PIPELINING\\r\\n'\n",
      "reply: b'250-CHUNKING\\r\\n'\n",
      "reply: b'250 SMTPUTF8\\r\\n'\n",
      "reply: retcode (250); Msg: b'smtp.gmail.com at your service, [52.38.54.186]\\nSIZE 35882577\\n8BITMIME\\nSTARTTLS\\nENHANCEDSTATUSCODES\\nPIPELINING\\nCHUNKING\\nSMTPUTF8'\n",
      "send: 'STARTTLS\\r\\n'\n",
      "reply: b'220 2.0.0 Ready to start TLS\\r\\n'\n",
      "reply: retcode (220); Msg: b'2.0.0 Ready to start TLS'\n",
      "send: 'ehlo [100.96.2.115]\\r\\n'\n",
      "reply: b'250-smtp.gmail.com at your service, [52.38.54.186]\\r\\n'\n",
      "reply: b'250-SIZE 35882577\\r\\n'\n",
      "reply: b'250-8BITMIME\\r\\n'\n",
      "reply: b'250-AUTH LOGIN PLAIN XOAUTH2 PLAIN-CLIENTTOKEN OAUTHBEARER XOAUTH\\r\\n'\n",
      "reply: b'250-ENHANCEDSTATUSCODES\\r\\n'\n",
      "reply: b'250-PIPELINING\\r\\n'\n",
      "reply: b'250-CHUNKING\\r\\n'\n",
      "reply: b'250 SMTPUTF8\\r\\n'\n",
      "reply: retcode (250); Msg: b'smtp.gmail.com at your service, [52.38.54.186]\\nSIZE 35882577\\n8BITMIME\\nAUTH LOGIN PLAIN XOAUTH2 PLAIN-CLIENTTOKEN OAUTHBEARER XOAUTH\\nENHANCEDSTATUSCODES\\nPIPELINING\\nCHUNKING\\nSMTPUTF8'\n",
      "send: 'AUTH PLAIN AHNjaGVkdWxlYWxhcnRzcnhtZ0BnbWFpbC5jb20AeXlqZXRlanRqaGpoenh4aw==\\r\\n'\n",
      "reply: b'235 2.7.0 Accepted\\r\\n'\n",
      "reply: retcode (235); Msg: b'2.7.0 Accepted'\n",
      "send: 'mail FROM:<schedulealartsrxmg@gmail.com> size=14670810\\r\\n'\n",
      "reply: b'250 2.1.0 OK h13-20020a05620a13ed00b007882bcc6c13sm1342975qkl.6 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.1.0 OK h13-20020a05620a13ed00b007882bcc6c13sm1342975qkl.6 - gsmtp'\n",
      "send: 'rcpt TO:<nina@rxmg.com>\\r\\n'\n",
      "reply: b'250 2.1.5 OK h13-20020a05620a13ed00b007882bcc6c13sm1342975qkl.6 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.1.5 OK h13-20020a05620a13ed00b007882bcc6c13sm1342975qkl.6 - gsmtp'\n",
      "send: 'data\\r\\n'\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "reply: b'250 2.0.0 OK  1710456562 h13-20020a05620a13ed00b007882bcc6c13sm1342975qkl.6 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.0.0 OK  1710456562 h13-20020a05620a13ed00b007882bcc6c13sm1342975qkl.6 - gsmtp'\n",
      "data: (250, b'2.0.0 OK  1710456562 h13-20020a05620a13ed00b007882bcc6c13sm1342975qkl.6 - gsmtp')\n",
      "send: 'quit\\r\\n'\n",
      "reply: b'221 2.0.0 closing connection h13-20020a05620a13ed00b007882bcc6c13sm1342975qkl.6 - gsmtp\\r\\n'\n",
      "reply: retcode (221); Msg: b'2.0.0 closing connection h13-20020a05620a13ed00b007882bcc6c13sm1342975qkl.6 - gsmtp'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import zipfile\n",
    "os.chdir(localfolder) \n",
    "filename = 'daily_data_update.zip'\n",
    "email_body = ''\n",
    "with zipfile.ZipFile('daily_data_update.zip', 'w',zipfile.ZIP_DEFLATED) as zipf:\n",
    "    zipf.write('SS_LC_merged_data.csv')\n",
    "toaddr = ['nathan@rxmg.com','g.chao@rxmg.com','lili@rxmg.com','nina@rxmg.com']\n",
    "#toaddr = ['lili@rxmg.com']\n",
    "subject_line = 'SMS Daily Data Update' \n",
    "email_body \n",
    "for i in toaddr:\n",
    "    send_email.send_email([filename],subject_line,email_body,i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726f9b71",
   "metadata": {},
   "source": [
    "## Verification Data Quality "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83966e3c",
   "metadata": {},
   "source": [
    "+ verify last month's Delivered volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3211a644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Shortcode Name</th>\n",
       "      <th>Delivered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DSS</td>\n",
       "      <td>183102.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FLC</td>\n",
       "      <td>2903723.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HZB</td>\n",
       "      <td>1658231.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MBC</td>\n",
       "      <td>3672159.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVT</td>\n",
       "      <td>897460.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>UAA</td>\n",
       "      <td>725563.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>UAATF</td>\n",
       "      <td>33008.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>W1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Shortcode Name  Delivered\n",
       "0            DSS   183102.0\n",
       "1            FLC  2903723.0\n",
       "2            HZB  1658231.0\n",
       "3            MBC  3672159.0\n",
       "4            SVT   897460.0\n",
       "5            UAA   725563.0\n",
       "6          UAATF    33008.0\n",
       "7             W1        0.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check delivered volumn by shortcode name \n",
    "merged_data['Date'] = pd.to_datetime(merged_data['Date'])\n",
    "merged_data['Month'] = merged_data['Date'].dt.strftime('%Y-%m')\n",
    "merged_data[ merged_data['Month'] == '2023-12'].groupby(['Shortcode Name'])[['Delivered']].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc6a1874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Revenue Source</th>\n",
       "      <th>Delivered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Email</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Push</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Short Code - SS Flow</td>\n",
       "      <td>3630005.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Short Code - SS Jobs</td>\n",
       "      <td>6438546.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Toll Free</td>\n",
       "      <td>33008.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Revenue Source  Delivered\n",
       "0                 Email        0.0\n",
       "1                  Push        0.0\n",
       "2  Short Code - SS Flow  3630005.0\n",
       "3  Short Code - SS Jobs  6438546.0\n",
       "4             Toll Free    33008.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data[ merged_data['Month'] == '2023-12'].groupby(['Revenue Source'])[['Delivered']].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c5ed3cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Delivered    10101559.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data[ merged_data['Month'] == '2023-12'][['Delivered']].sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "48f99afc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[nan,\n",
       " '11928',\n",
       " '12342',\n",
       " '8838',\n",
       " '9088',\n",
       " '11646',\n",
       " '6272',\n",
       " '11793',\n",
       " '11468',\n",
       " '12305',\n",
       " 'OW4',\n",
       " 'OW3',\n",
       " '11600',\n",
       " '5724',\n",
       " '11544',\n",
       " '6322',\n",
       " '10086',\n",
       " '12326',\n",
       " '12612',\n",
       " '12640',\n",
       " '12089',\n",
       " '11891',\n",
       " '7878',\n",
       " '8144',\n",
       " '6270',\n",
       " '12456',\n",
       " '11593',\n",
       " '12587',\n",
       " '12585',\n",
       " '12588',\n",
       " '12610',\n",
       " '12306',\n",
       " '12233',\n",
       " '12386',\n",
       " '11714',\n",
       " '12611',\n",
       " '12453',\n",
       " '9088OW2',\n",
       " '12586',\n",
       " 5724.0,\n",
       " 6322.0,\n",
       " 6272.0,\n",
       " 11793.0,\n",
       " 11714.0,\n",
       " 11928.0,\n",
       " 11646.0,\n",
       " 12305.0,\n",
       " 12342.0,\n",
       " 12386.0,\n",
       " 6270.0,\n",
       " 12456.0,\n",
       " 11600.0,\n",
       " 10086.0,\n",
       " 12611.0,\n",
       " 8838.0,\n",
       " 9088.0,\n",
       " 11544.0,\n",
       " 11468.0,\n",
       " 12586.0,\n",
       " 12233.0,\n",
       " 11891.0,\n",
       " 12145.0,\n",
       " 12610.0,\n",
       " 12440.0,\n",
       " 12115.0,\n",
       " 11593.0,\n",
       " 12306.0,\n",
       " 12119.0,\n",
       " 12089.0,\n",
       " 12514.0,\n",
       " 9980.0,\n",
       " '12385',\n",
       " '12514',\n",
       " '11890',\n",
       " '12384',\n",
       " '12440',\n",
       " '11673',\n",
       " '12441OW2',\n",
       " '12441',\n",
       " 12384.0,\n",
       " 12441.0,\n",
       " 12637.0,\n",
       " 12639.0,\n",
       " 12326.0,\n",
       " 11847.0,\n",
       " 12385.0,\n",
       " 11890.0,\n",
       " 12640.0,\n",
       " 7878.0,\n",
       " 12453.0,\n",
       " 12622.0,\n",
       " 11834.0,\n",
       " 12585.0,\n",
       " 8232.0,\n",
       " '12113',\n",
       " 12633.0,\n",
       " 12088.0,\n",
       " 12207.0,\n",
       " 12113.0,\n",
       " '12115',\n",
       " '12637',\n",
       " '12639',\n",
       " '8838OW1',\n",
       " '12088',\n",
       " '11928OW2',\n",
       " '12584',\n",
       " '12378',\n",
       " '12145',\n",
       " '6444',\n",
       " '12207',\n",
       " '12119',\n",
       " '12614']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data[ merged_data['Month'] == '2023-11']['Hitpath_Offer_ID'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d92816",
   "metadata": {},
   "source": [
    "+ verify Daily Revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7c17bd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-10T16:34:43.906980Z",
     "start_time": "2023-08-10T16:34:32.082989Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The revenue discrepency after November between raw file and Tableau data is $ 343.83\n",
      "The total revenue after November is $ 2519578.31\n",
      "The revenue discrepency is 0.0 % of the total revenue after November\n"
     ]
    }
   ],
   "source": [
    "# Check & Examination: \n",
    "# Revenue files sum up after November \n",
    "rev_file = combined_csv[combined_csv['date']>='2022-11-01'].reset_index(drop=True)['amount'].sum()\n",
    "# Revenue we made after November\n",
    "after_merge = combined_csv_ss_creative_na['Revenue'].sum() + combined_csv_ss_creative_notna['Revenue'].sum() + combined_csv_ss_exclude_11['Revenue'].sum() +flows_clean2['Revenue'].sum() + combined_csv_ss_last_11['amount'].sum() + lc_df_full_11['amount'].sum() + combined_csv_push_11['Revenue'].sum() + combined_csv_w1_11['Revenue'].sum()\n",
    "email_body = \"The revenue discrepency after November between raw file and Tableau data is $\"+str(round((rev_file - after_merge),2)) + \"\\n\" \n",
    "email_body += \"The total revenue after November is $\" + str(round(rev_file,2)) + \"\\n\" \n",
    "email_body += \"The revenue discrepency is \"+ str( round((rev_file - after_merge) /rev_file,2)*100 ) + \"% of the total revenue after November\" + \"\\n\" + \"The revenue we made after November is $\" + str(round(after_merge,2))\n",
    "print(\"The revenue discrepency after November between raw file and Tableau data is $\",round((rev_file - after_merge),2))\n",
    "print(\"The total revenue after November is $\", round(rev_file,2))\n",
    "print(\"The revenue discrepency is\", round((rev_file - after_merge) /rev_file,2)*100, \"% of the total revenue after November\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d45a8b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Month\n",
       "1899-12         0.000000\n",
       "1999-12       652.800000\n",
       "2020-02       163.100000\n",
       "2020-03      1116.300000\n",
       "2020-04      5060.200000\n",
       "2020-05      2218.760000\n",
       "2020-06      3551.800000\n",
       "2020-07      6578.100000\n",
       "2020-08     41870.600000\n",
       "2020-09     52336.370000\n",
       "2020-10     50779.900000\n",
       "2020-11     37785.730000\n",
       "2020-12     32358.100000\n",
       "2021-01     23948.040000\n",
       "2021-02     20137.840000\n",
       "2021-03     26584.030000\n",
       "2021-04     50262.400000\n",
       "2021-05     74756.520000\n",
       "2021-06     91146.100000\n",
       "2021-07    144141.810000\n",
       "2021-08    158429.500000\n",
       "2021-09    154983.178800\n",
       "2021-10    201033.211100\n",
       "2021-11    225191.850000\n",
       "2021-12    266561.980000\n",
       "2022-01    276933.964528\n",
       "2022-02    268013.423635\n",
       "2022-03    249501.175543\n",
       "2022-04    296194.537417\n",
       "2022-05    277356.288745\n",
       "2022-06    404793.833177\n",
       "2022-07    247402.947505\n",
       "2022-08    155207.055008\n",
       "2022-09    161623.944255\n",
       "2022-10     39895.442484\n",
       "2022-11     87337.163168\n",
       "2022-12    112797.439432\n",
       "2023-01    120882.586178\n",
       "2023-02    104659.552726\n",
       "2023-03    114402.376130\n",
       "2023-04    138092.207699\n",
       "2023-05    146932.479997\n",
       "2023-06    151868.903074\n",
       "2023-07    159070.812617\n",
       "2023-08    176552.914455\n",
       "2023-09    173993.620641\n",
       "2023-10    203605.155832\n",
       "2023-11    195007.793519\n",
       "2023-12    201446.806177\n",
       "2024-01    193449.194198\n",
       "2024-02    164773.410576\n",
       "2024-03     66320.312373\n",
       "NaT          2656.170000\n",
       "Name: Revenue, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#monthly revenue\n",
    "merged_data['Month'] = merged_data['Date'].astype(str).str[:7]\n",
    "temp = merged_data.groupby('Month')['Revenue'].sum()\n",
    "email_body +=  \"\\n\\nThe revenue by month is\\n\" \n",
    "email_body += str(temp)  + '\\n'\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9377a12c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "month\n",
       "2022-11    159.13\n",
       "2022-12    688.30\n",
       "2023-01    859.75\n",
       "2023-02    881.73\n",
       "2023-03    292.22\n",
       "2023-04    869.75\n",
       "2023-05    860.70\n",
       "2023-06     11.45\n",
       "2023-07      2.55\n",
       "2023-08    142.34\n",
       "2023-09    102.80\n",
       "2023-10     14.24\n",
       "2023-11      0.80\n",
       "2023-12      1.33\n",
       "2024-01     65.77\n",
       "2024-02    230.08\n",
       "2024-03    202.64\n",
       "Name: amount, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#unattributed revenue\n",
    "# not attribute to our data yet\n",
    "## Including KCP, NOC, and ROC \n",
    "## include missing subid drop \n",
    "## advertiser: NIC missing revenue in MAy $810. \n",
    "\n",
    "\n",
    "combined_csv_ss_last_11['month'] = combined_csv_ss_last_11['date'].astype(str).str[:7]\n",
    "\n",
    "email_body += \"\\n\" + \"The revenue by month for unclaimed revenue is \" + str(combined_csv_ss_last_11.groupby('month')['amount'].sum()) +\"\\n\"\n",
    "combined_csv_ss_last_11.groupby('month')['amount'].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f63eaf47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date\n",
       "1899-12-31       0.000000\n",
       "1999-12-31     652.800000\n",
       "2020-02-10       0.000000\n",
       "2020-02-11       4.500000\n",
       "2020-02-27      45.050000\n",
       "                 ...     \n",
       "2024-03-10    4011.650000\n",
       "2024-03-11    6560.563938\n",
       "2024-03-12    4115.810015\n",
       "2024-03-13    5153.014492\n",
       "2024-03-14     399.791500\n",
       "Name: Revenue, Length: 1480, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#daily revenue\n",
    "temp = merged_data.groupby('Date')['Revenue'].sum()\n",
    "email_body +=  \"\\n\\nThe revenue by Date is\\n\" \n",
    "email_body += str(temp)  + '\\n' \n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c966a9",
   "metadata": {},
   "source": [
    "# Send out email to the Team "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "56a514ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "connect: to ('smtp.gmail.com', 587) None\n",
      "reply: b'220 smtp.gmail.com ESMTP bl16-20020a05620a1a9000b00789da80c51asm998174qkb.1 - gsmtp\\r\\n'\n",
      "reply: retcode (220); Msg: b'smtp.gmail.com ESMTP bl16-20020a05620a1a9000b00789da80c51asm998174qkb.1 - gsmtp'\n",
      "connect: b'smtp.gmail.com ESMTP bl16-20020a05620a1a9000b00789da80c51asm998174qkb.1 - gsmtp'\n",
      "send: 'ehlo [100.96.2.115]\\r\\n'\n",
      "reply: b'250-smtp.gmail.com at your service, [52.38.54.186]\\r\\n'\n",
      "reply: b'250-SIZE 35882577\\r\\n'\n",
      "reply: b'250-8BITMIME\\r\\n'\n",
      "reply: b'250-STARTTLS\\r\\n'\n",
      "reply: b'250-ENHANCEDSTATUSCODES\\r\\n'\n",
      "reply: b'250-PIPELINING\\r\\n'\n",
      "reply: b'250-CHUNKING\\r\\n'\n",
      "reply: b'250 SMTPUTF8\\r\\n'\n",
      "reply: retcode (250); Msg: b'smtp.gmail.com at your service, [52.38.54.186]\\nSIZE 35882577\\n8BITMIME\\nSTARTTLS\\nENHANCEDSTATUSCODES\\nPIPELINING\\nCHUNKING\\nSMTPUTF8'\n",
      "send: 'STARTTLS\\r\\n'\n",
      "reply: b'220 2.0.0 Ready to start TLS\\r\\n'\n",
      "reply: retcode (220); Msg: b'2.0.0 Ready to start TLS'\n",
      "send: 'ehlo [100.96.2.115]\\r\\n'\n",
      "reply: b'250-smtp.gmail.com at your service, [52.38.54.186]\\r\\n'\n",
      "reply: b'250-SIZE 35882577\\r\\n'\n",
      "reply: b'250-8BITMIME\\r\\n'\n",
      "reply: b'250-AUTH LOGIN PLAIN XOAUTH2 PLAIN-CLIENTTOKEN OAUTHBEARER XOAUTH\\r\\n'\n",
      "reply: b'250-ENHANCEDSTATUSCODES\\r\\n'\n",
      "reply: b'250-PIPELINING\\r\\n'\n",
      "reply: b'250-CHUNKING\\r\\n'\n",
      "reply: b'250 SMTPUTF8\\r\\n'\n",
      "reply: retcode (250); Msg: b'smtp.gmail.com at your service, [52.38.54.186]\\nSIZE 35882577\\n8BITMIME\\nAUTH LOGIN PLAIN XOAUTH2 PLAIN-CLIENTTOKEN OAUTHBEARER XOAUTH\\nENHANCEDSTATUSCODES\\nPIPELINING\\nCHUNKING\\nSMTPUTF8'\n",
      "send: 'AUTH PLAIN AHNjaGVkdWxlYWxhcnRzcnhtZ0BnbWFpbC5jb20AeXlqZXRlanRqaGpoenh4aw==\\r\\n'\n",
      "reply: b'235 2.7.0 Accepted\\r\\n'\n",
      "reply: retcode (235); Msg: b'2.7.0 Accepted'\n",
      "send: 'mail FROM:<schedulealartsrxmg@gmail.com> size=2855\\r\\n'\n",
      "reply: b'250 2.1.0 OK bl16-20020a05620a1a9000b00789da80c51asm998174qkb.1 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.1.0 OK bl16-20020a05620a1a9000b00789da80c51asm998174qkb.1 - gsmtp'\n",
      "send: 'rcpt TO:<lili@rxmg.com>\\r\\n'\n",
      "reply: b'250 2.1.5 OK bl16-20020a05620a1a9000b00789da80c51asm998174qkb.1 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.1.5 OK bl16-20020a05620a1a9000b00789da80c51asm998174qkb.1 - gsmtp'\n",
      "send: 'data\\r\\n'\n",
      "reply: b'354  Go ahead bl16-20020a05620a1a9000b00789da80c51asm998174qkb.1 - gsmtp\\r\\n'\n",
      "reply: retcode (354); Msg: b'Go ahead bl16-20020a05620a1a9000b00789da80c51asm998174qkb.1 - gsmtp'\n",
      "data: (354, b'Go ahead bl16-20020a05620a1a9000b00789da80c51asm998174qkb.1 - gsmtp')\n",
      "send: b'Content-Type: multipart/mixed; boundary=\"===============7520843358831348777==\"\\r\\nMIME-Version: 1.0\\r\\nFrom: schedulealartsrxmg@gmail.com\\r\\nTo: lili@rxmg.com\\r\\nSubject: SMS Daily Update Report\\r\\n\\r\\n--===============7520843358831348777==\\r\\nContent-Type: text/plain; charset=\"us-ascii\"\\r\\nMIME-Version: 1.0\\r\\nContent-Transfer-Encoding: 7bit\\r\\n\\r\\nThe revenue discrepency after November between raw file and Tableau data is $343.83\\r\\nThe total revenue after November is $2519578.31\\r\\nThe revenue discrepency is 0.0% of the total revenue after November\\r\\nThe revenue we made after November is $2519234.48\\r\\n\\r\\nThe revenue by month is\\r\\nMonth\\r\\n1899-12         0.000000\\r\\n1999-12       652.800000\\r\\n2020-02       163.100000\\r\\n2020-03      1116.300000\\r\\n2020-04      5060.200000\\r\\n2020-05      2218.760000\\r\\n2020-06      3551.800000\\r\\n2020-07      6578.100000\\r\\n2020-08     41870.600000\\r\\n2020-09     52336.370000\\r\\n2020-10     50779.900000\\r\\n2020-11     37785.730000\\r\\n2020-12     32358.100000\\r\\n2021-01     23948.040000\\r\\n2021-02     20137.840000\\r\\n2021-03     26584.030000\\r\\n2021-04     50262.400000\\r\\n2021-05     74756.520000\\r\\n2021-06     91146.100000\\r\\n2021-07    144141.810000\\r\\n2021-08    158429.500000\\r\\n2021-09    154983.178800\\r\\n2021-10    201033.211100\\r\\n2021-11    225191.850000\\r\\n2021-12    266561.980000\\r\\n2022-01    276933.964528\\r\\n2022-02    268013.423635\\r\\n2022-03    249501.175543\\r\\n2022-04    296194.537417\\r\\n2022-05    277356.288745\\r\\n2022-06    404793.833177\\r\\n2022-07    247402.947505\\r\\n2022-08    155207.055008\\r\\n2022-09    161623.944255\\r\\n2022-10     39895.442484\\r\\n2022-11     87337.163168\\r\\n2022-12    112797.439432\\r\\n2023-01    120882.586178\\r\\n2023-02    104659.552726\\r\\n2023-03    114402.376130\\r\\n2023-04    138092.207699\\r\\n2023-05    146932.479997\\r\\n2023-06    151868.903074\\r\\n2023-07    159070.812617\\r\\n2023-08    176552.914455\\r\\n2023-09    173993.620641\\r\\n2023-10    203605.155832\\r\\n2023-11    195007.793519\\r\\n2023-12    201446.806177\\r\\n2024-01    193449.194198\\r\\n2024-02    164773.410576\\r\\n2024-03     66320.312373\\r\\nNaT          2656.170000\\r\\nName: Revenue, dtype: float64\\r\\n\\r\\nThe revenue by month for unclaimed revenue is month\\r\\n2022-11    159.13\\r\\n2022-12    688.30\\r\\n2023-01    859.75\\r\\n2023-02    881.73\\r\\n2023-03    292.22\\r\\n2023-04    869.75\\r\\n2023-05    860.70\\r\\n2023-06     11.45\\r\\n2023-07      2.55\\r\\n2023-08    142.34\\r\\n2023-09    102.80\\r\\n2023-10     14.24\\r\\n2023-11      0.80\\r\\n2023-12      1.33\\r\\n2024-01     65.77\\r\\n2024-02    230.08\\r\\n2024-03    202.64\\r\\nName: amount, dtype: float64\\r\\n\\r\\n\\r\\nThe revenue by Date is\\r\\nDate\\r\\n1899-12-31       0.000000\\r\\n1999-12-31     652.800000\\r\\n2020-02-10       0.000000\\r\\n2020-02-11       4.500000\\r\\n2020-02-27      45.050000\\r\\n                 ...     \\r\\n2024-03-10    4011.650000\\r\\n2024-03-11    6560.563938\\r\\n2024-03-12    4115.810015\\r\\n2024-03-13    5153.014492\\r\\n2024-03-14     399.791500\\r\\nName: Revenue, Length: 1480, dtype: float64\\r\\n\\r\\n--===============7520843358831348777==--\\r\\n.\\r\\n'\n",
      "reply: b'250 2.0.0 OK  1710456602 bl16-20020a05620a1a9000b00789da80c51asm998174qkb.1 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.0.0 OK  1710456602 bl16-20020a05620a1a9000b00789da80c51asm998174qkb.1 - gsmtp'\n",
      "data: (250, b'2.0.0 OK  1710456602 bl16-20020a05620a1a9000b00789da80c51asm998174qkb.1 - gsmtp')\n",
      "send: 'quit\\r\\n'\n",
      "reply: b'221 2.0.0 closing connection bl16-20020a05620a1a9000b00789da80c51asm998174qkb.1 - gsmtp\\r\\n'\n",
      "reply: retcode (221); Msg: b'2.0.0 closing connection bl16-20020a05620a1a9000b00789da80c51asm998174qkb.1 - gsmtp'\n",
      "connect: to ('smtp.gmail.com', 587) None\n",
      "reply: b'220 smtp.gmail.com ESMTP p3-20020a05622a00c300b0042ef88b7daesm1276002qtw.19 - gsmtp\\r\\n'\n",
      "reply: retcode (220); Msg: b'smtp.gmail.com ESMTP p3-20020a05622a00c300b0042ef88b7daesm1276002qtw.19 - gsmtp'\n",
      "connect: b'smtp.gmail.com ESMTP p3-20020a05622a00c300b0042ef88b7daesm1276002qtw.19 - gsmtp'\n",
      "send: 'ehlo [100.96.2.115]\\r\\n'\n",
      "reply: b'250-smtp.gmail.com at your service, [52.38.54.186]\\r\\n'\n",
      "reply: b'250-SIZE 35882577\\r\\n'\n",
      "reply: b'250-8BITMIME\\r\\n'\n",
      "reply: b'250-STARTTLS\\r\\n'\n",
      "reply: b'250-ENHANCEDSTATUSCODES\\r\\n'\n",
      "reply: b'250-PIPELINING\\r\\n'\n",
      "reply: b'250-CHUNKING\\r\\n'\n",
      "reply: b'250 SMTPUTF8\\r\\n'\n",
      "reply: retcode (250); Msg: b'smtp.gmail.com at your service, [52.38.54.186]\\nSIZE 35882577\\n8BITMIME\\nSTARTTLS\\nENHANCEDSTATUSCODES\\nPIPELINING\\nCHUNKING\\nSMTPUTF8'\n",
      "send: 'STARTTLS\\r\\n'\n",
      "reply: b'220 2.0.0 Ready to start TLS\\r\\n'\n",
      "reply: retcode (220); Msg: b'2.0.0 Ready to start TLS'\n",
      "send: 'ehlo [100.96.2.115]\\r\\n'\n",
      "reply: b'250-smtp.gmail.com at your service, [52.38.54.186]\\r\\n'\n",
      "reply: b'250-SIZE 35882577\\r\\n'\n",
      "reply: b'250-8BITMIME\\r\\n'\n",
      "reply: b'250-AUTH LOGIN PLAIN XOAUTH2 PLAIN-CLIENTTOKEN OAUTHBEARER XOAUTH\\r\\n'\n",
      "reply: b'250-ENHANCEDSTATUSCODES\\r\\n'\n",
      "reply: b'250-PIPELINING\\r\\n'\n",
      "reply: b'250-CHUNKING\\r\\n'\n",
      "reply: b'250 SMTPUTF8\\r\\n'\n",
      "reply: retcode (250); Msg: b'smtp.gmail.com at your service, [52.38.54.186]\\nSIZE 35882577\\n8BITMIME\\nAUTH LOGIN PLAIN XOAUTH2 PLAIN-CLIENTTOKEN OAUTHBEARER XOAUTH\\nENHANCEDSTATUSCODES\\nPIPELINING\\nCHUNKING\\nSMTPUTF8'\n",
      "send: 'AUTH PLAIN AHNjaGVkdWxlYWxhcnRzcnhtZ0BnbWFpbC5jb20AeXlqZXRlanRqaGpoenh4aw==\\r\\n'\n",
      "reply: b'235 2.7.0 Accepted\\r\\n'\n",
      "reply: retcode (235); Msg: b'2.7.0 Accepted'\n",
      "send: 'mail FROM:<schedulealartsrxmg@gmail.com> size=2857\\r\\n'\n",
      "reply: b'250 2.1.0 OK p3-20020a05622a00c300b0042ef88b7daesm1276002qtw.19 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.1.0 OK p3-20020a05622a00c300b0042ef88b7daesm1276002qtw.19 - gsmtp'\n",
      "send: 'rcpt TO:<g.chao@rxmg.com>\\r\\n'\n",
      "reply: b'250 2.1.5 OK p3-20020a05622a00c300b0042ef88b7daesm1276002qtw.19 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.1.5 OK p3-20020a05622a00c300b0042ef88b7daesm1276002qtw.19 - gsmtp'\n",
      "send: 'data\\r\\n'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reply: b'354  Go ahead p3-20020a05622a00c300b0042ef88b7daesm1276002qtw.19 - gsmtp\\r\\n'\n",
      "reply: retcode (354); Msg: b'Go ahead p3-20020a05622a00c300b0042ef88b7daesm1276002qtw.19 - gsmtp'\n",
      "data: (354, b'Go ahead p3-20020a05622a00c300b0042ef88b7daesm1276002qtw.19 - gsmtp')\n",
      "send: b'Content-Type: multipart/mixed; boundary=\"===============1226419223496680606==\"\\r\\nMIME-Version: 1.0\\r\\nFrom: schedulealartsrxmg@gmail.com\\r\\nTo: g.chao@rxmg.com\\r\\nSubject: SMS Daily Update Report\\r\\n\\r\\n--===============1226419223496680606==\\r\\nContent-Type: text/plain; charset=\"us-ascii\"\\r\\nMIME-Version: 1.0\\r\\nContent-Transfer-Encoding: 7bit\\r\\n\\r\\nThe revenue discrepency after November between raw file and Tableau data is $343.83\\r\\nThe total revenue after November is $2519578.31\\r\\nThe revenue discrepency is 0.0% of the total revenue after November\\r\\nThe revenue we made after November is $2519234.48\\r\\n\\r\\nThe revenue by month is\\r\\nMonth\\r\\n1899-12         0.000000\\r\\n1999-12       652.800000\\r\\n2020-02       163.100000\\r\\n2020-03      1116.300000\\r\\n2020-04      5060.200000\\r\\n2020-05      2218.760000\\r\\n2020-06      3551.800000\\r\\n2020-07      6578.100000\\r\\n2020-08     41870.600000\\r\\n2020-09     52336.370000\\r\\n2020-10     50779.900000\\r\\n2020-11     37785.730000\\r\\n2020-12     32358.100000\\r\\n2021-01     23948.040000\\r\\n2021-02     20137.840000\\r\\n2021-03     26584.030000\\r\\n2021-04     50262.400000\\r\\n2021-05     74756.520000\\r\\n2021-06     91146.100000\\r\\n2021-07    144141.810000\\r\\n2021-08    158429.500000\\r\\n2021-09    154983.178800\\r\\n2021-10    201033.211100\\r\\n2021-11    225191.850000\\r\\n2021-12    266561.980000\\r\\n2022-01    276933.964528\\r\\n2022-02    268013.423635\\r\\n2022-03    249501.175543\\r\\n2022-04    296194.537417\\r\\n2022-05    277356.288745\\r\\n2022-06    404793.833177\\r\\n2022-07    247402.947505\\r\\n2022-08    155207.055008\\r\\n2022-09    161623.944255\\r\\n2022-10     39895.442484\\r\\n2022-11     87337.163168\\r\\n2022-12    112797.439432\\r\\n2023-01    120882.586178\\r\\n2023-02    104659.552726\\r\\n2023-03    114402.376130\\r\\n2023-04    138092.207699\\r\\n2023-05    146932.479997\\r\\n2023-06    151868.903074\\r\\n2023-07    159070.812617\\r\\n2023-08    176552.914455\\r\\n2023-09    173993.620641\\r\\n2023-10    203605.155832\\r\\n2023-11    195007.793519\\r\\n2023-12    201446.806177\\r\\n2024-01    193449.194198\\r\\n2024-02    164773.410576\\r\\n2024-03     66320.312373\\r\\nNaT          2656.170000\\r\\nName: Revenue, dtype: float64\\r\\n\\r\\nThe revenue by month for unclaimed revenue is month\\r\\n2022-11    159.13\\r\\n2022-12    688.30\\r\\n2023-01    859.75\\r\\n2023-02    881.73\\r\\n2023-03    292.22\\r\\n2023-04    869.75\\r\\n2023-05    860.70\\r\\n2023-06     11.45\\r\\n2023-07      2.55\\r\\n2023-08    142.34\\r\\n2023-09    102.80\\r\\n2023-10     14.24\\r\\n2023-11      0.80\\r\\n2023-12      1.33\\r\\n2024-01     65.77\\r\\n2024-02    230.08\\r\\n2024-03    202.64\\r\\nName: amount, dtype: float64\\r\\n\\r\\n\\r\\nThe revenue by Date is\\r\\nDate\\r\\n1899-12-31       0.000000\\r\\n1999-12-31     652.800000\\r\\n2020-02-10       0.000000\\r\\n2020-02-11       4.500000\\r\\n2020-02-27      45.050000\\r\\n                 ...     \\r\\n2024-03-10    4011.650000\\r\\n2024-03-11    6560.563938\\r\\n2024-03-12    4115.810015\\r\\n2024-03-13    5153.014492\\r\\n2024-03-14     399.791500\\r\\nName: Revenue, Length: 1480, dtype: float64\\r\\n\\r\\n--===============1226419223496680606==--\\r\\n.\\r\\n'\n",
      "reply: b'250 2.0.0 OK  1710456603 p3-20020a05622a00c300b0042ef88b7daesm1276002qtw.19 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.0.0 OK  1710456603 p3-20020a05622a00c300b0042ef88b7daesm1276002qtw.19 - gsmtp'\n",
      "data: (250, b'2.0.0 OK  1710456603 p3-20020a05622a00c300b0042ef88b7daesm1276002qtw.19 - gsmtp')\n",
      "send: 'quit\\r\\n'\n",
      "reply: b'221 2.0.0 closing connection p3-20020a05622a00c300b0042ef88b7daesm1276002qtw.19 - gsmtp\\r\\n'\n",
      "reply: retcode (221); Msg: b'2.0.0 closing connection p3-20020a05622a00c300b0042ef88b7daesm1276002qtw.19 - gsmtp'\n",
      "connect: to ('smtp.gmail.com', 587) None\n",
      "reply: b'220 smtp.gmail.com ESMTP kb11-20020a05622a448b00b0042f2f791592sm1266917qtb.63 - gsmtp\\r\\n'\n",
      "reply: retcode (220); Msg: b'smtp.gmail.com ESMTP kb11-20020a05622a448b00b0042f2f791592sm1266917qtb.63 - gsmtp'\n",
      "connect: b'smtp.gmail.com ESMTP kb11-20020a05622a448b00b0042f2f791592sm1266917qtb.63 - gsmtp'\n",
      "send: 'ehlo [100.96.2.115]\\r\\n'\n",
      "reply: b'250-smtp.gmail.com at your service, [52.38.54.186]\\r\\n'\n",
      "reply: b'250-SIZE 35882577\\r\\n'\n",
      "reply: b'250-8BITMIME\\r\\n'\n",
      "reply: b'250-STARTTLS\\r\\n'\n",
      "reply: b'250-ENHANCEDSTATUSCODES\\r\\n'\n",
      "reply: b'250-PIPELINING\\r\\n'\n",
      "reply: b'250-CHUNKING\\r\\n'\n",
      "reply: b'250 SMTPUTF8\\r\\n'\n",
      "reply: retcode (250); Msg: b'smtp.gmail.com at your service, [52.38.54.186]\\nSIZE 35882577\\n8BITMIME\\nSTARTTLS\\nENHANCEDSTATUSCODES\\nPIPELINING\\nCHUNKING\\nSMTPUTF8'\n",
      "send: 'STARTTLS\\r\\n'\n",
      "reply: b'220 2.0.0 Ready to start TLS\\r\\n'\n",
      "reply: retcode (220); Msg: b'2.0.0 Ready to start TLS'\n",
      "send: 'ehlo [100.96.2.115]\\r\\n'\n",
      "reply: b'250-smtp.gmail.com at your service, [52.38.54.186]\\r\\n'\n",
      "reply: b'250-SIZE 35882577\\r\\n'\n",
      "reply: b'250-8BITMIME\\r\\n'\n",
      "reply: b'250-AUTH LOGIN PLAIN XOAUTH2 PLAIN-CLIENTTOKEN OAUTHBEARER XOAUTH\\r\\n'\n",
      "reply: b'250-ENHANCEDSTATUSCODES\\r\\n'\n",
      "reply: b'250-PIPELINING\\r\\n'\n",
      "reply: b'250-CHUNKING\\r\\n'\n",
      "reply: b'250 SMTPUTF8\\r\\n'\n",
      "reply: retcode (250); Msg: b'smtp.gmail.com at your service, [52.38.54.186]\\nSIZE 35882577\\n8BITMIME\\nAUTH LOGIN PLAIN XOAUTH2 PLAIN-CLIENTTOKEN OAUTHBEARER XOAUTH\\nENHANCEDSTATUSCODES\\nPIPELINING\\nCHUNKING\\nSMTPUTF8'\n",
      "send: 'AUTH PLAIN AHNjaGVkdWxlYWxhcnRzcnhtZ0BnbWFpbC5jb20AeXlqZXRlanRqaGpoenh4aw==\\r\\n'\n",
      "reply: b'235 2.7.0 Accepted\\r\\n'\n",
      "reply: retcode (235); Msg: b'2.7.0 Accepted'\n",
      "send: 'mail FROM:<schedulealartsrxmg@gmail.com> size=2857\\r\\n'\n",
      "reply: b'250 2.1.0 OK kb11-20020a05622a448b00b0042f2f791592sm1266917qtb.63 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.1.0 OK kb11-20020a05622a448b00b0042f2f791592sm1266917qtb.63 - gsmtp'\n",
      "send: 'rcpt TO:<nathan@rxmg.com>\\r\\n'\n",
      "reply: b'250 2.1.5 OK kb11-20020a05622a448b00b0042f2f791592sm1266917qtb.63 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.1.5 OK kb11-20020a05622a448b00b0042f2f791592sm1266917qtb.63 - gsmtp'\n",
      "send: 'data\\r\\n'\n",
      "reply: b'354  Go ahead kb11-20020a05622a448b00b0042f2f791592sm1266917qtb.63 - gsmtp\\r\\n'\n",
      "reply: retcode (354); Msg: b'Go ahead kb11-20020a05622a448b00b0042f2f791592sm1266917qtb.63 - gsmtp'\n",
      "data: (354, b'Go ahead kb11-20020a05622a448b00b0042f2f791592sm1266917qtb.63 - gsmtp')\n",
      "send: b'Content-Type: multipart/mixed; boundary=\"===============5480712705104988745==\"\\r\\nMIME-Version: 1.0\\r\\nFrom: schedulealartsrxmg@gmail.com\\r\\nTo: nathan@rxmg.com\\r\\nSubject: SMS Daily Update Report\\r\\n\\r\\n--===============5480712705104988745==\\r\\nContent-Type: text/plain; charset=\"us-ascii\"\\r\\nMIME-Version: 1.0\\r\\nContent-Transfer-Encoding: 7bit\\r\\n\\r\\nThe revenue discrepency after November between raw file and Tableau data is $343.83\\r\\nThe total revenue after November is $2519578.31\\r\\nThe revenue discrepency is 0.0% of the total revenue after November\\r\\nThe revenue we made after November is $2519234.48\\r\\n\\r\\nThe revenue by month is\\r\\nMonth\\r\\n1899-12         0.000000\\r\\n1999-12       652.800000\\r\\n2020-02       163.100000\\r\\n2020-03      1116.300000\\r\\n2020-04      5060.200000\\r\\n2020-05      2218.760000\\r\\n2020-06      3551.800000\\r\\n2020-07      6578.100000\\r\\n2020-08     41870.600000\\r\\n2020-09     52336.370000\\r\\n2020-10     50779.900000\\r\\n2020-11     37785.730000\\r\\n2020-12     32358.100000\\r\\n2021-01     23948.040000\\r\\n2021-02     20137.840000\\r\\n2021-03     26584.030000\\r\\n2021-04     50262.400000\\r\\n2021-05     74756.520000\\r\\n2021-06     91146.100000\\r\\n2021-07    144141.810000\\r\\n2021-08    158429.500000\\r\\n2021-09    154983.178800\\r\\n2021-10    201033.211100\\r\\n2021-11    225191.850000\\r\\n2021-12    266561.980000\\r\\n2022-01    276933.964528\\r\\n2022-02    268013.423635\\r\\n2022-03    249501.175543\\r\\n2022-04    296194.537417\\r\\n2022-05    277356.288745\\r\\n2022-06    404793.833177\\r\\n2022-07    247402.947505\\r\\n2022-08    155207.055008\\r\\n2022-09    161623.944255\\r\\n2022-10     39895.442484\\r\\n2022-11     87337.163168\\r\\n2022-12    112797.439432\\r\\n2023-01    120882.586178\\r\\n2023-02    104659.552726\\r\\n2023-03    114402.376130\\r\\n2023-04    138092.207699\\r\\n2023-05    146932.479997\\r\\n2023-06    151868.903074\\r\\n2023-07    159070.812617\\r\\n2023-08    176552.914455\\r\\n2023-09    173993.620641\\r\\n2023-10    203605.155832\\r\\n2023-11    195007.793519\\r\\n2023-12    201446.806177\\r\\n2024-01    193449.194198\\r\\n2024-02    164773.410576\\r\\n2024-03     66320.312373\\r\\nNaT          2656.170000\\r\\nName: Revenue, dtype: float64\\r\\n\\r\\nThe revenue by month for unclaimed revenue is month\\r\\n2022-11    159.13\\r\\n2022-12    688.30\\r\\n2023-01    859.75\\r\\n2023-02    881.73\\r\\n2023-03    292.22\\r\\n2023-04    869.75\\r\\n2023-05    860.70\\r\\n2023-06     11.45\\r\\n2023-07      2.55\\r\\n2023-08    142.34\\r\\n2023-09    102.80\\r\\n2023-10     14.24\\r\\n2023-11      0.80\\r\\n2023-12      1.33\\r\\n2024-01     65.77\\r\\n2024-02    230.08\\r\\n2024-03    202.64\\r\\nName: amount, dtype: float64\\r\\n\\r\\n\\r\\nThe revenue by Date is\\r\\nDate\\r\\n1899-12-31       0.000000\\r\\n1999-12-31     652.800000\\r\\n2020-02-10       0.000000\\r\\n2020-02-11       4.500000\\r\\n2020-02-27      45.050000\\r\\n                 ...     \\r\\n2024-03-10    4011.650000\\r\\n2024-03-11    6560.563938\\r\\n2024-03-12    4115.810015\\r\\n2024-03-13    5153.014492\\r\\n2024-03-14     399.791500\\r\\nName: Revenue, Length: 1480, dtype: float64\\r\\n\\r\\n--===============5480712705104988745==--\\r\\n.\\r\\n'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reply: b'250 2.0.0 OK  1710456604 kb11-20020a05622a448b00b0042f2f791592sm1266917qtb.63 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.0.0 OK  1710456604 kb11-20020a05622a448b00b0042f2f791592sm1266917qtb.63 - gsmtp'\n",
      "data: (250, b'2.0.0 OK  1710456604 kb11-20020a05622a448b00b0042f2f791592sm1266917qtb.63 - gsmtp')\n",
      "send: 'quit\\r\\n'\n",
      "reply: b'221 2.0.0 closing connection kb11-20020a05622a448b00b0042f2f791592sm1266917qtb.63 - gsmtp\\r\\n'\n",
      "reply: retcode (221); Msg: b'2.0.0 closing connection kb11-20020a05622a448b00b0042f2f791592sm1266917qtb.63 - gsmtp'\n",
      "connect: to ('smtp.gmail.com', 587) None\n",
      "reply: b'220 smtp.gmail.com ESMTP s14-20020a0562140cae00b0069102f97e08sm1051094qvs.97 - gsmtp\\r\\n'\n",
      "reply: retcode (220); Msg: b'smtp.gmail.com ESMTP s14-20020a0562140cae00b0069102f97e08sm1051094qvs.97 - gsmtp'\n",
      "connect: b'smtp.gmail.com ESMTP s14-20020a0562140cae00b0069102f97e08sm1051094qvs.97 - gsmtp'\n",
      "send: 'ehlo [100.96.2.115]\\r\\n'\n",
      "reply: b'250-smtp.gmail.com at your service, [52.38.54.186]\\r\\n'\n",
      "reply: b'250-SIZE 35882577\\r\\n'\n",
      "reply: b'250-8BITMIME\\r\\n'\n",
      "reply: b'250-STARTTLS\\r\\n'\n",
      "reply: b'250-ENHANCEDSTATUSCODES\\r\\n'\n",
      "reply: b'250-PIPELINING\\r\\n'\n",
      "reply: b'250-CHUNKING\\r\\n'\n",
      "reply: b'250 SMTPUTF8\\r\\n'\n",
      "reply: retcode (250); Msg: b'smtp.gmail.com at your service, [52.38.54.186]\\nSIZE 35882577\\n8BITMIME\\nSTARTTLS\\nENHANCEDSTATUSCODES\\nPIPELINING\\nCHUNKING\\nSMTPUTF8'\n",
      "send: 'STARTTLS\\r\\n'\n",
      "reply: b'220 2.0.0 Ready to start TLS\\r\\n'\n",
      "reply: retcode (220); Msg: b'2.0.0 Ready to start TLS'\n",
      "send: 'ehlo [100.96.2.115]\\r\\n'\n",
      "reply: b'250-smtp.gmail.com at your service, [52.38.54.186]\\r\\n'\n",
      "reply: b'250-SIZE 35882577\\r\\n'\n",
      "reply: b'250-8BITMIME\\r\\n'\n",
      "reply: b'250-AUTH LOGIN PLAIN XOAUTH2 PLAIN-CLIENTTOKEN OAUTHBEARER XOAUTH\\r\\n'\n",
      "reply: b'250-ENHANCEDSTATUSCODES\\r\\n'\n",
      "reply: b'250-PIPELINING\\r\\n'\n",
      "reply: b'250-CHUNKING\\r\\n'\n",
      "reply: b'250 SMTPUTF8\\r\\n'\n",
      "reply: retcode (250); Msg: b'smtp.gmail.com at your service, [52.38.54.186]\\nSIZE 35882577\\n8BITMIME\\nAUTH LOGIN PLAIN XOAUTH2 PLAIN-CLIENTTOKEN OAUTHBEARER XOAUTH\\nENHANCEDSTATUSCODES\\nPIPELINING\\nCHUNKING\\nSMTPUTF8'\n",
      "send: 'AUTH PLAIN AHNjaGVkdWxlYWxhcnRzcnhtZ0BnbWFpbC5jb20AeXlqZXRlanRqaGpoenh4aw==\\r\\n'\n",
      "reply: b'235 2.7.0 Accepted\\r\\n'\n",
      "reply: retcode (235); Msg: b'2.7.0 Accepted'\n",
      "send: 'mail FROM:<schedulealartsrxmg@gmail.com> size=2855\\r\\n'\n",
      "reply: b'250 2.1.0 OK s14-20020a0562140cae00b0069102f97e08sm1051094qvs.97 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.1.0 OK s14-20020a0562140cae00b0069102f97e08sm1051094qvs.97 - gsmtp'\n",
      "send: 'rcpt TO:<nina@rxmg.com>\\r\\n'\n",
      "reply: b'250 2.1.5 OK s14-20020a0562140cae00b0069102f97e08sm1051094qvs.97 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.1.5 OK s14-20020a0562140cae00b0069102f97e08sm1051094qvs.97 - gsmtp'\n",
      "send: 'data\\r\\n'\n",
      "reply: b'354  Go ahead s14-20020a0562140cae00b0069102f97e08sm1051094qvs.97 - gsmtp\\r\\n'\n",
      "reply: retcode (354); Msg: b'Go ahead s14-20020a0562140cae00b0069102f97e08sm1051094qvs.97 - gsmtp'\n",
      "data: (354, b'Go ahead s14-20020a0562140cae00b0069102f97e08sm1051094qvs.97 - gsmtp')\n",
      "send: b'Content-Type: multipart/mixed; boundary=\"===============8918873085384837624==\"\\r\\nMIME-Version: 1.0\\r\\nFrom: schedulealartsrxmg@gmail.com\\r\\nTo: nina@rxmg.com\\r\\nSubject: SMS Daily Update Report\\r\\n\\r\\n--===============8918873085384837624==\\r\\nContent-Type: text/plain; charset=\"us-ascii\"\\r\\nMIME-Version: 1.0\\r\\nContent-Transfer-Encoding: 7bit\\r\\n\\r\\nThe revenue discrepency after November between raw file and Tableau data is $343.83\\r\\nThe total revenue after November is $2519578.31\\r\\nThe revenue discrepency is 0.0% of the total revenue after November\\r\\nThe revenue we made after November is $2519234.48\\r\\n\\r\\nThe revenue by month is\\r\\nMonth\\r\\n1899-12         0.000000\\r\\n1999-12       652.800000\\r\\n2020-02       163.100000\\r\\n2020-03      1116.300000\\r\\n2020-04      5060.200000\\r\\n2020-05      2218.760000\\r\\n2020-06      3551.800000\\r\\n2020-07      6578.100000\\r\\n2020-08     41870.600000\\r\\n2020-09     52336.370000\\r\\n2020-10     50779.900000\\r\\n2020-11     37785.730000\\r\\n2020-12     32358.100000\\r\\n2021-01     23948.040000\\r\\n2021-02     20137.840000\\r\\n2021-03     26584.030000\\r\\n2021-04     50262.400000\\r\\n2021-05     74756.520000\\r\\n2021-06     91146.100000\\r\\n2021-07    144141.810000\\r\\n2021-08    158429.500000\\r\\n2021-09    154983.178800\\r\\n2021-10    201033.211100\\r\\n2021-11    225191.850000\\r\\n2021-12    266561.980000\\r\\n2022-01    276933.964528\\r\\n2022-02    268013.423635\\r\\n2022-03    249501.175543\\r\\n2022-04    296194.537417\\r\\n2022-05    277356.288745\\r\\n2022-06    404793.833177\\r\\n2022-07    247402.947505\\r\\n2022-08    155207.055008\\r\\n2022-09    161623.944255\\r\\n2022-10     39895.442484\\r\\n2022-11     87337.163168\\r\\n2022-12    112797.439432\\r\\n2023-01    120882.586178\\r\\n2023-02    104659.552726\\r\\n2023-03    114402.376130\\r\\n2023-04    138092.207699\\r\\n2023-05    146932.479997\\r\\n2023-06    151868.903074\\r\\n2023-07    159070.812617\\r\\n2023-08    176552.914455\\r\\n2023-09    173993.620641\\r\\n2023-10    203605.155832\\r\\n2023-11    195007.793519\\r\\n2023-12    201446.806177\\r\\n2024-01    193449.194198\\r\\n2024-02    164773.410576\\r\\n2024-03     66320.312373\\r\\nNaT          2656.170000\\r\\nName: Revenue, dtype: float64\\r\\n\\r\\nThe revenue by month for unclaimed revenue is month\\r\\n2022-11    159.13\\r\\n2022-12    688.30\\r\\n2023-01    859.75\\r\\n2023-02    881.73\\r\\n2023-03    292.22\\r\\n2023-04    869.75\\r\\n2023-05    860.70\\r\\n2023-06     11.45\\r\\n2023-07      2.55\\r\\n2023-08    142.34\\r\\n2023-09    102.80\\r\\n2023-10     14.24\\r\\n2023-11      0.80\\r\\n2023-12      1.33\\r\\n2024-01     65.77\\r\\n2024-02    230.08\\r\\n2024-03    202.64\\r\\nName: amount, dtype: float64\\r\\n\\r\\n\\r\\nThe revenue by Date is\\r\\nDate\\r\\n1899-12-31       0.000000\\r\\n1999-12-31     652.800000\\r\\n2020-02-10       0.000000\\r\\n2020-02-11       4.500000\\r\\n2020-02-27      45.050000\\r\\n                 ...     \\r\\n2024-03-10    4011.650000\\r\\n2024-03-11    6560.563938\\r\\n2024-03-12    4115.810015\\r\\n2024-03-13    5153.014492\\r\\n2024-03-14     399.791500\\r\\nName: Revenue, Length: 1480, dtype: float64\\r\\n\\r\\n--===============8918873085384837624==--\\r\\n.\\r\\n'\n",
      "reply: b'250 2.0.0 OK  1710456606 s14-20020a0562140cae00b0069102f97e08sm1051094qvs.97 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.0.0 OK  1710456606 s14-20020a0562140cae00b0069102f97e08sm1051094qvs.97 - gsmtp'\n",
      "data: (250, b'2.0.0 OK  1710456606 s14-20020a0562140cae00b0069102f97e08sm1051094qvs.97 - gsmtp')\n",
      "send: 'quit\\r\\n'\n",
      "reply: b'221 2.0.0 closing connection s14-20020a0562140cae00b0069102f97e08sm1051094qvs.97 - gsmtp\\r\\n'\n",
      "reply: retcode (221); Msg: b'2.0.0 closing connection s14-20020a0562140cae00b0069102f97e08sm1051094qvs.97 - gsmtp'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "toaddr = ['lili@rxmg.com','g.chao@rxmg.com','nathan@rxmg.com','nina@rxmg.com']\n",
    "#toaddr = ['lili@rxmg.com']\n",
    "subject_line = 'SMS Daily Update Report' \n",
    "email_body \n",
    "for i in toaddr:\n",
    "    send_email.send_email('',subject_line,email_body,i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc258a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
