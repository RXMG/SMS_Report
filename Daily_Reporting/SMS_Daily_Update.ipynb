{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fa84859",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-10T16:00:55.815503Z",
     "start_time": "2023-08-10T15:57:41.600924Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\lilig\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pygsheets\\worksheet.py:1554: UserWarning: At least one column name in the data frame is an empty string. If this is a concern, please specify include_tailing_empty=False and/or ensure that each column containing data has a name.\n",
      "  warnings.warn('At least one column name in the data frame is an empty string. If this is a concern, please specify include_tailing_empty=False and/or ensure that each column containing data has a name.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139689398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\lilig\\AppData\\Local\\anaconda3\\Lib\\site-packages\\dateutil\\parser\\_parser.py:1207: UnknownTimezoneWarning: tzname EST identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n",
      "D:\\Users\\lilig\\AppData\\Local\\anaconda3\\Lib\\site-packages\\dateutil\\parser\\_parser.py:1207: UnknownTimezoneWarning: tzname EST identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_11540\\2433679446.py:136: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  combined_csv = pd.concat([pd.read_csv(f) for f in all_files ])\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_11540\\2433679446.py:136: DtypeWarning: Columns (7,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  combined_csv = pd.concat([pd.read_csv(f) for f in all_files ])\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_11540\\2433679446.py:136: DtypeWarning: Columns (9,10,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  combined_csv = pd.concat([pd.read_csv(f) for f in all_files ])\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_11540\\2433679446.py:136: DtypeWarning: Columns (10,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  combined_csv = pd.concat([pd.read_csv(f) for f in all_files ])\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_11540\\2433679446.py:136: DtypeWarning: Columns (10,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  combined_csv = pd.concat([pd.read_csv(f) for f in all_files ])\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_11540\\2433679446.py:136: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  combined_csv = pd.concat([pd.read_csv(f) for f in all_files ])\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_11540\\2433679446.py:136: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  combined_csv = pd.concat([pd.read_csv(f) for f in all_files ])\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_11540\\2433679446.py:136: DtypeWarning: Columns (7,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  combined_csv = pd.concat([pd.read_csv(f) for f in all_files ])\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_11540\\2433679446.py:136: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  combined_csv = pd.concat([pd.read_csv(f) for f in all_files ])\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_11540\\2433679446.py:136: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  combined_csv = pd.concat([pd.read_csv(f) for f in all_files ])\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_11540\\2433679446.py:136: DtypeWarning: Columns (7,9,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  combined_csv = pd.concat([pd.read_csv(f) for f in all_files ])\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_11540\\2433679446.py:136: DtypeWarning: Columns (7,9,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  combined_csv = pd.concat([pd.read_csv(f) for f in all_files ])\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_11540\\2433679446.py:136: DtypeWarning: Columns (7,9,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  combined_csv = pd.concat([pd.read_csv(f) for f in all_files ])\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_11540\\2433679446.py:136: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  combined_csv = pd.concat([pd.read_csv(f) for f in all_files ])\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_11540\\2433679446.py:136: DtypeWarning: Columns (9,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  combined_csv = pd.concat([pd.read_csv(f) for f in all_files ])\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_11540\\2433679446.py:136: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  combined_csv = pd.concat([pd.read_csv(f) for f in all_files ])\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_11540\\2433679446.py:136: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  combined_csv = pd.concat([pd.read_csv(f) for f in all_files ])\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_11540\\2433679446.py:136: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  combined_csv = pd.concat([pd.read_csv(f) for f in all_files ])\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_11540\\2433679446.py:141: DtypeWarning: Columns (7,9,10,11,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  combined_csv = pd.read_csv(localfolder + 'SMS_master_revenue.csv', dtype={'advertiser_name':'str','campaign_name':'str'})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6028064.896347008\n",
      "6028064.896347008\n",
      "7054025.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>advertiser_name</th>\n",
       "      <th>campaign_id</th>\n",
       "      <th>campaign_name</th>\n",
       "      <th>site_id</th>\n",
       "      <th>affiliate_id</th>\n",
       "      <th>affiliate_name</th>\n",
       "      <th>subid_1</th>\n",
       "      <th>subid_2</th>\n",
       "      <th>subid_3</th>\n",
       "      <th>...</th>\n",
       "      <th>Jump Page Clicks</th>\n",
       "      <th>subid_1 uc</th>\n",
       "      <th>subid_1 ucs</th>\n",
       "      <th>subid_2 uc</th>\n",
       "      <th>subid_2 ucs</th>\n",
       "      <th>subid_5 uc</th>\n",
       "      <th>subid_5 ucs</th>\n",
       "      <th>sub_id</th>\n",
       "      <th>first_split</th>\n",
       "      <th>Dash_Date_from_subid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1503163</th>\n",
       "      <td>2021-06-30</td>\n",
       "      <td>Progrexion</td>\n",
       "      <td>8202.0</td>\n",
       "      <td>Lexington Law Linkout - 33824 - Progrexion (SMS)</td>\n",
       "      <td>460917.0</td>\n",
       "      <td>460917</td>\n",
       "      <td>1 - SMS LM: Jet Marketing</td>\n",
       "      <td>460917</td>\n",
       "      <td>MP_FT_460917_CT4231_JM.OP_C15_6571_82162_30Jun...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>MP_FT_460917_CT4231_JM.OP_C15_6571_82162_30Jun...</td>\n",
       "      <td>MP</td>\n",
       "      <td>2021-06-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1514201</th>\n",
       "      <td>2021-06-24</td>\n",
       "      <td>Progrexion</td>\n",
       "      <td>8202.0</td>\n",
       "      <td>Lexington Law Linkout - 33824 - Progrexion (SMS)</td>\n",
       "      <td>460939.0</td>\n",
       "      <td>460939</td>\n",
       "      <td>1 - SMS LM: Aramis_Credit</td>\n",
       "      <td>460939</td>\n",
       "      <td>03.04.21_ARM_460939_a_6571_SC_MP_j3-l_||postid||</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>03.04.21_ARM_460939_a_6571_SC_MP_j3-l_||postid||</td>\n",
       "      <td>03.04.21</td>\n",
       "      <td>2021-06-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1539629</th>\n",
       "      <td>2021-06-15</td>\n",
       "      <td>Progrexion</td>\n",
       "      <td>8202.0</td>\n",
       "      <td>Lexington Law Linkout - 33824 - Progrexion (SMS)</td>\n",
       "      <td>460654.0</td>\n",
       "      <td>460654</td>\n",
       "      <td>1 - SMS: RentOwnClub</td>\n",
       "      <td>460654</td>\n",
       "      <td>MP_HZB_460654_CT4073_I.RC_C45_6269_44345_15Jun...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>MP_HZB_460654_CT4073_I.RC_C45_6269_44345_15Jun...</td>\n",
       "      <td>MP</td>\n",
       "      <td>2021-06-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1557044</th>\n",
       "      <td>2021-06-06</td>\n",
       "      <td>Progrexion</td>\n",
       "      <td>8202.0</td>\n",
       "      <td>Lexington Law Linkout - 33824 - Progrexion (SMS)</td>\n",
       "      <td>460654.0</td>\n",
       "      <td>460654</td>\n",
       "      <td>1 - SMS: RentOwnClub</td>\n",
       "      <td>460654</td>\n",
       "      <td>MP_HZB_460654_CT3983_I.RC_C15_6264_44345_6Jun2...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>MP_HZB_460654_CT3983_I.RC_C15_6264_44345_6Jun2...</td>\n",
       "      <td>MP</td>\n",
       "      <td>2021-06-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4234985</th>\n",
       "      <td>2022-11-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8202.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>460654</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>SMS_SS_SC_HZB_44345_460654_416732_6264_124652_...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SMS_SS_SC_HZB_44345_460654_416732_6264_124652_...</td>\n",
       "      <td>SMS</td>\n",
       "      <td>2022-11-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6551754</th>\n",
       "      <td>2023-10-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8202.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>461810</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>SS_SC_SVT_61659_461810_AR2_11646_533010_25Oct23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SS_SC_SVT_61659_461810_AR2_11646_533010_25Oct23</td>\n",
       "      <td>SS</td>\n",
       "      <td>2023-10-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6551755</th>\n",
       "      <td>2023-10-28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8202.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>461810</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>SS_SC_SVT_61659_461810_AR2_11646_533010_28Oct23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SS_SC_SVT_61659_461810_AR2_11646_533010_28Oct23</td>\n",
       "      <td>SS</td>\n",
       "      <td>2023-10-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6551756</th>\n",
       "      <td>2023-10-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8202.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>461810</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>SS_SC_SVT_61659_461810_AR2_11646_538710_29Oct23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SS_SC_SVT_61659_461810_AR2_11646_538710_29Oct23</td>\n",
       "      <td>SS</td>\n",
       "      <td>2023-10-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6551757</th>\n",
       "      <td>2023-10-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8202.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>461810</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>SS_SC_SVT_61659_461810_AR2_11646_538936_31Oct23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SS_SC_SVT_61659_461810_AR2_11646_538936_31Oct23</td>\n",
       "      <td>SS</td>\n",
       "      <td>2023-10-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6551758</th>\n",
       "      <td>2023-11-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8202.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>461810</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>SS_SC_SVT_61659_461810_AR2_11646_538936_6Nov23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SS_SC_SVT_61659_461810_AR2_11646_538936_6Nov23</td>\n",
       "      <td>SS</td>\n",
       "      <td>2023-11-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1644 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              date advertiser_name  campaign_id  \\\n",
       "1503163 2021-06-30      Progrexion       8202.0   \n",
       "1514201 2021-06-24      Progrexion       8202.0   \n",
       "1539629 2021-06-15      Progrexion       8202.0   \n",
       "1557044 2021-06-06      Progrexion       8202.0   \n",
       "4234985 2022-11-02             NaN       8202.0   \n",
       "...            ...             ...          ...   \n",
       "6551754 2023-10-25             NaN       8202.0   \n",
       "6551755 2023-10-28             NaN       8202.0   \n",
       "6551756 2023-10-29             NaN       8202.0   \n",
       "6551757 2023-10-31             NaN       8202.0   \n",
       "6551758 2023-11-06             NaN       8202.0   \n",
       "\n",
       "                                            campaign_name   site_id  \\\n",
       "1503163  Lexington Law Linkout - 33824 - Progrexion (SMS)  460917.0   \n",
       "1514201  Lexington Law Linkout - 33824 - Progrexion (SMS)  460939.0   \n",
       "1539629  Lexington Law Linkout - 33824 - Progrexion (SMS)  460654.0   \n",
       "1557044  Lexington Law Linkout - 33824 - Progrexion (SMS)  460654.0   \n",
       "4234985                                               NaN       NaN   \n",
       "...                                                   ...       ...   \n",
       "6551754                                               NaN       NaN   \n",
       "6551755                                               NaN       NaN   \n",
       "6551756                                               NaN       NaN   \n",
       "6551757                                               NaN       NaN   \n",
       "6551758                                               NaN       NaN   \n",
       "\n",
       "         affiliate_id             affiliate_name subid_1  \\\n",
       "1503163        460917  1 - SMS LM: Jet Marketing  460917   \n",
       "1514201        460939  1 - SMS LM: Aramis_Credit  460939   \n",
       "1539629        460654       1 - SMS: RentOwnClub  460654   \n",
       "1557044        460654       1 - SMS: RentOwnClub  460654   \n",
       "4234985        460654                        NaN     nan   \n",
       "...               ...                        ...     ...   \n",
       "6551754        461810                        NaN     nan   \n",
       "6551755        461810                        NaN     nan   \n",
       "6551756        461810                        NaN     nan   \n",
       "6551757        461810                        NaN     nan   \n",
       "6551758        461810                        NaN     nan   \n",
       "\n",
       "                                                   subid_2 subid_3  ...  \\\n",
       "1503163  MP_FT_460917_CT4231_JM.OP_C15_6571_82162_30Jun...     NaN  ...   \n",
       "1514201   03.04.21_ARM_460939_a_6571_SC_MP_j3-l_||postid||     NaN  ...   \n",
       "1539629  MP_HZB_460654_CT4073_I.RC_C45_6269_44345_15Jun...     NaN  ...   \n",
       "1557044  MP_HZB_460654_CT3983_I.RC_C15_6264_44345_6Jun2...     NaN  ...   \n",
       "4234985  SMS_SS_SC_HZB_44345_460654_416732_6264_124652_...     NaN  ...   \n",
       "...                                                    ...     ...  ...   \n",
       "6551754    SS_SC_SVT_61659_461810_AR2_11646_533010_25Oct23     NaN  ...   \n",
       "6551755    SS_SC_SVT_61659_461810_AR2_11646_533010_28Oct23     NaN  ...   \n",
       "6551756    SS_SC_SVT_61659_461810_AR2_11646_538710_29Oct23     NaN  ...   \n",
       "6551757    SS_SC_SVT_61659_461810_AR2_11646_538936_31Oct23     NaN  ...   \n",
       "6551758     SS_SC_SVT_61659_461810_AR2_11646_538936_6Nov23     NaN  ...   \n",
       "\n",
       "        Jump Page Clicks subid_1 uc  subid_1 ucs subid_2 uc  subid_2 ucs  \\\n",
       "1503163              NaN          0            0         10            0   \n",
       "1514201              NaN          0            0          8            0   \n",
       "1539629              NaN          0            0         13            0   \n",
       "1557044              NaN          0            0         13            0   \n",
       "4234985              4.0          0            0         10            0   \n",
       "...                  ...        ...          ...        ...          ...   \n",
       "6551754              1.0          0            0          8            0   \n",
       "6551755              1.0          0            0          8            0   \n",
       "6551756              1.0          0            0          8            0   \n",
       "6551757              1.0          0            0          8            0   \n",
       "6551758              1.0          0            0          8            0   \n",
       "\n",
       "         subid_5 uc  subid_5 ucs  \\\n",
       "1503163           0            0   \n",
       "1514201           0            0   \n",
       "1539629           0            0   \n",
       "1557044           0            0   \n",
       "4234985           0            0   \n",
       "...             ...          ...   \n",
       "6551754           0            0   \n",
       "6551755           0            0   \n",
       "6551756           0            0   \n",
       "6551757           0            0   \n",
       "6551758           0            0   \n",
       "\n",
       "                                                    sub_id first_split  \\\n",
       "1503163  MP_FT_460917_CT4231_JM.OP_C15_6571_82162_30Jun...          MP   \n",
       "1514201   03.04.21_ARM_460939_a_6571_SC_MP_j3-l_||postid||    03.04.21   \n",
       "1539629  MP_HZB_460654_CT4073_I.RC_C45_6269_44345_15Jun...          MP   \n",
       "1557044  MP_HZB_460654_CT3983_I.RC_C15_6264_44345_6Jun2...          MP   \n",
       "4234985  SMS_SS_SC_HZB_44345_460654_416732_6264_124652_...         SMS   \n",
       "...                                                    ...         ...   \n",
       "6551754    SS_SC_SVT_61659_461810_AR2_11646_533010_25Oct23          SS   \n",
       "6551755    SS_SC_SVT_61659_461810_AR2_11646_533010_28Oct23          SS   \n",
       "6551756    SS_SC_SVT_61659_461810_AR2_11646_538710_29Oct23          SS   \n",
       "6551757    SS_SC_SVT_61659_461810_AR2_11646_538936_31Oct23          SS   \n",
       "6551758     SS_SC_SVT_61659_461810_AR2_11646_538936_6Nov23          SS   \n",
       "\n",
       "        Dash_Date_from_subid  \n",
       "1503163           2021-06-30  \n",
       "1514201           2021-06-24  \n",
       "1539629           2021-06-15  \n",
       "1557044           2021-06-06  \n",
       "4234985           2022-11-02  \n",
       "...                      ...  \n",
       "6551754           2023-10-25  \n",
       "6551755           2023-10-28  \n",
       "6551756           2023-10-29  \n",
       "6551757           2023-10-31  \n",
       "6551758           2023-11-06  \n",
       "\n",
       "[1644 rows x 30 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import filepath\n",
    "import send_email \n",
    "import pygsheets\n",
    "import infrastructure \n",
    "\n",
    "\n",
    "localfolder=filepath.output_folder\n",
    "\n",
    "#import publisher from google sheet \n",
    "gc = pygsheets.authorize(service_account_file=filepath.service_account_location)\n",
    "rxmgref = gc.open_by_url('https://docs.google.com/spreadsheets/d/1Tzda6Djr3zQmOhWu7Ief3GVR9Cjaml8238CeX7chj_U/edit#gid=1620368362') \n",
    "publisher_raw  = rxmgref.worksheet('title','Publisher Configurations').get_as_df()\n",
    "publisher_raw.to_csv(localfolder+'smartsheet/Publisher.csv')\n",
    "publisher = publisher_raw[['NEW DP.DS or DP.sV','PUBID']].drop_duplicates()\n",
    "\n",
    "# import SMS OMS\n",
    "offer_sheet = infrastructure.get_smartsheet('offers_sms')\n",
    "\n",
    "## import La Nina. \n",
    "gc = pygsheets.authorize(service_account_file=filepath.service_account_location)\n",
    "lanina = gc.open_by_url('https://docs.google.com/spreadsheets/d/1obszkCQoE0ELOR1O0CrLVETUEmEIWlGuyAmK3FgWSJg/edit#gid=1099746391') \n",
    "lanina_sheet  = lanina.worksheet('title','La Nina (Current)').get_as_df()\n",
    "lanina_sheet.to_csv(localfolder+'smartsheet/La_Nina.csv')\n",
    "\n",
    "## EMIT \n",
    "emit = infrastructure.get_smartsheet('emit')\n",
    "email_pubid = emit['Revenue Pub ID'].astype(str).str.split('.',expand = True)[0].unique().tolist()\n",
    "\n",
    "jobs = pd.read_csv(localfolder + 'SMS_SC_SS_Jobs.csv', dtype={'job_id' :'Int64'})\n",
    "creative_stats = pd.read_csv(localfolder + 'SMS_SC_SS_CreativesStats.csv', dtype={'CreativeId' :'str'})\n",
    "raw_creative_stats = creative_stats\n",
    "offers = pd.read_csv(localfolder + 'SMS_SC_SS_Offers.csv', usecols = ['name','hitpath_offer_id', 'type','status','redirect_type','conversion_event','conversion_payout','currency'] )\n",
    "historic_data = pd.read_csv(localfolder + 'SS_data.csv')#  data before Nov1\n",
    "flows = pd.read_csv(localfolder + 'SMS_SC_SS_Flows.csv')\n",
    "\n",
    "# add some information in flows\n",
    "flows['Revenue Source'] = 'Short Code - SS Flow'\n",
    "flows['Code_Type'] = 'Short Code' \n",
    "flows['Shortcode Name'] = flows['Name'].str.split('-',expand = True)[1]\n",
    "flows['Shortcode1'] = flows['Shortcode'].str.extract('(\\d{5})')\n",
    "flows = flows.drop('Shortcode',axis = 1)\n",
    "flows['Dataset'] = flows['Name'].str.split('-',expand = True)[0]\n",
    "flows['Dataset'] = flows['Dataset'].str.replace('JM.ONP','JET.ONP')\n",
    "flows['Dataset'] = flows['Dataset'].str.replace('JM.NTC','JET.NTC')\n",
    "flows.loc[flows['CountUnsub']==0,'CountUnsub'] = flows['CountAutoresponderStop']\n",
    "#flows.loc[flows['CountDeliver']==0, 'CountDeliver'] = flows['CountSent']\n",
    "flows = flows.rename(columns=({'CountSent':'Delivered', 'CostDeliver':'Cost','CountClick':'Clicks','Shortcode1':'Shortcode', 'CountUnsub':'Optout'}))\n",
    "flows['Date'] = pd.to_datetime(flows['Period'].str[:10])\n",
    "flows['AR Flow ID']=flows['Id'].astype('str')\n",
    "flows['AR Day'] = flows['OfferName'].str.extract('(\\d+)\\s*\\(')\n",
    "flows.loc[flows['AR Day'].isna(),'AR Day'] =  flows['OfferName'].str.extract('AR(\\d)')[0]\n",
    "flows.loc[flows['AR Day'].isna(),'AR Day'] = 'Null'\n",
    "flows['Hitpath ID'] = flows['OfferName'].str.extract(r'(\\d{4,5})')\n",
    "flows['AR Flow'] = flows['AR Flow ID'] + '_Day_' + flows['Shortcode Name']+'_'+  flows['AR Day']\n",
    "flows_clean = flows[['Hitpath ID','Date','Dataset','Shortcode','Shortcode Name','Revenue Source','Code_Type','Delivered','Optout','Cost','Clicks','AR Flow','AR Day','AR Flow ID']]\n",
    "flows_clean = flows_clean.merge(publisher[['NEW DP.DS or DP.sV','PUBID']], left_on ='Dataset', right_on = 'NEW DP.DS or DP.sV', how = 'left' )\n",
    "flows_clean = flows_clean.rename(columns=({'PUBID':'Affiliate_id'}))\n",
    "flows_clean.loc[flows_clean['Shortcode Name'] == 'FLC', 'Shortcode'] = '51797'\n",
    "flows_clean.loc[flows_clean['Shortcode Name'] == 'CSS', 'Shortcode'] = '70610'\n",
    "flows_clean.loc[flows_clean['Shortcode Name'] == 'HZB', 'Shortcode'] = '44345'\n",
    "flows_clean.loc[flows_clean['Shortcode Name'] == 'MBC', 'Shortcode'] = '80837'\n",
    "flows_clean.loc[flows_clean['Shortcode Name'] == 'DSS', 'Shortcode'] = '31232'\n",
    "flows_clean.loc[flows_clean['Shortcode Name'] == 'SVT', 'Shortcode'] = '61659'\n",
    "flows_clean.loc[flows_clean['Shortcode Name'] == 'UAA', 'Shortcode'] = '79743'\n",
    "flows_clean.loc[flows_clean['Shortcode Name'] == 'PRC', 'Shortcode'] = '18333164159'\n",
    "flows_clean.loc[flows_clean['Shortcode'].str.contains( '51797',na = False), 'Shortcode Name'] = 'FLC'\n",
    "flows_clean.loc[flows_clean['Shortcode'].str.contains( '70610',na = False), 'Shortcode Name'] = 'CSS'\n",
    "flows_clean.loc[flows_clean['Shortcode'].str.contains( '44345',na = False), 'Shortcode Name'] = 'HZB'\n",
    "flows_clean.loc[flows_clean['Shortcode'].str.contains( '80837',na = False), 'Shortcode Name'] = 'MBC'\n",
    "flows_clean.loc[flows_clean['Shortcode'].str.contains( '31232',na = False), 'Shortcode Name'] = 'DSS'\n",
    "flows_clean.loc[flows_clean['Shortcode'].str.contains( '61659',na = False), 'Shortcode Name'] = 'SVT'\n",
    "flows_clean.loc[flows_clean['Shortcode'].str.contains( '79743',na = False), 'Shortcode Name'] = 'UAA'\n",
    "flows_clean.loc[flows_clean['Shortcode'].str.contains( '18333164159',na = False), 'Shortcode Name'] = 'PRC'\n",
    "# change some segment name \n",
    "creative_stats['Segment'] = creative_stats['Segment'].str.replace('WW.YFA','WWM.YFA')\n",
    "creative_stats['Segment'] = creative_stats['Segment'].str.replace('ARM.CR','AI.CC')\n",
    "creative_stats['Segment'] = creative_stats['Segment'].str.replace('RH.3CS','RHD.CC')\n",
    "creative_stats['Segment'] = creative_stats['Segment'].str.replace('ED.247L','EDM.247L')\n",
    "creative_stats['Segment'] = creative_stats['Segment'].str.replace('PA.SWP','PA.PS')\n",
    "creative_stats['Segment'] = creative_stats['Segment'].str.replace('FM.YS','FSM.YS')\n",
    "creative_stats['Segment'] = creative_stats['Segment'].str.replace('CM_','CM.OSR_')\n",
    "creative_stats['Segment'] = creative_stats['Segment'].str.replace('CM.OSR.OSR','CM.OSR')\n",
    "creative_stats['Segment'] = creative_stats['Segment'].str.replace('CM.OSR.OSR.OSR','CM.OSR')\n",
    "creative_stats = creative_stats.rename(columns=({'DeliverCount':'Delivered', 'TotalCost':'Cost','ClickCount':'Clicks','UnsubCount':'Optout'}))\n",
    "creative_id_na = creative_stats.loc[creative_stats['CreativeId'].isna()]\n",
    "jobs_optout = jobs[['id','optout']]\n",
    "creative_stats =  creative_stats.merge(jobs_optout, left_on = 'JobId' ,right_on='id', how='left')\n",
    "jobid_count = creative_stats['JobId'].value_counts().to_dict()\n",
    "creative_stats['Optout'] = creative_stats.apply(lambda row: row['optout'] / jobid_count[row['JobId']], axis=1)\n",
    "creative_stats = creative_stats.groupby(['JobId', 'Tstamp', 'Offer', 'Segment', 'CreativeId', 'CreativeName','Creative']).sum().reset_index()\n",
    "print(creative_stats['Delivered'].sum())\n",
    "jobs['segments'] = jobs['segments'].str.replace('WW.YFA','WWM.YFA')\n",
    "jobs['segments'] = jobs['segments'].str.replace('ARM.CR','AI.CC')\n",
    "jobs['segments'] = jobs['segments'].str.replace('RH.3CS','RHD.CC')\n",
    "jobs['segments'] = jobs['segments'].str.replace('ED.247L','EDM.247L')\n",
    "jobs['segments'] = jobs['segments'].str.replace('PA.SWP','PA.PS')\n",
    "jobs['segments'] = jobs['segments'].str.replace('FM.YS','FSM.YS')\n",
    "jobs['segments'] = jobs['segments'].str.replace('CM_','CM.OSR_')\n",
    "jobs['segments'] = jobs['segments'].str.replace('CM.OSR.OSR','CM.OSR')\n",
    "jobs['segments'] = jobs['segments'].str.replace('CM.OSR.OSR.OSR','CM.OSR')\n",
    "\n",
    "\n",
    "#clean jobs file\n",
    "#drop columns with nulls\n",
    "jobs.isna().sum()\n",
    "drop_columns_jobs = ['remote_status']\n",
    "jobs.drop(columns=drop_columns_jobs, inplace=True, axis=1)\n",
    "jobs = jobs.rename(columns={ 'id':'job_id', 'name':'job_name'})\n",
    "#convert date fields to correct format\n",
    "jobs['start_tstamp'] = pd.to_datetime(jobs['start_tstamp'])\n",
    "jobs['end_tstamp'] = pd.to_datetime(jobs['end_tstamp'])\n",
    "jobs['scheduled_tstamp'] = pd.to_datetime(jobs['scheduled_tstamp'],format = 'mixed')\n",
    "jobs['Scheduling Time'] = jobs['scheduled_tstamp'].dt.tz_convert('US/Pacific').dt.strftime('%Y-%m-%d %H:%M')\n",
    "creative_stats =  creative_stats.merge(jobs[['job_id','Scheduling Time']], left_on = 'JobId' ,right_on='job_id', how='left')\n",
    "creative_id_na =  creative_id_na.merge(jobs[['job_id','Scheduling Time']], left_on = 'JobId' ,right_on='job_id', how='left')\n",
    "\n",
    "#clean creative_stats\n",
    "creative_stats.isna().sum()\n",
    "creative_stats['Tstamp'] = pd.to_datetime(creative_stats['Tstamp'])\n",
    "\n",
    "#clean offers\n",
    "offers.isna().sum()\n",
    "offers = offers.rename(columns={ 'id':'offer_id', 'name':'offer'})\n",
    "\n",
    "#####engagement stats for all sends#####\n",
    "#merge above df with reveneu file to get all of reveneu information\n",
    "## combine all months revenue CSVs into one master-revenue file.\n",
    "os.chdir(localfolder + \"SMS Rev\")\n",
    "\n",
    "all_files = [i for i in glob.glob('*.{}'.format('csv'))]\n",
    "#combine all files in the list\n",
    "combined_csv = pd.concat([pd.read_csv(f) for f in all_files ])\n",
    "####Exporting master revenue file####\n",
    "combined_csv.to_csv( localfolder + \"SMS_master_revenue.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "\n",
    "combined_csv = pd.read_csv(localfolder + 'SMS_master_revenue.csv', dtype={'advertiser_name':'str','campaign_name':'str'})\n",
    "daily_revenue = combined_csv\n",
    "daily_revenue['date'] = pd.to_datetime(daily_revenue['date'],format = 'mixed').dt.date\n",
    "daily_revenue_gpby = daily_revenue.groupby(['affiliate_id','date'])['amount'].sum().reset_index()\n",
    "jump_page_csv = pd.read_csv( localfolder+\"jumppage_click.csv\")\n",
    "jumppage_clicks = jump_page_csv.rename(columns = {'subid_1':\"subid_2\"})\n",
    "print(combined_csv['amount'].sum())\n",
    "combined_csv = pd.concat([combined_csv,jumppage_clicks])\n",
    "print(combined_csv['amount'].sum())\n",
    "print(combined_csv['Jump Page Clicks'].sum())\n",
    "#check subids\n",
    "subs = [\"subid_1\",\"subid_2\", \"subid_5\"]\n",
    "def sid(d):\n",
    "    sid = \"subid_3\"\n",
    "    for j in subs:\n",
    "        if (d[f\"{j} uc\"]>=5) & (d[f\"{j} ucs\"]==0):\n",
    "            sid = j\n",
    "    return d[sid]\n",
    "for i in subs:\n",
    "    combined_csv[i] = combined_csv[i].astype(str)\n",
    "    combined_csv[f\"{i} uc\"] = combined_csv[i].str.count(\"_\")\n",
    "    combined_csv[f\"{i} ucs\"] = combined_csv[i].str.count(\":\")\n",
    "#merge all subids from 1,2 and 5 in subid\n",
    "combined_csv[\"sub_id\"] = combined_csv.apply(sid,axis=1)\n",
    "\n",
    "########## SS BEGIN #########\n",
    "combined_csv['first_split']=combined_csv['sub_id'].str.split('_').str[0]\n",
    "\"\"\" \n",
    "jumppage_clicks[\"sub_id\"] = jumppage_clicks[\"subid_1\"]\n",
    "subs = [\"sub_id\"]\n",
    "# doing same cleaning with jumpapge files  \n",
    "for i in subs:\n",
    "    jumppage_clicks[i] = jumppage_clicks[i].astype(str)\n",
    "    jumppage_clicks[f\"{i} uc\"] = jumppage_clicks[i].str.count(\"_\")\n",
    "#merge all subids from 1,2 and 5 in subid\n",
    "\n",
    "jumppage_clicks['first_split']=jumppage_clicks['sub_id'].str.split('_').str[0]\n",
    "\"\"\"\n",
    "# get the date based on subid date \n",
    "combined_csv = combined_csv.reset_index(drop=True)\n",
    "combined_csv['Dash_Date_from_subid'] = combined_csv['sub_id'].str.extract(r'(\\d{1,2}[A-Za-z]{3}\\d{2})')\n",
    "combined_csv['Dash_Date_from_subid'] = pd.to_datetime(combined_csv['Dash_Date_from_subid'], format='%d%b%y',errors='coerce')\n",
    "combined_csv['date'] = pd.to_datetime(combined_csv['date'],errors='coerce')\n",
    "combined_csv.loc[combined_csv['Dash_Date_from_subid'].isna(),'Dash_Date_from_subid'] = combined_csv['date']\n",
    "combined_csv['date'] = combined_csv['Dash_Date_from_subid']\n",
    "combined_csv.loc[combined_csv['campaign_id']==8202]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6bd0a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3382415.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flows = pd.read_csv(localfolder + 'SMS_SC_SS_Flows.csv')\n",
    "flows[(flows['Period']>='2023-10-01')&(flows['Period']<='2023-10-31')]['CountSent'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db92e61",
   "metadata": {},
   "source": [
    "## Long Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d5efa5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-10T16:00:58.363893Z",
     "start_time": "2023-08-10T16:00:55.862467Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "988.45\n",
      "988.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_11540\\3551923609.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  lc_lanina['Long Code Content ID'] = lc_lanina['Long Code Content ID'].astype(str).str.zfill(5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" \\nmerged_data = merged_data[['Date','Scheduling Time', 'Offer','Hitpath_Offer_ID','DP.SV','Affiliate_Id', 'DP&Pub','Job_Id', 'Job_Name','Creative_Id','Creativename','Creative','Send Strategy', 'Shortcode', 'Start_Tstamp', 'Segments', 'Revenue','Jump Page Clicks', 'Delivered', 'Not_Delivered', 'Optout', 'Clicks',\\n       'Cost', 'Ecpm', 'Time', 'Publisher', 'Campaign', 'Route',  'Carrier', 'Dataset', 'Message',\\n       'Responder Template', 'Keyword', 'Responder', 'Router Domain Name' , 'c1', 'Responded', 'Response Rate', 'CTR',\\n        'Gross Profit' , 'Gross Margin', 'RPU' ,'Provider', 'Code_Type','Revenue Source',\\n     'Ar Day','Sub_Id','Campaign_Id','Roi','Shortcode Name','Total']]\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Long Code data from MD\n",
    "\n",
    "lc = gc.open_by_url('https://docs.google.com/spreadsheets/d/1De9UzMw5POUuS90s7dMKsYNTQVnxTj1dvAvwMhOCC5U/edit#gid=0') \n",
    "lc_wk_df  = lc.worksheet('title','Long Code').get_as_df()\n",
    "lc_wk_df['SMS Cost'] = lc_wk_df['SMS Cost'].str.split('$',expand = True)[1].astype('float')\n",
    "# import MD_\n",
    "md_optout = pd.read_csv(localfolder + 'MD_optout.csv', index_col= False)\n",
    "md_optout[['campaign','Date']]=md_optout['campaign'].str.split('\\nStarted At: ',expand = True)\n",
    "md_optout['Date'] = pd.to_datetime(md_optout['Date'], format='%Y-%m-%d %H:%M:%S',errors='coerce')\n",
    "md_optout['campaign'] = md_optout['campaign'].str.extract('([a-zA-Z].*)', expand=True)\n",
    "lc_wk_df['Date'] = pd.to_datetime(lc_wk_df['Date'], errors='coerce')\n",
    "lc_wk_df = lc_wk_df.merge(md_optout[['Date','campaign','optout']], left_on = ['Offer','Date'], right_on = ['campaign','Date'], how = 'left')\n",
    "# rename to Optout\n",
    "lc_wk_df = lc_wk_df.rename(columns = {'Offer':'Sub_Id','optout':'Optout'})\n",
    "lc_rev = combined_csv.loc[(combined_csv['first_split']=='MD') | (combined_csv['first_split']=='TB') ]\n",
    "#lc_rev = combined_csv.loc[(combined_csv['first_split']=='MD') ]\n",
    "lc_rev.loc[lc_rev['Dash_Date_from_subid'].isna(), 'Dash_Date_from_subid'] = lc_rev['date']\n",
    "lc_rev =lc_rev.rename(columns = {\"Dash_Date_from_subid\":'Date','sub_id':'Sub_Id'})\n",
    "#  define the pattern\n",
    "#pattern = r'(.+\\d{1,2}[A-Za-z]{3}\\d{2})'\n",
    "# extract the pattern\n",
    "#lc_rev['Sub_Id'] = lc_rev['sub_id'].str.extract(pattern)\n",
    "lc_rev_summary = lc_rev.groupby(['Sub_Id','Date'])['amount'].sum().reset_index()\n",
    "lc_df = pd.concat([lc_wk_df,lc_rev_summary],axis = 0, ignore_index=True)\n",
    "# remove Date from lc_df\n",
    "lc_df['Date']= pd.to_datetime(lc_df['Date'],errors='coerce')\n",
    "lc_df = lc_df.groupby(['Sub_Id','Date']).sum().reset_index()  \n",
    "lc_df = lc_df.fillna(0)\n",
    "                           \n",
    "                           \n",
    "lc_df.loc[lc_df['Sub_Id'] == '01JUN23_8838_W4_EDU_SVT','Sub_Id'] = 'MD_LC_OMG_SVT_460918__8838_OT_01JUN23'\n",
    "lc_df.loc[lc_df['Sub_Id'] == '05Jun23_MD_LC_OMG_SVT_460918_W1_11714_T1','Sub_Id'] = 'MD_LC_OMG_SVT_460918__11714_OT_05Jun23'\n",
    "lc_df.loc[lc_df['Sub_Id'] == '11714-2_OMG_SVT','Sub_Id'] = 'MD_LC_OMG_SVT_460918__11714_OT_'\n",
    "lc_df.loc[lc_df['Sub_Id'] == 'MD_LC_OMG_SVT_460919_00066_9088__29Jul23','Sub_Id'] = 'MD_LC_OMG_SVT_460919_00066_9088_P_29Jul23'\n",
    "lc_df.loc[lc_df['Sub_Id'] == 'MD_LC_OMG_SVT_460920_00068_12076__30Jul23','Sub_Id']  = 'MD_LC_OMG_SVT_460920_00068_12076_P_30Jul23'\n",
    "lc_df['split_column'] = lc_df['Sub_Id'].str.split('_')\n",
    "lc_df['subid_uc'] = lc_df['Sub_Id'].str.count('_')\n",
    "#lc_df['Date'] = lc_df.loc[lc_df['Date'].isna(), 'Date'] = pd.to_datetime(lc_df['split_column'].str[8], format='%d%b%y',errors='coerce')\n",
    "lc_df['Send Strategy'] = lc_df['split_column'].str[7]\n",
    "lc_df['Hitpath_Offer_ID'] = lc_df['split_column'].str[6]\n",
    "# detect a 6digit number that start with \"46\" from Sub_Id\n",
    "lc_df['Affiliate_Id'] = lc_df['Sub_Id'].str.extract(r'(46\\d{4})')\n",
    "#lc_df['Affiliate_Id'] = lc_df['split_column'].str[4]\n",
    "lc_df['Long Code Content ID'] = lc_df['split_column'].str[5]\n",
    "lc_df['Shortcode Name'] = ''\n",
    "lc_df.loc[lc_df['split_column'].str[3] == 'SVT','Shortcode Name' ] = 'SVTLC'\n",
    "lc_df.loc[lc_df['split_column'].str[3] == 'UAA','Shortcode Name' ] = 'UAALC'\n",
    "lc_df['Send Strategy'] = lc_df['Send Strategy'].str.replace('T0','OT')\n",
    "lc_df['Send Strategy'] = lc_df['Send Strategy'].str.replace('T1','OT') \n",
    "lc_df.loc[lc_df['split_column'].str[0] == 'TB', 'Revenue Source'] = 'Long Code - Textback'\n",
    "lc_df.loc[lc_df['split_column'].str[0] == 'MD', 'Revenue Source'] = 'Long Code - Mobile Drips'\n",
    "lc_df.loc[lc_df['split_column'].str[0] == 'TB', 'Send Strategy'] = 'P'\n",
    "lc_df.loc[lc_df['split_column'].str[0] == 'TB', 'Long Code Content ID'] = np.nan\n",
    "\n",
    "# currently we don't consider split test content option \n",
    "lc_df['amount'] = lc_df['amount'].fillna(0)\n",
    "print(lc_df.loc[lc_df['split_column'].str[0] == 'MD',]['amount'].sum())\n",
    "lc_lanina = lanina_sheet[(lanina_sheet['Long Code Content ID'].isna() == False) & ((lanina_sheet['Long Code Content ID'] !=\"\"))]\n",
    "lc_lanina['Long Code Content ID'] = lc_lanina['Long Code Content ID'].astype(str).str.zfill(5)\n",
    "lc_df_full = lc_df.merge(lc_lanina[['Long Code Content ID','Reporting Content ID','Content']],copy = False, how= 'left', on = 'Long Code Content ID')\n",
    "print(lc_df_full.loc[lc_df_full['split_column'].str[0] == 'MD',]['amount'].sum())\n",
    "lc_df_full['Code_Type'] = 'Long Code'\n",
    "lc_df_full_11 = lc_df_full[lc_df_full['Date'] >= '2022-11-01']\n",
    "\n",
    "\n",
    "lc_df_full = lc_df_full.rename(columns = {'Qty':'Sent','Daily Success Qty':'Delivered','Fail Qty': 'Undelivered',\\\n",
    "                            'Clicks Qty':'Clicks','SMS Cost':'Cost',\\\n",
    "                            'amount':'Revenue','Long Code Content ID':'Creative_Id', 'Reporting Content ID':'Creativename','Content':'Creative'})\n",
    "\n",
    "lc_df_full = lc_df_full[['Date','Affiliate_Id', 'Hitpath_Offer_ID','Sent','Delivered','Undelivered','Clicks','Optout',\\\n",
    "       'Cost','Revenue', 'Send Strategy', 'Shortcode Name', 'Revenue Source', 'Code_Type','Sub_Id','Creative_Id','Creativename','Creative']]\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "merged_data = merged_data[['Date','Scheduling Time', 'Offer','Hitpath_Offer_ID','DP.SV','Affiliate_Id', 'DP&Pub','Job_Id', 'Job_Name','Creative_Id','Creativename','Creative','Send Strategy', 'Shortcode', 'Start_Tstamp', 'Segments', 'Revenue','Jump Page Clicks', 'Delivered', 'Not_Delivered', 'Optout', 'Clicks',\n",
    "       'Cost', 'Ecpm', 'Time', 'Publisher', 'Campaign', 'Route',  'Carrier', 'Dataset', 'Message',\n",
    "       'Responder Template', 'Keyword', 'Responder', 'Router Domain Name' , 'c1', 'Responded', 'Response Rate', 'CTR',\n",
    "        'Gross Profit' , 'Gross Margin', 'RPU' ,'Provider', 'Code_Type','Revenue Source',\n",
    "     'Ar Day','Sub_Id','Campaign_Id','Roi','Shortcode Name','Total']]\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6ab16a",
   "metadata": {},
   "source": [
    "## Short Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebd487cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-10T16:01:35.522346Z",
     "start_time": "2023-08-10T16:00:57.519352Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567032.7225549994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_11540\\2414094494.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  combined_csv_ss_only.loc[combined_csv_ss_only['offer_id'].str.contains(\"^(\\d{4,5}).*\", regex = True, na = False) == False , 'offer_id'] =  combined_csv_ss_only['split_column'].str[5]\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_11540\\2414094494.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  combined_csv_ss_only.loc[combined_csv_ss_only['offer_id'].str.contains(\"^(\\d{4,5}).*\", regex = True, na = False) == False, 'offer_id'] =  combined_csv_ss_only['split_column'].str[4]\n"
     ]
    }
   ],
   "source": [
    "combined_csv['date'] = pd.to_datetime(combined_csv['date'])\n",
    "combined_csv_ss_only = combined_csv[(combined_csv['first_split']=='SS')  |  (combined_csv['first_split']=='SMS')] \n",
    "combined_csv_ss_only = combined_csv_ss_only[combined_csv_ss_only['date']>='2022-11-01'].reset_index(drop=True)\n",
    "print(combined_csv_ss_only['amount'].sum())\n",
    "#combined_csv_ss_only= combined_csv_ss_only.drop_duplicates()\n",
    "#count number of underscores in subid\n",
    "combined_csv_ss_only['subid_uc'] = combined_csv_ss_only.sub_id.str.count('_')\n",
    "#combined_csv_ss_only['subid_uc']=combined_csv.sub_id.str.count('_')\n",
    "\n",
    "#ignoring the subids that  are not formatted correctly ex: SS_HZB_{{datasource_id}}_{{job_id}}_ALL.SMS_10434_SM_44345_{{today_d}}{{today_mon}}{{today_yy}}_1_{{member_id}}\n",
    "#they have very less revenue under them\n",
    "#these are with sub id uc <12\n",
    "combined_csv_ss_only = combined_csv_ss_only[combined_csv_ss_only['subid_uc'] < 12]\n",
    "\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['advertiser_name'] == 'NIC','campaign_id'] = 10267\n",
    "Offer_name = combined_csv_ss_only.groupby(['campaign_id','campaign_name']).count().reset_index()[['campaign_id','campaign_name']]\n",
    "\n",
    "# get jump page sum and revenue sum \n",
    "combined_csv_ss_only['split_column'] = combined_csv_ss_only['sub_id'].str.split('_')\n",
    "combined_csv_ss_only.loc[(combined_csv_ss_only['first_split'] == 'SS')  , 'offer_id'] = combined_csv_ss_only['split_column'].str[6]\n",
    "combined_csv_ss_only.loc[(combined_csv_ss_only['first_split'] == 'SMS')  , 'offer_id'] = combined_csv_ss_only['split_column'].str[7]\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['offer_id'].str.contains(\"^(\\d{4,5}).*\", regex = True, na = False) == False , 'offer_id'] =  combined_csv_ss_only['split_column'].str[5]\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['offer_id'].str.contains(\"^(\\d{4,5}).*\", regex = True, na = False) == False, 'offer_id'] =  combined_csv_ss_only['split_column'].str[4]\n",
    "\n",
    "combined_csv_ss_only['campaign_id']= combined_csv_ss_only['campaign_id'].astype('str').str.split('.',expand = True)[0]\n",
    "\n",
    "combined_csv_ss_only['Jump Page Version'] = '0'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['offer_id'].str.contains('v',na = False),'Jump Page Version'] = combined_csv_ss_only['offer_id'].str.split('v',expand = True)[1]\n",
    "combined_csv_ss_only['Offer Type'] = 'Single Offer'\n",
    "combined_csv_ss_only['offer_id'] = combined_csv_ss_only['offer_id'].astype('str').str.extract(r'\\b(\\d{4,5}).*')\n",
    "combined_csv_ss_only.loc[(combined_csv_ss_only['campaign_id'].isna()) | (combined_csv_ss_only['campaign_id'] == 'nan'),'campaign_id' ] = combined_csv_ss_only['offer_id']\n",
    "#combined_csv_ss_only['c1'] = combined_csv_ss_only['sub_id'].str[:-9]\n",
    "\n",
    "# import offer wall database\n",
    "ofwall = gc.open_by_url('https://docs.google.com/spreadsheets/d/1n8qaY4bxSKVDGc0bds1o33toLa7g9G9-T-nKDwlECVs/edit#gid=1913280388') \n",
    "ofwall_df  = ofwall.worksheet('title','Offer Wall Database').get_as_df()\n",
    "offer_wall_id = ofwall_df['Offer Wall ID'].astype(str).unique().tolist()\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['offer_id'].isin(offer_wall_id), 'Offer Type'] = 'Offer Wall'\n",
    "offerwall = combined_csv_ss_only.loc[combined_csv_ss_only['Offer Type']=='Offer Wall' ,]\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['Offer Type']=='Offer Wall' , 'campaign_id'] = combined_csv_ss_only['offer_id'].str.split('OW',expand = True)[0]\n",
    "combined_csv_ss_only.loc[(combined_csv_ss_only['campaign_id'] != combined_csv_ss_only['offer_id'])  ,'campaign_id'] = combined_csv_ss_only['offer_id']\n",
    "\n",
    "\n",
    "combined_csv_ss_only = combined_csv_ss_only[['date', 'campaign_id', 'Jump Page Version',\n",
    "       'affiliate_id',  'amount', 'Jump Page Clicks',\n",
    "        'sub_id', 'first_split','Dash_Date_from_subid', 'subid_uc','Offer Type']]\n",
    "\n",
    "combined_csv_ss_only['amount'] = combined_csv_ss_only['amount'].fillna(0)\n",
    "combined_csv_ss_only[ 'Jump Page Clicks'] = combined_csv_ss_only[ 'Jump Page Clicks'].fillna(0)\n",
    "combined_csv_ss_only = combined_csv_ss_only.groupby(['date', 'campaign_id', 'affiliate_id', 'Jump Page Version', 'sub_id', 'first_split','Dash_Date_from_subid', 'subid_uc','Offer Type'])[[ 'amount', 'Jump Page Clicks']].sum().reset_index()\n",
    "# use for verification \n",
    "jumppageclicks1 = combined_csv_ss_only['Jump Page Clicks'].sum()\n",
    "revenue1 = combined_csv_ss_only['amount'].sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d38681df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-10T16:30:44.688910Z",
     "start_time": "2023-08-10T16:01:36.810599Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_11540\\1819646670.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  combined_csv_ss_only_reformatjobs['Job_id'] = combined_csv_ss_only_reformatjobs['sub_id'].str.extract(f'({\"|\".join(job_ids)})')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6988745.0\n"
     ]
    }
   ],
   "source": [
    "#*** Extracting creative information. ****#\n",
    "# find creative id from list of creative ids from creativestats file.\n",
    "combined_csv_ss_only['split_column'] = combined_csv_ss_only['sub_id'].str.split('_')\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only[combined_csv_ss_only['first_split'] == 'SS'].index , 'creative_id'] = combined_csv_ss_only['split_column'].str[7]\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only[combined_csv_ss_only['first_split'] == 'SMS'].index , 'creative_id'] = combined_csv_ss_only['split_column'].str[8]\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only[combined_csv_ss_only['first_split'] == 'SS'].index , 'Job_id'] = combined_csv_ss_only['split_column'].str[5]\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only[combined_csv_ss_only['first_split'] == 'SMS'].index , 'Job_id'] = combined_csv_ss_only['split_column'].str[6]\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only[combined_csv_ss_only['first_split'] == 'SS'].index , 'Code_Type'] = combined_csv_ss_only['split_column'].str[1]\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only[combined_csv_ss_only['first_split'] == 'SMS'].index , 'Code_Type'] = combined_csv_ss_only['split_column'].str[2] \n",
    "\n",
    "# \n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['Code_Type'] == 'TF', 'Code_Type' ] ='Toll Free' \n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['Code_Type'] == 'SC', 'Code_Type' ] ='Short Code' \n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['Code_Type'] == 'FLC', 'Code_Type' ] ='Short Code' \n",
    "# some AR subids in december formatted incorrectly, some residuals formatted incorrectly.\n",
    "#creative ids are length>6, creative ids len =1 are ARs, ignoring both the cases and repalcing creative ids with nans for rest of the length values\n",
    "combined_csv_ss_only['creative_idlen'] = combined_csv_ss_only['creative_id'].str.len()\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only[~combined_csv_ss_only['creative_idlen'].isin([1,6])].index,'creative_id']=np.nan \n",
    "\n",
    "# identify shortcode \n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['sub_id'].str.contains('FLC', na=False), 'Shortcode Name'] = 'FLC'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['sub_id'].str.contains('CSS', na=False), 'Shortcode Name'] = 'CSS'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['sub_id'].str.contains('HZB', na=False), 'Shortcode Name'] = 'HZB'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['sub_id'].str.contains('MBC', na=False), 'Shortcode Name'] = 'MBC'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['sub_id'].str.contains('DSS', na=False), 'Shortcode Name'] = 'DSS'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['sub_id'].str.contains('SVT', na=False), 'Shortcode Name'] = 'SVT'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['sub_id'].str.contains('UAA', na=False), 'Shortcode Name'] = 'UAA'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['sub_id'].str.contains('PRC', na=False), 'Shortcode Name'] = 'PRC'\n",
    "\n",
    "combined_csv_ss_only.loc[(combined_csv_ss_only['Shortcode Name'] == 'SVT') & (combined_csv_ss_only['Code_Type'] == 'LC' ) , 'Shortcode Name'] = 'SVTLC'\n",
    "combined_csv_ss_only.loc[(combined_csv_ss_only['Shortcode Name'] == 'UAA') & (combined_csv_ss_only['Code_Type'] == 'LC' ) , 'Shortcode Name'] = 'UAALC'\n",
    "\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['Shortcode Name'] == 'FLC', 'Shortcode'] = '51797'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['Shortcode Name'] == 'CSS', 'Shortcode'] = '70610'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['Shortcode Name'] == 'HZB', 'Shortcode'] = '44345'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['Shortcode Name'] == 'MBC', 'Shortcode'] = '80837'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['Shortcode Name'] == 'DSS', 'Shortcode'] = '31232'\n",
    "combined_csv_ss_only.loc[(combined_csv_ss_only['Shortcode Name'] == 'SVT') , 'Shortcode'] = '61659'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['Shortcode Name'] == 'UAA', 'Shortcode'] = '79743'\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['Shortcode Name'] == 'PRC', 'Shortcode'] = '18333164159'\n",
    "\n",
    "\n",
    "#reformat jobs from residuals and AR\n",
    "combined_csv_ss_only['job_idlen'] = combined_csv_ss_only['Job_id'].str.len()\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only[~combined_csv_ss_only['job_idlen'].isin([1,6])].index,'Job_id']=np.nan \n",
    "combined_csv_ss_only_reformatjobs = combined_csv_ss_only[combined_csv_ss_only['Job_id'].isna() ]\n",
    "jobs['job_id'] =jobs['job_id'].astype('str')\n",
    "\"\"\"\n",
    "for i in jobs['job_id']:\n",
    "    check_len = combined_csv_ss_only_reformatjobs[combined_csv_ss_only_reformatjobs['sub_id'].str.contains(i)]\n",
    "    if len(check_len) > 0:\n",
    "        combined_csv_ss_only_reformatjobs.loc[check_len.index,'Job_id'] = i\n",
    "\"\"\"\n",
    "# Extract the job IDs to a set for faster membership testing\n",
    "creative_stats['JobId'] = creative_stats['JobId'].astype('str')\n",
    "job_ids = set(jobs['job_id']) | set(creative_stats['JobId']) - set(['0'])\n",
    "#creativeid = set(creative_stats['Creative_Id'])\n",
    "#combined_csv_ss_only['creative_id'] = combined_csv_ss_only_reformatjobs['sub_id'].str.extract(f'({\"|\".join(creativeid)})')\n",
    "\n",
    "# Use str.extract to extract the job ID from the sub_id column\n",
    "combined_csv_ss_only_reformatjobs['Job_id'] = combined_csv_ss_only_reformatjobs['sub_id'].str.extract(f'({\"|\".join(job_ids)})')\n",
    "\n",
    "combined_csv_ss_only =   pd.concat([combined_csv_ss_only_reformatjobs,(combined_csv_ss_only[~combined_csv_ss_only['Job_id'].isna()])] )\n",
    "\n",
    "combined_csv_ss_only = combined_csv_ss_only[combined_csv_ss_only['creative_idlen']!= 8]\n",
    "combined_csv_ss_only = combined_csv_ss_only[~combined_csv_ss_only['creative_idlen'].isna()]\n",
    "#get all creatives information.\n",
    "combined_csv_ss_only['Job_id'] = combined_csv_ss_only['Job_id'].astype('float')\n",
    "combined_csv_ss_only['creative_id'] = combined_csv_ss_only['creative_id'].replace('6Oct22', np.nan)\n",
    "combined_csv_ss_only['creative_id'] = combined_csv_ss_only['creative_id'].astype('float')\n",
    "\n",
    "creative_stats['CreativeId'] = creative_stats['CreativeId'].astype('float')\n",
    "# make sure we don't want to merge jobid = 0 \n",
    "#combined_csv_ss_only = combined_csv_ss_only.groupby(['Job_id', 'creative_id'])['amount'].sum().reset_index()\n",
    "#combined_csv_ss_creative = combined_csv_ss_only.merge(creative_stats, left_on = ['Job_id', 'creative_id'], right_on = ['JobId', 'CreativeId'], how = 'left')\n",
    "combined_csv_ss_only['date']  = pd.to_datetime(combined_csv_ss_only['date'] )\n",
    "combined_csv_ss_only['date'] = combined_csv_ss_only['date'].dt.date\n",
    "\n",
    "print(combined_csv_ss_only['Jump Page Clicks'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3e1c76",
   "metadata": {},
   "source": [
    "### SS flow, SS jobs and SS data without jobid(such as optin) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36485303",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-10T16:30:55.238446Z",
     "start_time": "2023-08-10T16:30:44.738392Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2695321.0\n",
      "2695321.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_11540\\4195179563.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  flows_clean_no_delivered['Affiliate_Id']= flows_clean_no_delivered['Affiliate_Id1']\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_11540\\4195179563.py:118: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  creative_id_na11['Campaign_Id'] = creative_id_na11['Offer'].str.split(' ',expand = True)[0]\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_11540\\4195179563.py:120: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  creative_id_na11['Campaign_Id']  = creative_id_na11['Campaign_Id'].astype(float)\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_11540\\4195179563.py:121: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  creative_id_na11['Shortcode Name'] = ''\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "combined_csv_ss_only['Dash_Date_from_subid'] = combined_csv_ss_only['sub_id'].str.extract(r'(\\d{1,2}[A-Za-z]{3}\\d{2})')\n",
    "# convert the dateinfo to datetime format using pd.to_datetime()\n",
    "\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['first_split'] == 'SS' , 'Dash_Date_from_subid'] = combined_csv_ss_only['split_column'].str[8]\n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['first_split'] == 'SMS' , 'Dash_Date_from_subid'] = combined_csv_ss_only['split_column'].str[9] \n",
    "combined_csv_ss_only.loc[combined_csv_ss_only['Dash_Date_from_subid'] == '1','Dash_Date_from_subid' ] = combined_csv_ss_only['split_column'].str[7] \n",
    "combined_csv_ss_only['Dash_Date_from_subid'] = pd.to_datetime(combined_csv_ss_only['Dash_Date_from_subid'], format=\"%d%b%y\", errors='coerce')\n",
    "\"\"\"\n",
    "combined_csv_ss_only['Hitpath ID'] = combined_csv_ss_only['campaign_id']\n",
    "#combined_csv_ss_only['Dash_Date_from_subid'] = pd.to_datetime(combined_csv_ss_only['Dash_Date_from_subid'], format=\"%d%b%y\")\n",
    "# flow data (51797 & 80837 & jobid = 0)\n",
    "#  by 2023/4/11, we don't include creative id. Subid has that info\n",
    "\n",
    "flows_clean1 = flows_clean.groupby(['Hitpath ID', 'Date', 'Dataset', 'Shortcode', 'Shortcode Name',\\\n",
    "       'Revenue Source', 'Code_Type', 'AR Flow', 'AR Day', 'AR Flow ID', 'NEW DP.DS or DP.sV','Affiliate_id']).sum().reset_index()\n",
    "flows_clean1.loc[flows_clean1['AR Day']=='Null', 'AR Day'] = 0 \n",
    "flows_clean1['AR Day'] = flows_clean1['AR Day'].astype(int)\n",
    "flows_clean1 = flows_clean1.groupby(['Hitpath ID', 'Date', 'Dataset', 'Shortcode', 'Shortcode Name',\n",
    "       'Revenue Source', 'Code_Type',  'NEW DP.DS or DP.sV',\n",
    "       'Affiliate_id']).sum().reset_index()\n",
    "flows_clean1['AR Day'] = flows_clean1['AR Day'].astype(str)\n",
    "\n",
    "combined_csv_ss_only.loc[(combined_csv_ss_only['split_column'].str[5].str.contains('AR',na = False)) ,'Hitpath ID'] =combined_csv_ss_only['split_column'].str[6].str.extract(r'\\b(\\d{4,5}).*')\n",
    "combined_csv_ss_ar_flow = combined_csv_ss_only[(combined_csv_ss_only['sub_id'].str.contains('AR')) |  (combined_csv_ss_only['Job_id']==0)]\n",
    "combined_csv_ss_ar_flow1 = combined_csv_ss_ar_flow.groupby(['affiliate_id','Hitpath ID','Shortcode','Dash_Date_from_subid','Offer Type','Jump Page Version'],dropna = False)[['amount','Jump Page Clicks']].sum().reset_index()\n",
    "\n",
    "combined_csv_ss_ar_flow1 = combined_csv_ss_ar_flow1.rename(columns=({'amount': 'Revenue','affiliate_id':'affiliate_id1'}))\n",
    "flows_clean1 = flows_clean1.groupby(['Hitpath ID', 'Date', 'Dataset', 'Shortcode', 'Shortcode Name',\\\n",
    "       'Revenue Source', 'Code_Type', 'AR Day', \\\n",
    "       'NEW DP.DS or DP.sV', 'Affiliate_id']).sum().reset_index()\n",
    "print(combined_csv_ss_ar_flow1['Jump Page Clicks'].sum())\n",
    "flows_clean2 = flows_clean1.merge(combined_csv_ss_ar_flow1, left_on = ['Date','Hitpath ID','Shortcode','Affiliate_id'],right_on = ['Dash_Date_from_subid','Hitpath ID','Shortcode','affiliate_id1'],how = 'outer')\n",
    "print(flows_clean2['Jump Page Clicks'].sum())\n",
    "flows_clean2['Send Strategy'] = 'AR'\n",
    "flows_clean2['Revenue Source'] = 'Short Code - SS Flow'\n",
    "flows_clean2['Code_Type'] = 'Short Code'\n",
    "flows_clean2.loc[flows_clean2['Date'].isna(), 'Date'] = flows_clean2['Dash_Date_from_subid']\n",
    "flows_clean2.loc[(flows_clean2['Shortcode'] == '51797') & (flows_clean2['Shortcode Name'].isna()), 'Shortcode Name'] = 'FLC'\n",
    "flows_clean2.loc[(flows_clean2['Shortcode'] == '80837') & (flows_clean2['Shortcode Name'].isna()), 'Shortcode Name'] = 'MBC'\n",
    "flows_clean2['Revenue'] =  flows_clean2['Revenue'].fillna(0)\n",
    "flows_clean2.columns = flows_clean2.columns.str.title()\n",
    "#flows_clean2.loc[flows_clean2['Affiliate_Id'].isna(),'Affiliate_Id' ]  = flows_clean2['Affiliate_Id1']\n",
    "flows_clean_no_delivered = flows_clean2.loc[flows_clean2['Affiliate_Id'].isna(), ] \n",
    "flows_clean_no_delivered['Affiliate_Id']= flows_clean_no_delivered['Affiliate_Id1']\n",
    "flows_clean_no_delivered = flows_clean_no_delivered.dropna(axis =1 , how ='all') \n",
    "flows_clean2 = flows_clean2.loc[flows_clean2['Affiliate_Id'].isna() == False, ] \n",
    "\n",
    "check_no_match = combined_csv_ss_ar_flow.merge(flows_clean1, how = 'left', right_on = ['Date','Hitpath ID','Shortcode','Affiliate_id'],left_on = ['Dash_Date_from_subid','Hitpath ID','Shortcode','affiliate_id'])\n",
    "check_no_match1 = check_no_match[check_no_match['Delivered'].isna()]\n",
    "check_no_match_with_flow = check_no_match1.dropna(axis =1 , how ='all') \n",
    "\n",
    "\n",
    "#jobs without creative or job \n",
    "combined_csv_ss_only1 = combined_csv_ss_only[~((combined_csv_ss_only['sub_id'].str.contains('AR')) |  (combined_csv_ss_only['Job_id']==0))]\n",
    "combined_csv_ss_only1 = pd.concat([combined_csv_ss_only1,check_no_match_with_flow]).reset_index(drop = True)\n",
    "combined_csv_ss_creative_na = combined_csv_ss_only1[(combined_csv_ss_only1['creative_id'].isna())| (combined_csv_ss_only1['creative_id']<100000)| ( (combined_csv_ss_only1['Job_id'].isna()))]\n",
    "\n",
    "#jobs with creative and job \n",
    "combined_csv_ss_creative_notna = combined_csv_ss_only1[~((combined_csv_ss_only1['creative_id'].isna()) | (combined_csv_ss_only1['creative_id']<100000)| ( (combined_csv_ss_only1['Job_id'].isna())))]\n",
    "\n",
    "# get revenue and delivery stats for jobs with creatives\n",
    "\"\"\" \n",
    "combined_csv_ss_creative_notna = combined_csv_ss_creative_notna.groupby(['Job_id', 'creative_id'])['amount'].sum().reset_index()\n",
    "merge_frame = creative_stats[['JobId', 'CreativeId']].append(combined_csv_ss_creative_notna[['Job_id', 'creative_id']]).drop_duplicates()\n",
    "creative_stats = creative_stats[creative_stats['Tstamp']>= '2022-11-01']\n",
    "print(combined_csv_ss_creative_notna['amount'].sum())\n",
    "combined_csv_ss_creative_notna = combined_csv_ss_creative_notna.merge(creative_stats, left_on = ['Job_id', 'creative_id'], right_on = ['JobId', 'CreativeId'], how = 'left')\n",
    "print(combined_csv_ss_creative_notna['amount'].sum())\n",
    "\"\"\" \n",
    "# get revenue and delivery stats for jobs with creatives\n",
    "combined_csv_ss_creative_notna = combined_csv_ss_creative_notna.groupby(['Job_id', 'creative_id','Jump Page Version','Offer Type'])[['amount','Jump Page Clicks']].sum().reset_index()\n",
    "creative_stats11 = creative_stats[creative_stats['Tstamp']>= '2022-11-01']\n",
    "creative_stats11 = creative_stats11.rename(columns=({'JobId': 'Job_id', 'CreativeId':'creative_id'}))\n",
    "creative_stats11['Job_id'] = creative_stats11['Job_id'].astype('int')\n",
    "\n",
    "merge_frame = pd.concat([creative_stats11[['Job_id', 'creative_id']],combined_csv_ss_creative_notna[['Job_id', 'creative_id']]]).drop_duplicates()\n",
    "merge_frame = merge_frame.merge(combined_csv_ss_creative_notna, how = 'left')\n",
    "merge_frame = merge_frame.merge(creative_stats11, how = 'left')\n",
    "\n",
    "combined_csv_ss_creative_notna = merge_frame.fillna(0)\n",
    "combined_csv_ss_creative_notna = combined_csv_ss_creative_notna[['Offer','Job_id', 'creative_id','Jump Page Version','Offer Type', 'CreativeName',\\\n",
    "       'Creative', 'Delivered', 'Cost', 'Optout', 'Clicks', 'amount','Jump Page Clicks']]\n",
    "combined_csv_ss_creative_notna = combined_csv_ss_creative_notna.rename(columns=({'amount': 'Revenue'}))\n",
    "#combined_csv_ss_creative_notna = combined_csv_ss_creative_notna.drop_duplicates()\n",
    "#combined_csv_ss_creative_notna = combined_csv_ss_creative_notna.groupby(['date','Offer','Job_id','CreativeId', 'CreativeName', 'Creative', 'Delivered', 'Cost', 'Unsubcount', 'Clicks']).sum('Revenue').reset_index()\n",
    "\n",
    "#get jobs information from jobs file\n",
    "jobs['job_id'] =jobs['job_id'].astype('int')\n",
    "combined_csv_ss_creative_notna = combined_csv_ss_creative_notna.merge(jobs[['job_id','job_name', 'shortcode', 'start_tstamp',\n",
    "       'end_tstamp', 'scheduled_tstamp', 'status_text', 'segments','Scheduling Time']], left_on = 'Job_id', right_on = 'job_id', how = 'left')\n",
    "combined_csv_ss_creative_notna['date'] = pd.to_datetime(combined_csv_ss_creative_notna['scheduled_tstamp']).dt.strftime(\"%Y-%m-%d\")\n",
    "combined_csv_ss_creative_notna = combined_csv_ss_creative_notna.drop(columns ='job_id')\n",
    "combined_csv_ss_creative_notna.columns = combined_csv_ss_creative_notna.columns.str.title()\n",
    "# create send strategy \n",
    "combined_csv_ss_creative_notna['Send Strategy'] = np.nan \n",
    "combined_csv_ss_creative_notna.loc[ combined_csv_ss_creative_notna['Job_Name'].str.split('_|[|]| ').str[-2:].astype('str').apply(lambda l: 'P' in l),'Send Strategy'] = 'P'\n",
    "combined_csv_ss_creative_notna.loc[ combined_csv_ss_creative_notna['Job_Name'].str.split('_|[|]| ').str[-2:].astype('str').apply(lambda l: 'T' in l),'Send Strategy'] = 'OT'\n",
    "combined_csv_ss_creative_notna.loc[ combined_csv_ss_creative_notna['Job_Name'].str.split('_|[|]| ').str[-2:].astype('str').apply(lambda l: 'OT' in l),'Send Strategy'] = 'OT'\n",
    "combined_csv_ss_creative_notna.loc[ combined_csv_ss_creative_notna['Job_Name'].str.split('_|[|]| ').str[-2:].astype('str').apply(lambda l: 'PT' in l),'Send Strategy'] = 'PT'\n",
    "combined_csv_ss_creative_notna.loc[ combined_csv_ss_creative_notna['Job_Name'].str.split('_|[|]| ').str[-2:].astype('str').apply(lambda l: 'AR' in l),'Send Strategy'] = 'AR'\n",
    "combined_csv_ss_creative_notna.loc[ combined_csv_ss_creative_notna['Job_Name'].str.split('_|[|]| ').str[-2:].astype('str').apply(lambda l: 'CT' in l),'Send Strategy'] = 'CT'\n",
    "combined_csv_ss_creative_notna.loc[ combined_csv_ss_creative_notna['Job_Name'].str.split('_|[|]| ').str[-2:].astype('str').apply(lambda l: 'MI' in l),'Send Strategy'] = 'MI'\n",
    "combined_csv_ss_creative_notna.loc[ combined_csv_ss_creative_notna['Job_Name'].str.split('_|[|]| ').str[-2:].astype('str').apply(lambda l: 'JT' in l),'Send Strategy'] = 'JT'\n",
    "# we didn't define the right send strategy for the following jobs in mamba\n",
    "combined_csv_ss_creative_notna.loc[combined_csv_ss_creative_notna['Job_Name'].str.contains('SS_FLC_PN-FC-21DC-VZN_12305_P_01Nov23', na = False),'Send Strategy'] ='JT'\n",
    "combined_csv_ss_creative_notna.loc[combined_csv_ss_creative_notna['Job_Name'].str.contains('SS_FLC_PN-FC-21DC-VZN_12305v1_CT_01Nov23', na = False),'Send Strategy'] ='JT'\n",
    "\n",
    "combined_csv_ss_creative_notna = combined_csv_ss_creative_notna.reset_index(drop=True)\n",
    "\n",
    "#get revenue stats for jobs without creatives.\n",
    "combined_csv_ss_creative_na = combined_csv_ss_creative_na[['date','Job_id', 'campaign_id','amount','sub_id','Jump Page Clicks','Jump Page Version','Offer Type']]\n",
    "combined_csv_ss_creative_na = combined_csv_ss_creative_na.rename(columns=({'amount': 'Revenue'}))\n",
    "#get delivery and click stats for jobs without creatives from job file\n",
    " \n",
    "creative_id_na['Date'] = pd.to_datetime(creative_id_na['Tstamp']).dt.strftime(\"%Y-%m-%d\")\n",
    "creative_id_na11=creative_id_na[creative_id_na['Date'] >= '2022-11-01']\n",
    "creative_id_na11['Campaign_Id'] = creative_id_na11['Offer'].str.split(' ',expand = True)[0]\n",
    "creative_id_na11.loc[creative_id_na11['Campaign_Id'].str.isdigit() == False,'Campaign_Id' ]  = np.nan\n",
    "creative_id_na11['Campaign_Id']  = creative_id_na11['Campaign_Id'].astype(float)\n",
    "creative_id_na11['Shortcode Name'] = ''\n",
    "creative_id_na11.loc[creative_id_na11['Offer'].str.contains('CSS', na = False),'Shortcode Name'] = 'CSS'\n",
    "creative_id_na11.loc[creative_id_na11['Offer'].str.contains('HZB', na = False),'Shortcode Name'] = 'HZB'\n",
    "# the jobid didn't use in the combined_csv_ss_creative_notna\n",
    "#unjoined_creative_stats11 = creative_stats11[~creative_stats11['Job_id'].isin(combined_csv_ss_creative_notna['Job_Id'])]\n",
    "creative_stats11['Date'] = pd.to_datetime(creative_stats11['Tstamp']).dt.strftime(\"%Y-%m-%d\")\n",
    "creative_stats11['Campaign_Id'] = creative_stats11['Offer'].str.split(' ',expand = True)[0]\n",
    "creative_stats11.loc[creative_stats11['Campaign_Id'].str.isdigit() == False,'Campaign_Id' ]  = np.nan\n",
    "creative_stats11['Campaign_Id']  = creative_stats11['Campaign_Id'].astype(float)\n",
    "creative_stats11['Shortcode Name'] = ''\n",
    "creative_stats11.loc[creative_stats11['Offer'].str.contains('CSS', na = False),'Shortcode Name'] = 'CSS'\n",
    "creative_stats11.loc[creative_stats11['Offer'].str.contains('HZB', na = False),'Shortcode Name'] = 'HZB'\n",
    "\n",
    "#\n",
    "combined_csv_ss_creative_na.loc[combined_csv_ss_creative_na['sub_id'].str.contains('70610' ,na=False ),'shortcode']= '70610'\n",
    "combined_csv_ss_creative_na.loc[combined_csv_ss_creative_na['sub_id'].str.contains('44345',na=False),'shortcode']= '44345'\n",
    "combined_csv_ss_creative_na.loc[combined_csv_ss_creative_na['sub_id'].str.contains('70610' ,na=False ),'Shortcode Name']= 'CSS'\n",
    "combined_csv_ss_creative_na.loc[combined_csv_ss_creative_na['sub_id'].str.contains('44345',na=False),'Shortcode Name']= 'HZB'\n",
    "combined_csv_ss_creative_na['campaign_id'] = combined_csv_ss_creative_na['sub_id'].str.split('_',expand = True)[6]\n",
    "combined_csv_ss_creative_na.loc[combined_csv_ss_creative_na['campaign_id'].str.isdigit()== False, 'campaign_id'] = combined_csv_ss_creative_na['sub_id'].str.split('_',expand = True)[5]\n",
    "combined_csv_ss_creative_na.loc[combined_csv_ss_creative_na['campaign_id'].str.isdigit()== False, 'campaign_id'] = combined_csv_ss_creative_na['sub_id'].str.split('_',expand = True)[4]\n",
    "combined_csv_ss_creative_na['affiliate_id'] = combined_csv_ss_creative_na['sub_id'].str.extract(r'(46\\d{4})')\n",
    "combined_csv_ss_creative_na.loc[combined_csv_ss_creative_na['campaign_id'].str.isdigit() == False,'campaign_id' ]  = np.nan\n",
    "combined_csv_ss_creative_na['campaign_id'] = combined_csv_ss_creative_na['campaign_id'].astype(float)\n",
    "#combined_csv_ss_creative_na['date'] = pd.to_datetime(combined_csv_ss_creative_na['scheduled_tstamp']).dt.strftime(\"%Y-%m-%d\")\n",
    "job_11 = jobs[jobs['scheduled_tstamp'] >= '2022-11-01']\n",
    "#creative_stats_limit = creative_stats11[creative_stats11['Job_id'].isin(combined_csv_ss_creative_na['Job_id'].unique().tolist())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "442ce515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3502821.0\n",
      "3502821.0\n",
      "3502821.0\n"
     ]
    }
   ],
   "source": [
    "print(flows_clean[(flows_clean['Date']>= '2023-10-01') &(flows_clean['Date']< '2023-11-01')]['Delivered'].sum())\n",
    "print(flows_clean1[(flows_clean1['Date']>= '2023-10-01') &(flows_clean1['Date']< '2023-11-01')]['Delivered'].sum())\n",
    "print(flows_clean2[(flows_clean2['Date']>= '2023-10-01') & (flows_clean2['Date']< '2023-11-01')]['Delivered'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f98e6fad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-10T16:31:58.443111Z",
     "start_time": "2023-08-10T16:30:55.238240Z"
    }
   },
   "outputs": [],
   "source": [
    "combined_csv_ss_creative_na = combined_csv_ss_creative_na.merge(creative_id_na11[['Date','Scheduling Time','JobId','Offer','Campaign_Id', 'Segment', 'CreativeId', 'CreativeName','Shortcode Name',\n",
    "       'Creative','Delivered',  'Optout', 'Clicks', 'Cost']], left_on = ['Job_id','campaign_id','date','Shortcode Name'], right_on = ['JobId','Campaign_Id','Date','Shortcode Name'], how = 'outer', copy = False)\n",
    "combined_csv_ss_creative_na = combined_csv_ss_creative_na.merge(job_11[['job_id','job_name', 'offer', 'shortcode', 'scheduled_tstamp', 'status_text', 'segments', \n",
    "        'delivered', 'optout', 'clicks', 'cost']], left_on = ['Job_id'], right_on =['job_id'] , how = 'left' , copy = False)\n",
    "combined_csv_ss_creative_na.loc[combined_csv_ss_creative_na['date'].isna(), 'date'] = combined_csv_ss_creative_na['Date']\n",
    "combined_csv_ss_creative_na.loc[combined_csv_ss_creative_na['campaign_id'].isna(), 'campaign_id'] = combined_csv_ss_creative_na['Campaign_Id']\n",
    "combined_csv_ss_creative_na.loc[combined_csv_ss_creative_na['Offer'].isna(), 'Offer'] = combined_csv_ss_creative_na['offer']\n",
    "combined_csv_ss_creative_na.loc[combined_csv_ss_creative_na['Delivered'].isna(), 'Delivered'] = combined_csv_ss_creative_na['delivered']\n",
    "combined_csv_ss_creative_na.loc[combined_csv_ss_creative_na['Optout'].isna(), 'Optout'] = combined_csv_ss_creative_na['optout']\n",
    "combined_csv_ss_creative_na.loc[combined_csv_ss_creative_na['Clicks'].isna(), 'Clicks'] = combined_csv_ss_creative_na['clicks']\n",
    "combined_csv_ss_creative_na.loc[combined_csv_ss_creative_na['Cost'].isna(), 'Cost'] = combined_csv_ss_creative_na['cost']\n",
    "combined_csv_ss_creative_na.loc[combined_csv_ss_creative_na['Job_id'].isna(), 'Job_id'] = combined_csv_ss_creative_na['JobId']\n",
    "combined_csv_ss_creative_na = combined_csv_ss_creative_na.drop(columns = ['job_id','Campaign_Id','Date','offer', 'delivered', 'optout', 'clicks', 'cost','JobId'])\n",
    "#combined_csv_ss_creative_na = combined_csv_ss_creative_na.reset_index()\n",
    "#combined_csv_ss_creative_na['date'] = pd.to_datetime(combined_csv_ss_creative_na['scheduled_tstamp']).dt.strftime(\"%Y-%m-%d\")\n",
    "combined_csv_ss_creative_na.columns = combined_csv_ss_creative_na.columns.str.title()\n",
    "\n",
    "# create send strategy \n",
    "combined_csv_ss_creative_na['Send Strategy'] = np.nan \n",
    "combined_csv_ss_creative_na.loc[ combined_csv_ss_creative_na['Job_Name'].str.split('_|[|]| ').str[-2:].astype('str').apply(lambda l: 'P' in l),'Send Strategy'] = 'P'\n",
    "combined_csv_ss_creative_na.loc[ combined_csv_ss_creative_na['Job_Name'].str.split('_|[|]| ').str[-2:].astype('str').apply(lambda l: 'T' in l),'Send Strategy'] = 'OT'\n",
    "combined_csv_ss_creative_na.loc[ combined_csv_ss_creative_na['Job_Name'].str.split('_|[|]| ').str[-2:].astype('str').apply(lambda l: 'OT' in l),'Send Strategy'] = 'OT'\n",
    "combined_csv_ss_creative_na.loc[ combined_csv_ss_creative_na['Job_Name'].str.split('_|[|]| ').str[-2:].astype('str').apply(lambda l: 'PT' in l),'Send Strategy'] = 'PT'\n",
    "combined_csv_ss_creative_na.loc[ combined_csv_ss_creative_na['Job_Name'].str.split('_|[|]| ').str[-2:].astype('str').apply(lambda l: 'AR' in l),'Send Strategy'] = 'AR'\n",
    "combined_csv_ss_creative_na.loc[ combined_csv_ss_creative_na['Job_Name'].str.split('_|[|]| ').str[-2:].astype('str').apply(lambda l: 'CT' in l),'Send Strategy'] = 'CT'\n",
    "combined_csv_ss_creative_na.loc[ combined_csv_ss_creative_na['Job_Name'].str.split('_|[|]| ').str[-2:].astype('str').apply(lambda l: 'MI' in l),'Send Strategy'] = 'MI'\n",
    "combined_csv_ss_creative_na.loc[ combined_csv_ss_creative_na['Job_Name'].str.split('_|[|]| ').str[-2:].astype('str').apply(lambda l: 'JT' in l),'Send Strategy'] = 'JT'\n",
    "combined_csv_ss_creative_na.loc[combined_csv_ss_creative_na['Job_Id'] == 0, 'Send Strategy'] = 'AR'\n",
    "combined_csv_ss_creative_na.loc[combined_csv_ss_creative_na['Job_Id'] == 0, 'Revenue Source'] =  'Short Code - SS Jobs'\n",
    "#combined_csv_ss_creative_na = combined_csv_ss_creative_na.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a414a442",
   "metadata": {},
   "source": [
    "### Other Revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79f3c129",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_11540\\2575836291.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  combined_csv_ss_exclude['date'] = pd.to_datetime(combined_csv_ss_exclude['date'])\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_11540\\2575836291.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  combined_csv_push['date'] = pd.to_datetime(combined_csv_push['date'])\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_11540\\2575836291.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  combined_csv_w1['Send Strategy'] = 'AR'\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_11540\\2575836291.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  combined_csv_w1['Revenue Source'] = 'Short Code - SS Jobs'\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_11540\\2575836291.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  combined_csv_w1['date'] = pd.to_datetime(combined_csv_w1['date'])\n",
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_11540\\2575836291.py:39: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  combined_csv_ss_last =  combined_csv_ss_rest[ ((combined_csv['affiliate_name'].str.lower().str.contains(\"push\", na = False) == False)) & (combined_csv_ss_rest['subid_2'].str.lower().str.contains(\"w1\", na = False) == False)]\n"
     ]
    }
   ],
   "source": [
    "#case #3: revenue for email \n",
    "combined_csv_ss_exclude = combined_csv\n",
    "combined_csv_ss_exclude['affiliate_id'] = combined_csv_ss_exclude['affiliate_id'].astype(str).str.split(\".\",expand = True)[0]\n",
    "combined_csv_ss_exclude['site_id'] = combined_csv_ss_exclude['site_id'].astype(str).str.split(\".\",expand = True)[0]\n",
    "\n",
    "combined_csv_ss_exclude =  combined_csv_ss_exclude.loc[(combined_csv_ss_exclude['affiliate_id'].isin(email_pubid)) | (combined_csv_ss_exclude['site_id'].isin(email_pubid)),] \n",
    "combined_csv_ss_exclude['date'] = pd.to_datetime(combined_csv_ss_exclude['date'])\n",
    "combined_csv_ss_exclude = combined_csv_ss_exclude[[ 'amount', 'date']]\n",
    "combined_csv_ss_exclude = combined_csv_ss_exclude.rename(columns=({'amount': 'Revenue','date': 'Date'}))\n",
    "combined_csv_ss_exclude_11 = combined_csv_ss_exclude[combined_csv_ss_exclude['Date']>='2022-11-01'].reset_index(drop=True)\n",
    "# case #4: push revenue \n",
    "email_pubid_int = emit['Revenue Pub ID'].unique().tolist()\n",
    "combined_csv_ss_rest =  combined_csv[(combined_csv['first_split']!='SS') & (combined_csv['first_split']!='SMS') & (combined_csv['affiliate_id'].astype(float).isin(email_pubid_int)== False) & (combined_csv['site_id'].astype(float).isin(email_pubid_int)== False) & (combined_csv['first_split']!='MD') &  (combined_csv['first_split']!='TB')  ] \n",
    "#combined_csv_ss_rest =  combined_csv[(combined_csv['first_split']!='SS') & (combined_csv['first_split']!='SMS') & (combined_csv['subid_2'].str.contains('FLC|MBC',na = False)== False)& (combined_csv['affiliate_id'].astype(float).isin(email_pubid_int)== False) & (combined_csv['site_id'].astype(float).isin(email_pubid_int)== False) & (combined_csv['first_split']!='MD')   ] \n",
    "\n",
    "combined_csv_push = combined_csv_ss_rest.loc[combined_csv_ss_rest['affiliate_name'].str.lower().str.contains(\"push\", na = False),]\n",
    "combined_csv_push['date'] = pd.to_datetime(combined_csv_push['date'])\n",
    "combined_csv_push = combined_csv_push[[ 'amount', 'date']]\n",
    "combined_csv_push = combined_csv_push.rename(columns=({'amount': 'Revenue','date': 'Date'}))\n",
    "combined_csv_push_11 =  combined_csv_push[combined_csv_push['Date']>='2022-11-01'].reset_index(drop=True)\n",
    "\n",
    "# case #6: previos AR job\n",
    "combined_csv_w1= combined_csv_ss_rest.loc[combined_csv_ss_rest['subid_2'].str.lower().str.contains(\"_w1\", na = False),]\n",
    "combined_csv_w1['Send Strategy'] = 'AR'\n",
    "combined_csv_w1['Revenue Source'] = 'Short Code - SS Jobs'\n",
    "combined_csv_w1['date'] = pd.to_datetime(combined_csv_w1['date'])\n",
    "combined_csv_w1 = combined_csv_w1.rename(columns=({'amount': 'Revenue','date': 'Date','affiliate_id':'Affiliate_Id', 'subid_2':'Sub_Id'}))\n",
    "combined_csv_w1['Hitpath Id'] = combined_csv_w1['campaign_id'].astype('str').str.split('.',expand = True)[0]\n",
    "# split subid_2 with \"_\"\n",
    "combined_csv_w1['Shortcode Name'] = np.nan\n",
    "combined_csv_w1.loc[combined_csv_w1['Date']>= '2022-11-01','Shortcode Name'] = combined_csv_w1['Sub_Id'].str.split(\"_\",expand = True)[2]\n",
    "## when shortcode name is 460654\n",
    "combined_csv_w1.loc[combined_csv_w1['Shortcode Name'] == '460654', 'Shortcode Name' ] = np.nan\n",
    "## when shortcode name is PLV, it's long code \n",
    "combined_csv_w1 = combined_csv_w1[['Revenue', 'Date', 'Send Strategy', 'Revenue Source', 'Hitpath Id', 'Affiliate_Id', 'Sub_Id','Shortcode Name']]\n",
    "combined_csv_w1_11 = combined_csv_w1[combined_csv_w1['Date']>='2022-11-01']\n",
    "# case #5: revenue the rest of the revenue. \n",
    "\n",
    "combined_csv_ss_last =  combined_csv_ss_rest[ ((combined_csv['affiliate_name'].str.lower().str.contains(\"push\", na = False) == False)) & (combined_csv_ss_rest['subid_2'].str.lower().str.contains(\"w1\", na = False) == False)] \n",
    "combined_csv_ss_last_11 = combined_csv_ss_last[combined_csv_ss_last['date']>='2022-11-01'].reset_index(drop=True)\n",
    "\n",
    "# identify Revenue Sourc\n",
    "combined_csv_ss_creative_na['Revenue Source'] = 'Short Code - SS Jobs'\n",
    "combined_csv_ss_creative_notna['Revenue Source'] = 'Short Code - SS Jobs'\n",
    "combined_csv_ss_exclude['Revenue Source'] = 'Email'\n",
    "combined_csv_push['Revenue Source'] = 'Push'\n",
    "historic_data['Revenue Source'] = 'Short Code - SS Jobs'\n",
    "historic_data['Send Strategy'] = np.nan\n",
    "\n",
    "#combined_csv_ss_creative_na = combined_csv_ss_creative_na.drop_duplicates()\n",
    "SS_New_data = pd.concat([combined_csv_ss_creative_notna,combined_csv_ss_creative_na,flows_clean2,combined_csv_ss_exclude, combined_csv_push,combined_csv_w1], axis=0)\n",
    "SS_New_data['Ecpm'] = SS_New_data['Revenue'] * 1000/SS_New_data['Delivered']\n",
    "SS_New_data['Roi'] = SS_New_data['Cost'] - SS_New_data['Revenue']\n",
    "#SS_New_data['Sent'] = SS_New_data['Total']\n",
    "\n",
    "#combining historic data(before Nov 1) with new data to get SS full data.\n",
    "#SS_New_data = SS_New_data[historic_data.columns]\n",
    "\n",
    "\n",
    "SS_Full_data = pd.concat([SS_New_data,historic_data], axis = 0)\n",
    "SS_Full_data['Date'] = pd.to_datetime(SS_Full_data['Date'])\n",
    "SS_Full_data = SS_Full_data.sort_values('Date', ascending=False)\n",
    "#SS_Full_data.loc[(SS_Full_data['Job_Id'].isna()) & (SS_Full_data['Revenue Source']==  'Short Code - SS Jobs')  ,'Revenue Source'] = 'Short Code - Opt In'\n",
    "SS_Full_data.loc[ (SS_Full_data['Job_Id'].isna()) & (SS_Full_data['Sub_Id'].str.contains('AR', na = False)) ,'Revenue Source'] =  'Short Code - SS Flow'\n",
    "SS_Full_data.loc[ (SS_Full_data['Job_Id'].isna())&  (SS_Full_data['Sub_Id'].str.contains('AR', na = False)) ,'Send Strategy'] = 'AR'\n",
    "SS_Full_data.loc[(SS_Full_data['Job_Id'].isna()) & (SS_Full_data['Revenue Source']==  'Short Code - Opt In') ,'Send Strategy'] = 'Opt In'\n",
    "SS_Full_data.loc[(SS_Full_data['Job_Id']==0) & (SS_Full_data['Revenue Source']==  'Short Code - SS Jobs') & (SS_Full_data['Send Strategy'].isna()) ,'Send Strategy'] = 'AR'\n",
    "SS_Full_data.to_csv(localfolder + 'SS_Fulldata.csv', index =False)  \n",
    "\n",
    "######## SS END ########\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a389806",
   "metadata": {},
   "source": [
    "## long code MP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54acff91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-10T16:31:59.037237Z",
     "start_time": "2023-08-10T16:31:58.487041Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_11540\\3977380537.py:7: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  MP_campaigns['ACTDATE']= pd.to_datetime(MP_campaigns['ACTDATE'])\n"
     ]
    }
   ],
   "source": [
    "######## MP BEGIN ########\n",
    "#Read MP file and clean. All stats present in MP_Campaigns file.\n",
    "\n",
    "MP_campaigns = pd.read_csv(localfolder + 'SMS_SC_MP_Campaigns.csv')\n",
    "MP_campaigns = MP_campaigns[~MP_campaigns['ACTDATE'].isna()]\n",
    "MP_campaigns = MP_campaigns[~MP_campaigns['ACTDATE'].isin(['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])]\n",
    "MP_campaigns['ACTDATE']= pd.to_datetime(MP_campaigns['ACTDATE'])\n",
    "#a = MP_campaigns.isna().sum()\n",
    "drop_columns = ['Daily Opt Out', 'Unnamed: 28', 'Unnamed: 53', 'Reference', 'c1NEW' , 'Unnamed: 61', 'Unnamed: 62', 'Unnamed: 64', 'Unnamed: 72', 'Unnamed: 73']\n",
    "MP_campaigns.drop(columns=drop_columns, inplace=True, axis=1)\n",
    "MP_campaigns= MP_campaigns[~MP_campaigns['Done.'].isna()]\n",
    "MP_campaigns.columns = MP_campaigns.columns.str.strip()\n",
    "\n",
    "# remove % and $ symbold\n",
    "MP_campaigns['REV'] = MP_campaigns['REV'].str.strip()\n",
    "MP_campaigns['REV'] = MP_campaigns['REV'].replace('$ -', np.nan)\n",
    "MP_campaigns['REV'] = MP_campaigns['REV'].str.replace('$','')\n",
    "MP_campaigns['REV'] = MP_campaigns['REV'].str.replace(',','')\n",
    "MP_campaigns['REV'] = MP_campaigns['REV'].astype('float')\n",
    "\n",
    "MP_campaigns['COST'] = MP_campaigns['COST'].str.strip()\n",
    "MP_campaigns['COST'] = MP_campaigns['COST'].replace('$ -', np.nan)\n",
    "MP_campaigns['COST'] = MP_campaigns['COST'].str.replace('$','')\n",
    "MP_campaigns['COST'] = MP_campaigns['COST'].astype('float')\n",
    "\n",
    "MP_campaigns['eCPM'] = MP_campaigns['eCPM'].str.strip()\n",
    "MP_campaigns['eCPM'] = MP_campaigns['eCPM'].replace('$ -', np.nan)\n",
    "MP_campaigns['eCPM'] = MP_campaigns['eCPM'].str.replace('$','')\n",
    "MP_campaigns['eCPM'] = MP_campaigns['eCPM'].astype('float')\n",
    "\n",
    "MP_campaigns['gPROFIT'] = MP_campaigns['gPROFIT'].str.strip()\n",
    "MP_campaigns['gPROFIT'] = MP_campaigns['gPROFIT'].str.replace('$','')\n",
    "MP_campaigns['gPROFIT'] = MP_campaigns['gPROFIT'].replace('-',np.nan)\n",
    "MP_campaigns['gPROFIT'] = MP_campaigns['gPROFIT'].replace(' -',np.nan)\n",
    "MP_campaigns['gPROFIT'] = MP_campaigns['gPROFIT'].astype(str).str.replace('\\((.*)\\)', '-\\\\1')\n",
    "MP_campaigns['gPROFIT'] = MP_campaigns['gPROFIT'].str.replace(',','')\n",
    "MP_campaigns['gPROFIT'] = MP_campaigns['gPROFIT'].astype('float')\n",
    "\n",
    "MP_campaigns['CLICK %'] = MP_campaigns['CLICK %'].str.strip()\n",
    "MP_campaigns['CLICK %'] = MP_campaigns['CLICK %'].str.replace('%','')\n",
    "MP_campaigns['CLICK %'] = MP_campaigns['CLICK %'].astype('float')\n",
    "\n",
    "MP_campaigns['gMARGIN'] = MP_campaigns['gMARGIN'].str.strip()\n",
    "MP_campaigns['gMARGIN'] = MP_campaigns['gMARGIN'].str.replace('%','')\n",
    "MP_campaigns['gMARGIN'] = MP_campaigns['gMARGIN'].astype('float')\n",
    "\n",
    "MP_campaigns.to_csv(localfolder + 'MP_data.csv', index =False)  \n",
    "\n",
    "###### MP ENDS #######\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb66254",
   "metadata": {},
   "source": [
    "## Combine all data from all sources "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a37a9e56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-10T16:34:25.685817Z",
     "start_time": "2023-08-10T16:31:59.036893Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_11540\\4029943520.py:3: DtypeWarning: Columns (0,4,5,6,13,15,16,17,18,19,20,27,33,34,36,37,40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  SS_data = pd.read_csv(localfolder + 'SS_Fulldata.csv')\n"
     ]
    }
   ],
   "source": [
    "#4 merge SS and MP: not possible as there are columns in MP not in SS.\n",
    "LC_data = pd.read_csv(localfolder + 'SMS_LC_Campaigns_clean.csv')\n",
    "SS_data = pd.read_csv(localfolder + 'SS_Fulldata.csv')\n",
    "\n",
    "lc_df_full = lc_df_full.reset_index(drop=True)\n",
    "LC_data = LC_data.reset_index(drop=True)\n",
    "LC_data = pd.concat([lc_df_full,LC_data], axis=0, ignore_index=True)\n",
    "LC_data.loc[LC_data['Affiliate_Id'].isna(),'Affiliate_Id'] = LC_data['pubID']\n",
    "LC_data.loc[LC_data['Hitpath_Offer_ID'].isna(),'Hitpath_Offer_ID' ] = LC_data['sid']\n",
    "LC_data['Affiliate_Id'] = LC_data['Affiliate_Id'].astype(str).str.split('.',expand = True)[0]\n",
    "LC_data['Hitpath_Offer_ID'] = LC_data['Hitpath_Offer_ID'].astype(str).str.split('.',expand = True)[0]\n",
    "SS_data['Code_Type'] = 'Short Code'\n",
    "LC_data['Code_Type'] = 'Long Code'\n",
    "SS_data.loc[SS_data['Revenue Source'] == 'Email', 'Code_Type'] = 'Email'\n",
    "SS_data.loc[SS_data['Revenue Source'] == 'Push', 'Code_Type'] = 'Push'\n",
    "SS_data.loc[SS_data['Shortcode'].astype(str).str.replace('.0','').str.len() ==11, 'Code_Type'] = 'Toll Free'\n",
    "SS_data.loc[SS_data['Shortcode'].astype(str).str.replace('.0','').str.len() ==11, 'Revenue Source'] = 'Toll Free'\n",
    "SS_data['Hitpath_Offer_ID'] = SS_data['Offer'].astype('str').str.extract(r'\\b(\\d{4,5}).*')\n",
    "SS_data['Affiliate_Id'] = SS_data['Affiliate_Id'].astype(str).str.split('.',expand = True)[0]\n",
    "#LC_data= LC_data.rename(columns=({'eCPM' : 'Ecpm', 'Clicked': 'Clicks'}))\n",
    "\n",
    "\n",
    "#merge short code and longcode\n",
    "merged_data = pd.concat([SS_data,LC_data], axis=0, ignore_index=True)\n",
    "\n",
    "#merged_data['Hitpath_Offer_ID'] = merged_data['Offer'].astype('str').str.extract(r'\\b(\\d{4,5})\\b')\n",
    "merged_data.loc[merged_data['Revenue Source']=='Short Code - SS Flow','Hitpath_Offer_ID'] =merged_data['Hitpath Id'].astype('str').str.split('.',expand = True)[0]\n",
    "merged_data['DP.SV'] = merged_data['New Dp.Ds Or Dp.Sv']\n",
    "merged_data.loc[merged_data['DP.SV'].isnull(), 'DP.SV'] =  merged_data['Segments'].str.split('_',expand = True)[1]\n",
    "dict_publisher = {'NEW DP.DS or DP.sV':['WWM.YFA','ZM.PL'], 'PUBID': ['461680','461681']}\n",
    "dict_publisher = pd.DataFrame(dict_publisher)\n",
    "new_publisher = pd.concat([publisher,dict_publisher])\n",
    "new_publisher = new_publisher.reset_index(drop=True)\n",
    "# publisher drop na rows\n",
    "new_publisher = new_publisher[~new_publisher['NEW DP.DS or DP.sV'].isna()]\n",
    "new_publisher = new_publisher[['NEW DP.DS or DP.sV','PUBID']]\n",
    "new_publisher['PUBID'] = new_publisher['PUBID'].astype(str).str.split('.',expand = True)[0]\n",
    "merged_data = merged_data.merge(new_publisher[['NEW DP.DS or DP.sV','PUBID']], left_on ='DP.SV', right_on = 'NEW DP.DS or DP.sV', how = 'left' )\n",
    "merged_data.loc[(merged_data['Affiliate_Id'].isna()) |  (merged_data['Affiliate_Id']=='nan'), 'Affiliate_Id'] = merged_data['PUBID']\n",
    "merged_data['Affiliate_Id'] = merged_data['Affiliate_Id'].astype(str).str.split('.',expand = True)[0]\n",
    "merged_data['DP&Pub'] = merged_data['DP.SV']+'_'+ merged_data['Affiliate_Id']\n",
    "\n",
    "\n",
    "merged_data = merged_data[['Date','Scheduling Time', 'Offer','Hitpath_Offer_ID','DP.SV','Affiliate_Id', 'DP&Pub','Job_Id', 'Job_Name','Creative_Id','Creativename','Creative','Send Strategy', 'Shortcode', 'Start_Tstamp','Sent', 'Segments', 'Revenue','Jump Page Clicks', 'Delivered', 'Not_Delivered', 'Optout', 'Clicks',\n",
    "       'Cost', 'Ecpm', 'Time', 'Publisher', 'Campaign', 'Route',  'Carrier', 'Dataset', 'Message',\n",
    "       'Responder Template', 'Keyword', 'Responder', 'Router Domain Name' , 'c1', 'Responded', 'Response Rate', 'CTR',\n",
    "        'Gross Profit' , 'Gross Margin', 'RPU' ,'Provider', 'Code_Type','Revenue Source',\n",
    "     'Ar Day','Sub_Id','Campaign_Id','Roi','Shortcode Name','Total','Jump Page Version','Offer Type']]\n",
    "merged_data.loc[merged_data['Clicks'] == '','Clicks'] = np.nan\n",
    "merged_data['Clicks'] = merged_data['Clicks'].astype(float)\n",
    "merged_data.loc[merged_data['Shortcode'] == 51797, 'Shortcode Name'] = 'FLC'\n",
    "merged_data.loc[merged_data['Shortcode'] == 70610, 'Shortcode Name'] = 'CSS'\n",
    "merged_data.loc[merged_data['Shortcode'] == 44345, 'Shortcode Name'] = 'HZB'\n",
    "merged_data.loc[merged_data['Shortcode'] == 80837, 'Shortcode Name'] = 'MBC'\n",
    "merged_data.loc[merged_data['Shortcode'] == 31232, 'Shortcode Name'] = 'DSS'\n",
    "merged_data.loc[merged_data['Shortcode'] == 61659, 'Shortcode Name'] = 'SVT'\n",
    "merged_data.loc[merged_data['Shortcode'] == 79743, 'Shortcode Name'] = 'UAA'\n",
    "merged_data.loc[merged_data['Shortcode'] == 18333164159, 'Shortcode Name'] = 'PRC'\n",
    "\n",
    "merged_data.loc[merged_data['Shortcode Name'] == 'FLC', 'Shortcode'] = '51797'\n",
    "merged_data.loc[merged_data['Shortcode Name'] == 'CSS', 'Shortcode'] = '70610'\n",
    "merged_data.loc[merged_data['Shortcode Name'] == 'HZB', 'Shortcode'] = '44345'\n",
    "merged_data.loc[merged_data['Shortcode Name'] == 'MBC', 'Shortcode'] = '80837'\n",
    "merged_data.loc[merged_data['Shortcode Name'] == 'DSS', 'Shortcode'] = '31232'\n",
    "merged_data.loc[merged_data['Shortcode Name'] == 'SVT', 'Shortcode'] = '61659'\n",
    "merged_data.loc[merged_data['Shortcode Name'] == 'UAA', 'Shortcode'] = '79743'\n",
    "merged_data.loc[merged_data['Shortcode Name'] == 'PRC', 'Shortcode'] = '18333164159'\n",
    "\n",
    "merged_data.loc[merged_data['Delivered'] == '','Delivered'] = np.nan\n",
    "merged_data['Delivered'] =  merged_data['Delivered'].astype(float)\n",
    "merged_data.loc[(merged_data['Shortcode Name'] == 'HZB'), 'Cost' ] = 0.007 * merged_data['Delivered']\n",
    "merged_data.loc[(merged_data['Shortcode Name'] == 'MBC') | (merged_data['Shortcode Name'] == 'FLC'),'Cost']  =  0.00563 *  merged_data['Delivered'] +  merged_data['Cost']\n",
    "merged_data.loc[(merged_data['Code_Type'] == 'Short Code')&((merged_data['Shortcode Name'] == 'UAA') | (merged_data['Shortcode Name'] == 'SVT') | (merged_data['Shortcode Name'] == 'DSS')), 'Cost']  =  0.00504 *  merged_data['Delivered'] \n",
    "Offer_name[ 'campaign_id'] = Offer_name[ 'campaign_id'].astype('str')\n",
    "merged_data = merged_data.merge(Offer_name,left_on = 'Hitpath_Offer_ID',right_on = 'campaign_id',how = 'left')\n",
    "merged_data['Job_Id']  = merged_data['Job_Id'].astype(str).str.split('.',expand = True)[0].str.strip()\n",
    "merged_data['Date'] = pd.to_datetime(merged_data['Date'],format ='mixed').dt.strftime(\"%Y-%m-%d\")\n",
    "merged_data['shortcode_DP.SV'] = merged_data['Shortcode Name'] + \"_\" + merged_data['DP.SV']\n",
    "merged_data.loc[merged_data['shortcode_DP.SV']=='_', 'shortcode_DP.SV'] = np.nan\n",
    "# calculate opportunity cost\n",
    "merged_data['Opportunity Cost Send Strategy'] =  True\n",
    "merged_data.loc[merged_data['Send Strategy'].isin(['Null','Opt In',np.nan]), 'Opportunity Cost Send Strategy'] = False\n",
    "merged_data = merged_data.sort_values('Date')\n",
    "temp1= merged_data.groupby(['shortcode_DP.SV','Opportunity Cost Send Strategy','Date']).agg({'Revenue':'sum','Delivered':'sum'}).reset_index()\n",
    "temp1[['rolling Revenue','rolling Delivered']] = temp1.groupby('shortcode_DP.SV').shift(1).rolling(30, min_periods=5)[['Revenue','Delivered']].sum().reset_index(drop=True)\n",
    "temp1['Dataset_Agg_30D_eCPM'] = temp1['rolling Revenue'] * 1000/ temp1['rolling Delivered']\n",
    "dataset_agg_eCPM =  temp1[['shortcode_DP.SV','Date','Opportunity Cost Send Strategy','Dataset_Agg_30D_eCPM']]\n",
    "merged_data = merged_data.merge(dataset_agg_eCPM, how = 'left')\n",
    "merged_data['Opportunity Cost'] = merged_data['Revenue'] - merged_data['Dataset_Agg_30D_eCPM'] * merged_data['Delivered'] /1000 \n",
    "merged_data = merged_data.sort_values(by = 'Date',ascending = False)\n",
    "merged_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "merged_data.to_csv(localfolder + 'SS_LC_merged_data.csv', index =False)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4057e786",
   "metadata": {},
   "source": [
    "# P&L DATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9416546e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-10T16:34:25.691033Z",
     "start_time": "2023-08-10T16:34:22.182771Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\lilig\\AppData\\Local\\Temp\\1\\ipykernel_11540\\3631652747.py:1: DtypeWarning: Columns (1,2,3,4,6,8,9,10,11,14,16,25,26,27,28,29,30,31,32,33,34,35,36,43,50,53,56) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  merged_data = pd.read_csv(localfolder + 'SS_LC_merged_data.csv')\n"
     ]
    }
   ],
   "source": [
    "merged_data = pd.read_csv(localfolder + 'SS_LC_merged_data.csv')\n",
    "engagement_daily = merged_data.groupby(['Date','Affiliate_Id'])[['Delivered','Clicks','Jump Page Clicks','Optout','Cost']].sum()\n",
    "engagement_daily = engagement_daily[~np.all(engagement_daily==0,axis = 1)]\n",
    "engagement_daily = engagement_daily.reset_index()\n",
    "daily_revenue_gpby = daily_revenue_gpby.rename(columns = {'amount':'Revenue','affiliate_id': 'Affiliate_Id', 'date': 'Date'})\n",
    "#Pl_data = daily_revenue_gpby.merge(engagement_daily, left_on =['affiliate_id','date'] ,right_on = ['Affiliate_Id','Date'],copy = False, how = 'outer')\n",
    "daily_revenue_gpby['Date'] = pd.to_datetime(daily_revenue_gpby['Date'])\n",
    "engagement_daily['Date'] =  pd.to_datetime(engagement_daily['Date'])\n",
    "Pl_data = pd.concat([daily_revenue_gpby,engagement_daily], axis=0, ignore_index=True)\n",
    "Pl_data['PUBID'] =Pl_data['Affiliate_Id'].astype('str').str.split(\".\",expand = True)[0]\n",
    "Pl_data = Pl_data.fillna(0)\n",
    "Pl_data = Pl_data[[ 'Date', 'PUBID','Revenue', 'Delivered', 'Clicks',\\\n",
    "      'Jump Page Clicks', 'Optout','Cost']]\n",
    "Pl_data = Pl_data.groupby(['PUBID','Date']).sum().reset_index()\n",
    "#Pl_data.to_csv(localfolder+'p&l_data.csv',index =False)\n",
    "\n",
    "api_key = pd.read_csv(localfolder+'SMS_SC_SS_Apikey.csv')\n",
    "#publisher1 = publisher\n",
    "#publisher1['NEW DP.DS or DP.sV'] = publisher1['NEW DP.DS or DP.sV'].str.replace('WWM.YFA.2','WWM.YFA')\n",
    "#publisher1['NEW DP.DS or DP.sV'] = publisher1['NEW DP.DS or DP.sV'].str.replace('ZM.PL.2','ZM.PL')\n",
    "api_key_sms = api_key.merge(new_publisher, left_on = 'DP.SV',right_on ='NEW DP.DS or DP.sV',how = 'inner' )\n",
    "api_key_sms = api_key_sms[['DP.SV','PUBID','Date', 'AcceptedTotal', 'AcceptedNew',\\\n",
    "       'AcceptedDuplicate', 'RejectedTotal', 'RejectedMobile',\\\n",
    "       'RejectedBlacklist', 'RejectedData', 'CostData', 'CostChecks',\\\n",
    "       'CostTotal']]\n",
    "api_key_sms['Date'] = pd.to_datetime(api_key_sms['Date'])\n",
    "Pl_data['Date'] =  pd.to_datetime(Pl_data['Date'] )\n",
    "Pl_data['PUBID']  = Pl_data['PUBID'].astype('str').str.split(\".\",expand = True)[0]\n",
    "api_key_sms['PUBID']  = api_key_sms['PUBID'].astype('str').str.split(\".\",expand = True)[0]\n",
    "rev_accept_df = pd.concat([Pl_data, api_key_sms], axis=0, ignore_index=True)\n",
    "rev_accept_df = rev_accept_df.fillna(0)\n",
    "rev_accept_df['PUBID']  = rev_accept_df['PUBID'].astype('str').str.split(\".\",expand = True)[0]\n",
    "rev_accept_df = rev_accept_df.drop(columns = 'DP.SV')\n",
    "rev_accept_df = rev_accept_df.groupby(['PUBID','Date']).sum().reset_index()\n",
    "\n",
    "\n",
    "sms_post = pd.read_csv(localfolder + 'SMSposts.csv')\n",
    "sms_post['Date'] = pd.to_datetime(sms_post['Dates'])\n",
    "sms_post['PUBID'] = sms_post['Pub Id'].astype('str').str.split(\".\",expand = True)[0]\n",
    "sms_post = sms_post.drop(columns = ['Dates','Pub Id','Pub Name'])\n",
    "post_data = pd.concat([rev_accept_df,sms_post], axis=0, ignore_index=True)\n",
    "post_data = post_data.fillna(0)\n",
    "post_data = post_data.groupby(['PUBID','Date']).sum().reset_index()\n",
    "publisher_raw['PUBID'] = publisher_raw['PUBID'].astype('str').str.split(\".\",expand = True)[0]\n",
    "post_data = post_data.merge(publisher_raw[['PUBID', 'NEW PUBLISHER','NEW DP.DS or DP.sV','DMA', 'INTERNAL STATUS',\n",
    "       'DATA TEAM STATUS', 'Sub Vertical', 'SCOPE', 'COMPANY (DP)','DP ID']], how = 'left', on = 'PUBID')\n",
    "post_data.to_csv(localfolder+'p&l_data.csv',index =False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a828a65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8851.29\n",
      "8851.29\n",
      "8851.289999999999\n",
      "8851.29\n"
     ]
    }
   ],
   "source": [
    "# offerwall reporting \n",
    "offer_sheet = infrastructure.get_smartsheet('offers_sms')\n",
    "offer_sheet = offer_sheet[offer_sheet['Hitpath ID'].isna() == False]\n",
    "offer_sheet['Hitpath ID'] = offer_sheet['Hitpath ID'].astype(str).str.split(\".\",expand = True)[0]\n",
    "offerwall = offerwall.rename(columns = {'offer_id':'Offer Wall ID'})\n",
    "print(offerwall[ 'amount'].sum())\n",
    "offerwall_update = offerwall.merge(offer_sheet[['Hitpath ID','Scheduling Name']], copy = False, how = 'left', left_on = 'Offer Wall ID', right_on = 'Hitpath ID')\n",
    "print(offerwall_update[ 'amount'].sum())\n",
    "offerwall_update = offerwall_update.rename(columns = {'Scheduling Name':\"Offer Wall Scheduling Name\" })\n",
    "offerwall_update = offerwall_update.merge(offer_sheet[['Hitpath ID','Scheduling Name']], copy = False, how = 'left', left_on = 'campaign_id', right_on = 'Hitpath ID')\n",
    "offerwall_update.loc[offerwall_update['Scheduling Name'].isna(),'Scheduling Name' ] = offerwall_update['campaign_name']\n",
    "\n",
    "offerwall_update = offerwall_update.rename(columns = {'Scheduling Name':\"Child Offer Campaign Name\" })\n",
    "offerwall_update[['Dash_Date_from_subid','affiliate_id','campaign_id',\"Child Offer Campaign Name\",'sub_id','Offer Wall ID',\"Offer Wall Scheduling Name\" ]] = offerwall_update[['Dash_Date_from_subid','affiliate_id','campaign_id',\"Child Offer Campaign Name\",'sub_id','Offer Wall ID',\"Offer Wall Scheduling Name\" ]].fillna(\"NULL\")\n",
    "offerwall_rev = offerwall_update.groupby(['Dash_Date_from_subid','affiliate_id','campaign_id',\"Child Offer Campaign Name\",'sub_id','Offer Wall ID',\"Offer Wall Scheduling Name\" ])[[ 'amount', 'Jump Page Clicks']].sum().reset_index()\n",
    "print(offerwall_rev[ 'amount'].sum())\n",
    "offerwall_rev['split_column'] = offerwall_rev['sub_id'].str.split('_')\n",
    "offerwall_rev['Send Strategy'] = 'P'\n",
    "offerwall_rev.loc[offerwall_rev['split_column'].str[5].str.contains(\"AR\",na = False),'Send Strategy'] = 'AR'\n",
    "offerwall_rev.loc[offerwall_rev['split_column'].str[5].str.contains(\"CT\",na = False),'Send Strategy'] = 'CT'\n",
    "offerwall_rev = offerwall_rev.rename(columns = {'amount':'Child Offer Revenue','Jump Page Clicks':'Child Offer Jump Page Clicks', 'campaign_id':'Child Offer ID'})\n",
    "merged_data_update = merged_data.add_suffix('- Offer Wall')\n",
    "offerwall_engage_merge = merged_data[merged_data['Offer Type'] == 'Offer Wall']\n",
    "offerwall_engage_subid = merged_data_update.groupby(['Sub_Id- Offer Wall'])[['Delivered- Offer Wall','Clicks- Offer Wall','Optout- Offer Wall','Cost- Offer Wall','Revenue- Offer Wall','Jump Page Clicks- Offer Wall']].sum().reset_index()\n",
    "offerwall_engage =  merged_data_update.groupby(['Date- Offer Wall','Affiliate_Id- Offer Wall','Hitpath_Offer_ID- Offer Wall','Send Strategy- Offer Wall'])[['Delivered- Offer Wall','Clicks- Offer Wall','Optout- Offer Wall','Cost- Offer Wall','Revenue- Offer Wall','Jump Page Clicks- Offer Wall']].sum().reset_index()\n",
    "offerwall_rev['affiliate_id'] = offerwall_rev['affiliate_id'].astype(str).str.split('.',expand = True)[0]\n",
    "offerwall_rev['Offer Wall ID'] = offerwall_rev['Offer Wall ID'].astype(str).str.split('.',expand = True)[0]\n",
    "offerwall_engage['Date- Offer Wall'] = pd.to_datetime(offerwall_engage['Date- Offer Wall'])\n",
    "offerwall_rev['Dash_Date_from_subid'] = pd.to_datetime(offerwall_rev['Dash_Date_from_subid'])\n",
    "offerwall_performance = offerwall_rev.merge(offerwall_engage_subid, left_on = ['sub_id'], right_on =['Sub_Id- Offer Wall'], how = 'left' )\n",
    "offerwall_performance_1 = offerwall_performance.loc[offerwall_performance['Sub_Id- Offer Wall'].isna() == False]\n",
    "offerwall_performance_2 = offerwall_performance.loc[offerwall_performance['Sub_Id- Offer Wall'].isna()== True].drop(columns = ['Sub_Id- Offer Wall','Delivered- Offer Wall','Clicks- Offer Wall','Optout- Offer Wall','Cost- Offer Wall','Revenue- Offer Wall','Jump Page Clicks- Offer Wall'])\n",
    "offerwall_performance_2 = offerwall_performance_2.groupby(['Dash_Date_from_subid','affiliate_id','Offer Wall ID','Send Strategy','Child Offer ID'])[[ 'Child Offer Revenue', 'Child Offer Jump Page Clicks']].sum().reset_index()\n",
    "offerwall_engage['Hitpath_Offer_ID- Offer Wall'] = offerwall_engage['Hitpath_Offer_ID- Offer Wall'].astype(str).str.split(\".\",expand = True)[0]\n",
    "offerwall_engage['Affiliate_Id- Offer Wall'] = offerwall_engage['Affiliate_Id- Offer Wall'].astype(str).str.split(\".\",expand = True)[0]\n",
    "offerwall_performance_2 = offerwall_performance_2.merge(offerwall_engage, left_on = ['Dash_Date_from_subid','affiliate_id','Offer Wall ID','Send Strategy'], right_on = ['Date- Offer Wall','Affiliate_Id- Offer Wall','Hitpath_Offer_ID- Offer Wall','Send Strategy- Offer Wall'], how ='left')\n",
    "offerwall_report = pd.concat([offerwall_performance_1,offerwall_performance_2])\n",
    "print(offerwall_report['Child Offer Revenue'].sum())\n",
    "offerwall_report.to_csv(localfolder + \"offerwall_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23dd9381",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "connect: to ('smtp.gmail.com', 587) None\n",
      "reply: b'220 smtp.gmail.com ESMTP i24-20020a056a00225800b006bdc8bb2ed5sm236482pfu.82 - gsmtp\\r\\n'\n",
      "reply: retcode (220); Msg: b'smtp.gmail.com ESMTP i24-20020a056a00225800b006bdc8bb2ed5sm236482pfu.82 - gsmtp'\n",
      "connect: b'smtp.gmail.com ESMTP i24-20020a056a00225800b006bdc8bb2ed5sm236482pfu.82 - gsmtp'\n",
      "send: 'ehlo WSAMZN-G2LI17MA.corp.rxmg.tech\\r\\n'\n",
      "reply: b'250-smtp.gmail.com at your service, [52.38.54.186]\\r\\n'\n",
      "reply: b'250-SIZE 35882577\\r\\n'\n",
      "reply: b'250-8BITMIME\\r\\n'\n",
      "reply: b'250-STARTTLS\\r\\n'\n",
      "reply: b'250-ENHANCEDSTATUSCODES\\r\\n'\n",
      "reply: b'250-PIPELINING\\r\\n'\n",
      "reply: b'250-CHUNKING\\r\\n'\n",
      "reply: b'250 SMTPUTF8\\r\\n'\n",
      "reply: retcode (250); Msg: b'smtp.gmail.com at your service, [52.38.54.186]\\nSIZE 35882577\\n8BITMIME\\nSTARTTLS\\nENHANCEDSTATUSCODES\\nPIPELINING\\nCHUNKING\\nSMTPUTF8'\n",
      "send: 'STARTTLS\\r\\n'\n",
      "reply: b'220 2.0.0 Ready to start TLS\\r\\n'\n",
      "reply: retcode (220); Msg: b'2.0.0 Ready to start TLS'\n",
      "send: 'ehlo WSAMZN-G2LI17MA.corp.rxmg.tech\\r\\n'\n",
      "reply: b'250-smtp.gmail.com at your service, [52.38.54.186]\\r\\n'\n",
      "reply: b'250-SIZE 35882577\\r\\n'\n",
      "reply: b'250-8BITMIME\\r\\n'\n",
      "reply: b'250-AUTH LOGIN PLAIN XOAUTH2 PLAIN-CLIENTTOKEN OAUTHBEARER XOAUTH\\r\\n'\n",
      "reply: b'250-ENHANCEDSTATUSCODES\\r\\n'\n",
      "reply: b'250-PIPELINING\\r\\n'\n",
      "reply: b'250-CHUNKING\\r\\n'\n",
      "reply: b'250 SMTPUTF8\\r\\n'\n",
      "reply: retcode (250); Msg: b'smtp.gmail.com at your service, [52.38.54.186]\\nSIZE 35882577\\n8BITMIME\\nAUTH LOGIN PLAIN XOAUTH2 PLAIN-CLIENTTOKEN OAUTHBEARER XOAUTH\\nENHANCEDSTATUSCODES\\nPIPELINING\\nCHUNKING\\nSMTPUTF8'\n",
      "send: 'AUTH PLAIN AHNjaGVkdWxlYWxhcnRzcnhtZ0BnbWFpbC5jb20AeXlqZXRlanRqaGpoenh4aw==\\r\\n'\n",
      "reply: b'235 2.7.0 Accepted\\r\\n'\n",
      "reply: retcode (235); Msg: b'2.7.0 Accepted'\n",
      "send: 'mail FROM:<schedulealartsrxmg@gmail.com> size=17664468\\r\\n'\n",
      "reply: b'250 2.1.0 OK i24-20020a056a00225800b006bdc8bb2ed5sm236482pfu.82 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.1.0 OK i24-20020a056a00225800b006bdc8bb2ed5sm236482pfu.82 - gsmtp'\n",
      "send: 'rcpt TO:<nathan@rxmg.com>\\r\\n'\n",
      "reply: b'250 2.1.5 OK i24-20020a056a00225800b006bdc8bb2ed5sm236482pfu.82 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.1.5 OK i24-20020a056a00225800b006bdc8bb2ed5sm236482pfu.82 - gsmtp'\n",
      "send: 'data\\r\\n'\n",
      "reply: b'354  Go ahead i24-20020a056a00225800b006bdc8bb2ed5sm236482pfu.82 - gsmtp\\r\\n'\n",
      "reply: retcode (354); Msg: b'Go ahead i24-20020a056a00225800b006bdc8bb2ed5sm236482pfu.82 - gsmtp'\n",
      "data: (354, b'Go ahead i24-20020a056a00225800b006bdc8bb2ed5sm236482pfu.82 - gsmtp')\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "reply: b'250 2.0.0 OK  1699658002 i24-20020a056a00225800b006bdc8bb2ed5sm236482pfu.82 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.0.0 OK  1699658002 i24-20020a056a00225800b006bdc8bb2ed5sm236482pfu.82 - gsmtp'\n",
      "data: (250, b'2.0.0 OK  1699658002 i24-20020a056a00225800b006bdc8bb2ed5sm236482pfu.82 - gsmtp')\n",
      "send: 'quit\\r\\n'\n",
      "reply: b'221 2.0.0 closing connection i24-20020a056a00225800b006bdc8bb2ed5sm236482pfu.82 - gsmtp\\r\\n'\n",
      "reply: retcode (221); Msg: b'2.0.0 closing connection i24-20020a056a00225800b006bdc8bb2ed5sm236482pfu.82 - gsmtp'\n",
      "connect: to ('smtp.gmail.com', 587) None\n",
      "reply: b'220 smtp.gmail.com ESMTP bk11-20020a17090b080b00b002807ec010e3sm2204154pjb.48 - gsmtp\\r\\n'\n",
      "reply: retcode (220); Msg: b'smtp.gmail.com ESMTP bk11-20020a17090b080b00b002807ec010e3sm2204154pjb.48 - gsmtp'\n",
      "connect: b'smtp.gmail.com ESMTP bk11-20020a17090b080b00b002807ec010e3sm2204154pjb.48 - gsmtp'\n",
      "send: 'ehlo WSAMZN-G2LI17MA.corp.rxmg.tech\\r\\n'\n",
      "reply: b'250-smtp.gmail.com at your service, [52.38.54.186]\\r\\n'\n",
      "reply: b'250-SIZE 35882577\\r\\n'\n",
      "reply: b'250-8BITMIME\\r\\n'\n",
      "reply: b'250-STARTTLS\\r\\n'\n",
      "reply: b'250-ENHANCEDSTATUSCODES\\r\\n'\n",
      "reply: b'250-PIPELINING\\r\\n'\n",
      "reply: b'250-CHUNKING\\r\\n'\n",
      "reply: b'250 SMTPUTF8\\r\\n'\n",
      "reply: retcode (250); Msg: b'smtp.gmail.com at your service, [52.38.54.186]\\nSIZE 35882577\\n8BITMIME\\nSTARTTLS\\nENHANCEDSTATUSCODES\\nPIPELINING\\nCHUNKING\\nSMTPUTF8'\n",
      "send: 'STARTTLS\\r\\n'\n",
      "reply: b'220 2.0.0 Ready to start TLS\\r\\n'\n",
      "reply: retcode (220); Msg: b'2.0.0 Ready to start TLS'\n",
      "send: 'ehlo WSAMZN-G2LI17MA.corp.rxmg.tech\\r\\n'\n",
      "reply: b'250-smtp.gmail.com at your service, [52.38.54.186]\\r\\n'\n",
      "reply: b'250-SIZE 35882577\\r\\n'\n",
      "reply: b'250-8BITMIME\\r\\n'\n",
      "reply: b'250-AUTH LOGIN PLAIN XOAUTH2 PLAIN-CLIENTTOKEN OAUTHBEARER XOAUTH\\r\\n'\n",
      "reply: b'250-ENHANCEDSTATUSCODES\\r\\n'\n",
      "reply: b'250-PIPELINING\\r\\n'\n",
      "reply: b'250-CHUNKING\\r\\n'\n",
      "reply: b'250 SMTPUTF8\\r\\n'\n",
      "reply: retcode (250); Msg: b'smtp.gmail.com at your service, [52.38.54.186]\\nSIZE 35882577\\n8BITMIME\\nAUTH LOGIN PLAIN XOAUTH2 PLAIN-CLIENTTOKEN OAUTHBEARER XOAUTH\\nENHANCEDSTATUSCODES\\nPIPELINING\\nCHUNKING\\nSMTPUTF8'\n",
      "send: 'AUTH PLAIN AHNjaGVkdWxlYWxhcnRzcnhtZ0BnbWFpbC5jb20AeXlqZXRlanRqaGpoenh4aw==\\r\\n'\n",
      "reply: b'235 2.7.0 Accepted\\r\\n'\n",
      "reply: retcode (235); Msg: b'2.7.0 Accepted'\n",
      "send: 'mail FROM:<schedulealartsrxmg@gmail.com> size=17664468\\r\\n'\n",
      "reply: b'250 2.1.0 OK bk11-20020a17090b080b00b002807ec010e3sm2204154pjb.48 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.1.0 OK bk11-20020a17090b080b00b002807ec010e3sm2204154pjb.48 - gsmtp'\n",
      "send: 'rcpt TO:<g.chao@rxmg.com>\\r\\n'\n",
      "reply: b'250 2.1.5 OK bk11-20020a17090b080b00b002807ec010e3sm2204154pjb.48 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.1.5 OK bk11-20020a17090b080b00b002807ec010e3sm2204154pjb.48 - gsmtp'\n",
      "send: 'data\\r\\n'\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "reply: b'250 2.0.0 OK  1699658014 bk11-20020a17090b080b00b002807ec010e3sm2204154pjb.48 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.0.0 OK  1699658014 bk11-20020a17090b080b00b002807ec010e3sm2204154pjb.48 - gsmtp'\n",
      "data: (250, b'2.0.0 OK  1699658014 bk11-20020a17090b080b00b002807ec010e3sm2204154pjb.48 - gsmtp')\n",
      "send: 'quit\\r\\n'\n",
      "reply: b'221 2.0.0 closing connection bk11-20020a17090b080b00b002807ec010e3sm2204154pjb.48 - gsmtp\\r\\n'\n",
      "reply: retcode (221); Msg: b'2.0.0 closing connection bk11-20020a17090b080b00b002807ec010e3sm2204154pjb.48 - gsmtp'\n",
      "connect: to ('smtp.gmail.com', 587) None\n",
      "reply: b'220 smtp.gmail.com ESMTP j15-20020a170903024f00b001c62b9a51a4sm165742plh.239 - gsmtp\\r\\n'\n",
      "reply: retcode (220); Msg: b'smtp.gmail.com ESMTP j15-20020a170903024f00b001c62b9a51a4sm165742plh.239 - gsmtp'\n",
      "connect: b'smtp.gmail.com ESMTP j15-20020a170903024f00b001c62b9a51a4sm165742plh.239 - gsmtp'\n",
      "send: 'ehlo WSAMZN-G2LI17MA.corp.rxmg.tech\\r\\n'\n",
      "reply: b'250-smtp.gmail.com at your service, [52.38.54.186]\\r\\n'\n",
      "reply: b'250-SIZE 35882577\\r\\n'\n",
      "reply: b'250-8BITMIME\\r\\n'\n",
      "reply: b'250-STARTTLS\\r\\n'\n",
      "reply: b'250-ENHANCEDSTATUSCODES\\r\\n'\n",
      "reply: b'250-PIPELINING\\r\\n'\n",
      "reply: b'250-CHUNKING\\r\\n'\n",
      "reply: b'250 SMTPUTF8\\r\\n'\n",
      "reply: retcode (250); Msg: b'smtp.gmail.com at your service, [52.38.54.186]\\nSIZE 35882577\\n8BITMIME\\nSTARTTLS\\nENHANCEDSTATUSCODES\\nPIPELINING\\nCHUNKING\\nSMTPUTF8'\n",
      "send: 'STARTTLS\\r\\n'\n",
      "reply: b'220 2.0.0 Ready to start TLS\\r\\n'\n",
      "reply: retcode (220); Msg: b'2.0.0 Ready to start TLS'\n",
      "send: 'ehlo WSAMZN-G2LI17MA.corp.rxmg.tech\\r\\n'\n",
      "reply: b'250-smtp.gmail.com at your service, [52.38.54.186]\\r\\n'\n",
      "reply: b'250-SIZE 35882577\\r\\n'\n",
      "reply: b'250-8BITMIME\\r\\n'\n",
      "reply: b'250-AUTH LOGIN PLAIN XOAUTH2 PLAIN-CLIENTTOKEN OAUTHBEARER XOAUTH\\r\\n'\n",
      "reply: b'250-ENHANCEDSTATUSCODES\\r\\n'\n",
      "reply: b'250-PIPELINING\\r\\n'\n",
      "reply: b'250-CHUNKING\\r\\n'\n",
      "reply: b'250 SMTPUTF8\\r\\n'\n",
      "reply: retcode (250); Msg: b'smtp.gmail.com at your service, [52.38.54.186]\\nSIZE 35882577\\n8BITMIME\\nAUTH LOGIN PLAIN XOAUTH2 PLAIN-CLIENTTOKEN OAUTHBEARER XOAUTH\\nENHANCEDSTATUSCODES\\nPIPELINING\\nCHUNKING\\nSMTPUTF8'\n",
      "send: 'AUTH PLAIN AHNjaGVkdWxlYWxhcnRzcnhtZ0BnbWFpbC5jb20AeXlqZXRlanRqaGpoenh4aw==\\r\\n'\n",
      "reply: b'235 2.7.0 Accepted\\r\\n'\n",
      "reply: retcode (235); Msg: b'2.7.0 Accepted'\n",
      "send: 'mail FROM:<schedulealartsrxmg@gmail.com> size=17664466\\r\\n'\n",
      "reply: b'250 2.1.0 OK j15-20020a170903024f00b001c62b9a51a4sm165742plh.239 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.1.0 OK j15-20020a170903024f00b001c62b9a51a4sm165742plh.239 - gsmtp'\n",
      "send: 'rcpt TO:<lili@rxmg.com>\\r\\n'\n",
      "reply: b'250 2.1.5 OK j15-20020a170903024f00b001c62b9a51a4sm165742plh.239 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.1.5 OK j15-20020a170903024f00b001c62b9a51a4sm165742plh.239 - gsmtp'\n",
      "send: 'data\\r\\n'\n",
      "reply: b'354  Go ahead j15-20020a170903024f00b001c62b9a51a4sm165742plh.239 - gsmtp\\r\\n'\n",
      "reply: retcode (354); Msg: b'Go ahead j15-20020a170903024f00b001c62b9a51a4sm165742plh.239 - gsmtp'\n",
      "data: (354, b'Go ahead j15-20020a170903024f00b001c62b9a51a4sm165742plh.239 - gsmtp')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "reply: b'250 2.0.0 OK  1699658022 j15-20020a170903024f00b001c62b9a51a4sm165742plh.239 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.0.0 OK  1699658022 j15-20020a170903024f00b001c62b9a51a4sm165742plh.239 - gsmtp'\n",
      "data: (250, b'2.0.0 OK  1699658022 j15-20020a170903024f00b001c62b9a51a4sm165742plh.239 - gsmtp')\n",
      "send: 'quit\\r\\n'\n",
      "reply: b'221 2.0.0 closing connection j15-20020a170903024f00b001c62b9a51a4sm165742plh.239 - gsmtp\\r\\n'\n",
      "reply: retcode (221); Msg: b'2.0.0 closing connection j15-20020a170903024f00b001c62b9a51a4sm165742plh.239 - gsmtp'\n"
     ]
    }
   ],
   "source": [
    " \n",
    "import zipfile\n",
    "os.chdir(localfolder) \n",
    "filename = 'daily_data_update.zip'\n",
    "email_body = ''\n",
    "with zipfile.ZipFile('daily_data_update.zip', 'w',zipfile.ZIP_DEFLATED) as zipf:\n",
    "    zipf.write('SS_LC_merged_data.csv')\n",
    "toaddr = ['nathan@rxmg.com','g.chao@rxmg.com','lili@rxmg.com']\n",
    "subject_line = 'SMS Daily Data Update' \n",
    "email_body \n",
    "for i in toaddr:\n",
    "    send_email.send_email([filename],subject_line,email_body,i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726f9b71",
   "metadata": {},
   "source": [
    "## Verification Data Quality "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83966e3c",
   "metadata": {},
   "source": [
    "+ verify last month's Delivered volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3211a644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Shortcode Name</th>\n",
       "      <th>Delivered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DSS</td>\n",
       "      <td>697203.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FLC</td>\n",
       "      <td>3286372.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HZB</td>\n",
       "      <td>1084112.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MBC</td>\n",
       "      <td>3709625.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PRC</td>\n",
       "      <td>360486.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVT</td>\n",
       "      <td>245087.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>UAA</td>\n",
       "      <td>188201.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Shortcode Name  Delivered\n",
       "0            DSS   697203.0\n",
       "1            FLC  3286372.0\n",
       "2            HZB  1084112.0\n",
       "3            MBC  3709625.0\n",
       "4            PRC   360486.0\n",
       "5            SVT   245087.0\n",
       "6            UAA   188201.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check delivered volumn by shortcode name \n",
    "merged_data['Date'] = pd.to_datetime(merged_data['Date'])\n",
    "merged_data['Month'] = merged_data['Date'].dt.strftime('%Y-%m')\n",
    "merged_data[ merged_data['Month'] == '2023-10'].groupby(['Shortcode Name'])[['Delivered']].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc6a1874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Revenue Source</th>\n",
       "      <th>Delivered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Email</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Push</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Short Code - SS Flow</td>\n",
       "      <td>3153683.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Short Code - SS Jobs</td>\n",
       "      <td>6058229.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Toll Free</td>\n",
       "      <td>360486.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Revenue Source  Delivered\n",
       "0                 Email        0.0\n",
       "1                  Push        0.0\n",
       "2  Short Code - SS Flow  3153683.0\n",
       "3  Short Code - SS Jobs  6058229.0\n",
       "4             Toll Free   360486.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data[ merged_data['Month'] == '2023-10'].groupby(['Revenue Source'])[['Delivered']].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c5ed3cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Delivered    9572398.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data[ merged_data['Month'] == '2023-10'][['Delivered']].sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d92816",
   "metadata": {},
   "source": [
    "+ verify Daily Revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7c17bd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-10T16:34:43.906980Z",
     "start_time": "2023-08-10T16:34:32.082989Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The revenue discrepency after November between raw file and Tableau data is $ 361.13\n",
      "The total revenue after November is $ 1753210.39\n",
      "The revenue discrepency is 0.0 % of the total revenue after November\n"
     ]
    }
   ],
   "source": [
    "# Check & Examination: \n",
    "# Revenue files sum up after November \n",
    "rev_file = combined_csv[combined_csv['date']>='2022-11-01'].reset_index(drop=True)['amount'].sum()\n",
    "# Revenue we made after November\n",
    "after_merge = combined_csv_ss_creative_na['Revenue'].sum() + combined_csv_ss_creative_notna['Revenue'].sum() + combined_csv_ss_exclude_11['Revenue'].sum() +flows_clean2['Revenue'].sum() + combined_csv_ss_last_11['amount'].sum() + lc_df_full_11['amount'].sum() + combined_csv_push_11['Revenue'].sum() + combined_csv_w1_11['Revenue'].sum()\n",
    "email_body = \"The revenue discrepency after November between raw file and Tableau data is $\"+str(round((rev_file - after_merge),2)) + \"\\n\" \n",
    "email_body += \"The total revenue after November is $\" + str(round(rev_file,2)) + \"\\n\" \n",
    "email_body += \"The revenue discrepency is \"+ str( round((rev_file - after_merge) /rev_file,2)*100 ) + \"% of the total revenue after November\" + \"\\n\" + \"The revenue we made after November is $\" + str(round(after_merge,2))\n",
    "print(\"The revenue discrepency after November between raw file and Tableau data is $\",round((rev_file - after_merge),2))\n",
    "print(\"The total revenue after November is $\", round(rev_file,2))\n",
    "print(\"The revenue discrepency is\", round((rev_file - after_merge) /rev_file,2)*100, \"% of the total revenue after November\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d45a8b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Month\n",
       "1899-12         0.000000\n",
       "1999-12       652.800000\n",
       "2020-02       163.100000\n",
       "2020-03      1116.300000\n",
       "2020-04      5060.200000\n",
       "2020-05      2218.760000\n",
       "2020-06      3551.800000\n",
       "2020-07      6578.100000\n",
       "2020-08     41870.600000\n",
       "2020-09     52336.370000\n",
       "2020-10     50779.900000\n",
       "2020-11     37785.730000\n",
       "2020-12     32358.100000\n",
       "2021-01     23948.040000\n",
       "2021-02     20137.840000\n",
       "2021-03     26584.030000\n",
       "2021-04     50262.400000\n",
       "2021-05     74756.520000\n",
       "2021-06     91146.100000\n",
       "2021-07    144141.810000\n",
       "2021-08    158429.050000\n",
       "2021-09    154983.178800\n",
       "2021-10    201033.121100\n",
       "2021-11    225191.670000\n",
       "2021-12    266561.890000\n",
       "2022-01    276933.564528\n",
       "2022-02    268013.423635\n",
       "2022-03    249499.745543\n",
       "2022-04    296154.537417\n",
       "2022-05    277356.288745\n",
       "2022-06    404793.833177\n",
       "2022-07    247401.727505\n",
       "2022-08    155206.965008\n",
       "2022-09    161623.544255\n",
       "2022-10     39894.072484\n",
       "2022-11     87336.983168\n",
       "2022-12    112797.259432\n",
       "2023-01    126360.496178\n",
       "2023-02    106176.609126\n",
       "2023-03    145345.576130\n",
       "2023-04    141748.617699\n",
       "2023-05    140341.309997\n",
       "2023-06    133078.956674\n",
       "2023-07    159062.982617\n",
       "2023-08    176501.414455\n",
       "2023-09    172955.250641\n",
       "2023-10    190057.228332\n",
       "2023-11     53951.161001\n",
       "NaT          2254.170000\n",
       "Name: Revenue, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#monthly revenue\n",
    "merged_data['Month'] = merged_data['Date'].astype(str).str[:7]\n",
    "temp = merged_data.groupby('Month')['Revenue'].sum()\n",
    "email_body +=  \"\\n\\nThe revenue by month is\\n\" \n",
    "email_body += str(temp)  + '\\n'\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9377a12c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "month\n",
       "2022-11    159.13\n",
       "2022-12    684.18\n",
       "2023-01    859.75\n",
       "2023-02    881.73\n",
       "2023-03    292.22\n",
       "2023-04    869.75\n",
       "2023-05    860.70\n",
       "2023-06     11.45\n",
       "2023-07      2.55\n",
       "2023-08    142.34\n",
       "2023-09    102.80\n",
       "2023-10     14.24\n",
       "2023-11      0.40\n",
       "Name: amount, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#unattributed revenue\n",
    "# not attribute to our data yet\n",
    "## Including KCP, NOC, and ROC \n",
    "## include missing subid drop \n",
    "## advertiser: NIC missing revenue in MAy $810. \n",
    "\n",
    "\n",
    "combined_csv_ss_last_11['month'] = combined_csv_ss_last_11['date'].astype(str).str[:7]\n",
    "\n",
    "email_body += \"\\n\" + \"The revenue by month for unclaimed revenue is \" + str(combined_csv_ss_last_11.groupby('month')['amount'].sum()) +\"\\n\"\n",
    "combined_csv_ss_last_11.groupby('month')['amount'].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f63eaf47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date\n",
       "1899-12-31       0.000000\n",
       "1999-12-31     652.800000\n",
       "2020-02-11       4.500000\n",
       "2020-02-27      45.050000\n",
       "2020-02-28      38.500000\n",
       "                 ...     \n",
       "2023-11-06    6805.593469\n",
       "2023-11-07    7515.576476\n",
       "2023-11-08    4851.210000\n",
       "2023-11-09    3928.687000\n",
       "2023-11-10     275.570000\n",
       "Name: Revenue, Length: 1354, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#daily revenue\n",
    "temp = merged_data.groupby('Date')['Revenue'].sum()\n",
    "email_body +=  \"\\n\\nThe revenue by Date is\\n\" \n",
    "email_body += str(temp)  + '\\n'\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c966a9",
   "metadata": {},
   "source": [
    "# Send out email to the Team "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56a514ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "connect: to ('smtp.gmail.com', 587) None\n",
      "reply: b'220 smtp.gmail.com ESMTP e12-20020a170902d38c00b001bde65894c8sm162769pld.268 - gsmtp\\r\\n'\n",
      "reply: retcode (220); Msg: b'smtp.gmail.com ESMTP e12-20020a170902d38c00b001bde65894c8sm162769pld.268 - gsmtp'\n",
      "connect: b'smtp.gmail.com ESMTP e12-20020a170902d38c00b001bde65894c8sm162769pld.268 - gsmtp'\n",
      "send: 'ehlo WSAMZN-G2LI17MA.corp.rxmg.tech\\r\\n'\n",
      "reply: b'250-smtp.gmail.com at your service, [52.38.54.186]\\r\\n'\n",
      "reply: b'250-SIZE 35882577\\r\\n'\n",
      "reply: b'250-8BITMIME\\r\\n'\n",
      "reply: b'250-STARTTLS\\r\\n'\n",
      "reply: b'250-ENHANCEDSTATUSCODES\\r\\n'\n",
      "reply: b'250-PIPELINING\\r\\n'\n",
      "reply: b'250-CHUNKING\\r\\n'\n",
      "reply: b'250 SMTPUTF8\\r\\n'\n",
      "reply: retcode (250); Msg: b'smtp.gmail.com at your service, [52.38.54.186]\\nSIZE 35882577\\n8BITMIME\\nSTARTTLS\\nENHANCEDSTATUSCODES\\nPIPELINING\\nCHUNKING\\nSMTPUTF8'\n",
      "send: 'STARTTLS\\r\\n'\n",
      "reply: b'220 2.0.0 Ready to start TLS\\r\\n'\n",
      "reply: retcode (220); Msg: b'2.0.0 Ready to start TLS'\n",
      "send: 'ehlo WSAMZN-G2LI17MA.corp.rxmg.tech\\r\\n'\n",
      "reply: b'250-smtp.gmail.com at your service, [52.38.54.186]\\r\\n'\n",
      "reply: b'250-SIZE 35882577\\r\\n'\n",
      "reply: b'250-8BITMIME\\r\\n'\n",
      "reply: b'250-AUTH LOGIN PLAIN XOAUTH2 PLAIN-CLIENTTOKEN OAUTHBEARER XOAUTH\\r\\n'\n",
      "reply: b'250-ENHANCEDSTATUSCODES\\r\\n'\n",
      "reply: b'250-PIPELINING\\r\\n'\n",
      "reply: b'250-CHUNKING\\r\\n'\n",
      "reply: b'250 SMTPUTF8\\r\\n'\n",
      "reply: retcode (250); Msg: b'smtp.gmail.com at your service, [52.38.54.186]\\nSIZE 35882577\\n8BITMIME\\nAUTH LOGIN PLAIN XOAUTH2 PLAIN-CLIENTTOKEN OAUTHBEARER XOAUTH\\nENHANCEDSTATUSCODES\\nPIPELINING\\nCHUNKING\\nSMTPUTF8'\n",
      "send: 'AUTH PLAIN AHNjaGVkdWxlYWxhcnRzcnhtZ0BnbWFpbC5jb20AeXlqZXRlanRqaGpoenh4aw==\\r\\n'\n",
      "reply: b'235 2.7.0 Accepted\\r\\n'\n",
      "reply: retcode (235); Msg: b'2.7.0 Accepted'\n",
      "send: 'mail FROM:<schedulealartsrxmg@gmail.com> size=2675\\r\\n'\n",
      "reply: b'250 2.1.0 OK e12-20020a170902d38c00b001bde65894c8sm162769pld.268 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.1.0 OK e12-20020a170902d38c00b001bde65894c8sm162769pld.268 - gsmtp'\n",
      "send: 'rcpt TO:<lili@rxmg.com>\\r\\n'\n",
      "reply: b'250 2.1.5 OK e12-20020a170902d38c00b001bde65894c8sm162769pld.268 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.1.5 OK e12-20020a170902d38c00b001bde65894c8sm162769pld.268 - gsmtp'\n",
      "send: 'data\\r\\n'\n",
      "reply: b'354  Go ahead e12-20020a170902d38c00b001bde65894c8sm162769pld.268 - gsmtp\\r\\n'\n",
      "reply: retcode (354); Msg: b'Go ahead e12-20020a170902d38c00b001bde65894c8sm162769pld.268 - gsmtp'\n",
      "data: (354, b'Go ahead e12-20020a170902d38c00b001bde65894c8sm162769pld.268 - gsmtp')\n",
      "send: b'Content-Type: multipart/mixed; boundary=\"===============0531060721504982123==\"\\r\\nMIME-Version: 1.0\\r\\nFrom: schedulealartsrxmg@gmail.com\\r\\nTo: lili@rxmg.com\\r\\nSubject: SMS Daily Update Report\\r\\n\\r\\n--===============0531060721504982123==\\r\\nContent-Type: text/plain; charset=\"us-ascii\"\\r\\nMIME-Version: 1.0\\r\\nContent-Transfer-Encoding: 7bit\\r\\n\\r\\nThe revenue discrepency after November between raw file and Tableau data is $361.13\\r\\nThe total revenue after November is $1753210.39\\r\\nThe revenue discrepency is 0.0% of the total revenue after November\\r\\nThe revenue we made after November is $1752849.26\\r\\n\\r\\nThe revenue by month is\\r\\nMonth\\r\\n1899-12         0.000000\\r\\n1999-12       652.800000\\r\\n2020-02       163.100000\\r\\n2020-03      1116.300000\\r\\n2020-04      5060.200000\\r\\n2020-05      2218.760000\\r\\n2020-06      3551.800000\\r\\n2020-07      6578.100000\\r\\n2020-08     41870.600000\\r\\n2020-09     52336.370000\\r\\n2020-10     50779.900000\\r\\n2020-11     37785.730000\\r\\n2020-12     32358.100000\\r\\n2021-01     23948.040000\\r\\n2021-02     20137.840000\\r\\n2021-03     26584.030000\\r\\n2021-04     50262.400000\\r\\n2021-05     74756.520000\\r\\n2021-06     91146.100000\\r\\n2021-07    144141.810000\\r\\n2021-08    158429.050000\\r\\n2021-09    154983.178800\\r\\n2021-10    201033.121100\\r\\n2021-11    225191.670000\\r\\n2021-12    266561.890000\\r\\n2022-01    276933.564528\\r\\n2022-02    268013.423635\\r\\n2022-03    249499.745543\\r\\n2022-04    296154.537417\\r\\n2022-05    277356.288745\\r\\n2022-06    404793.833177\\r\\n2022-07    247401.727505\\r\\n2022-08    155206.965008\\r\\n2022-09    161623.544255\\r\\n2022-10     39894.072484\\r\\n2022-11     87336.983168\\r\\n2022-12    112797.259432\\r\\n2023-01    126360.496178\\r\\n2023-02    106176.609126\\r\\n2023-03    145345.576130\\r\\n2023-04    141748.617699\\r\\n2023-05    140341.309997\\r\\n2023-06    133078.956674\\r\\n2023-07    159062.982617\\r\\n2023-08    176501.414455\\r\\n2023-09    172955.250641\\r\\n2023-10    190057.228332\\r\\n2023-11     53951.161001\\r\\nNaT          2254.170000\\r\\nName: Revenue, dtype: float64\\r\\n\\r\\nThe revenue by month for unclaimed revenue is month\\r\\n2022-11    159.13\\r\\n2022-12    684.18\\r\\n2023-01    859.75\\r\\n2023-02    881.73\\r\\n2023-03    292.22\\r\\n2023-04    869.75\\r\\n2023-05    860.70\\r\\n2023-06     11.45\\r\\n2023-07      2.55\\r\\n2023-08    142.34\\r\\n2023-09    102.80\\r\\n2023-10     14.24\\r\\n2023-11      0.40\\r\\nName: amount, dtype: float64\\r\\n\\r\\n\\r\\nThe revenue by Date is\\r\\nDate\\r\\n1899-12-31       0.000000\\r\\n1999-12-31     652.800000\\r\\n2020-02-11       4.500000\\r\\n2020-02-27      45.050000\\r\\n2020-02-28      38.500000\\r\\n                 ...     \\r\\n2023-11-06    6805.593469\\r\\n2023-11-07    7515.576476\\r\\n2023-11-08    4851.210000\\r\\n2023-11-09    3928.687000\\r\\n2023-11-10     275.570000\\r\\nName: Revenue, Length: 1354, dtype: float64\\r\\n\\r\\n--===============0531060721504982123==--\\r\\n.\\r\\n'\n",
      "reply: b'250 2.0.0 OK  1699658072 e12-20020a170902d38c00b001bde65894c8sm162769pld.268 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.0.0 OK  1699658072 e12-20020a170902d38c00b001bde65894c8sm162769pld.268 - gsmtp'\n",
      "data: (250, b'2.0.0 OK  1699658072 e12-20020a170902d38c00b001bde65894c8sm162769pld.268 - gsmtp')\n",
      "send: 'quit\\r\\n'\n",
      "reply: b'221 2.0.0 closing connection e12-20020a170902d38c00b001bde65894c8sm162769pld.268 - gsmtp\\r\\n'\n",
      "reply: retcode (221); Msg: b'2.0.0 closing connection e12-20020a170902d38c00b001bde65894c8sm162769pld.268 - gsmtp'\n",
      "connect: to ('smtp.gmail.com', 587) None\n",
      "reply: b'220 smtp.gmail.com ESMTP e19-20020a170902ed9300b001c72c07c9d9sm164778plj.308 - gsmtp\\r\\n'\n",
      "reply: retcode (220); Msg: b'smtp.gmail.com ESMTP e19-20020a170902ed9300b001c72c07c9d9sm164778plj.308 - gsmtp'\n",
      "connect: b'smtp.gmail.com ESMTP e19-20020a170902ed9300b001c72c07c9d9sm164778plj.308 - gsmtp'\n",
      "send: 'ehlo WSAMZN-G2LI17MA.corp.rxmg.tech\\r\\n'\n",
      "reply: b'250-smtp.gmail.com at your service, [52.38.54.186]\\r\\n'\n",
      "reply: b'250-SIZE 35882577\\r\\n'\n",
      "reply: b'250-8BITMIME\\r\\n'\n",
      "reply: b'250-STARTTLS\\r\\n'\n",
      "reply: b'250-ENHANCEDSTATUSCODES\\r\\n'\n",
      "reply: b'250-PIPELINING\\r\\n'\n",
      "reply: b'250-CHUNKING\\r\\n'\n",
      "reply: b'250 SMTPUTF8\\r\\n'\n",
      "reply: retcode (250); Msg: b'smtp.gmail.com at your service, [52.38.54.186]\\nSIZE 35882577\\n8BITMIME\\nSTARTTLS\\nENHANCEDSTATUSCODES\\nPIPELINING\\nCHUNKING\\nSMTPUTF8'\n",
      "send: 'STARTTLS\\r\\n'\n",
      "reply: b'220 2.0.0 Ready to start TLS\\r\\n'\n",
      "reply: retcode (220); Msg: b'2.0.0 Ready to start TLS'\n",
      "send: 'ehlo WSAMZN-G2LI17MA.corp.rxmg.tech\\r\\n'\n",
      "reply: b'250-smtp.gmail.com at your service, [52.38.54.186]\\r\\n'\n",
      "reply: b'250-SIZE 35882577\\r\\n'\n",
      "reply: b'250-8BITMIME\\r\\n'\n",
      "reply: b'250-AUTH LOGIN PLAIN XOAUTH2 PLAIN-CLIENTTOKEN OAUTHBEARER XOAUTH\\r\\n'\n",
      "reply: b'250-ENHANCEDSTATUSCODES\\r\\n'\n",
      "reply: b'250-PIPELINING\\r\\n'\n",
      "reply: b'250-CHUNKING\\r\\n'\n",
      "reply: b'250 SMTPUTF8\\r\\n'\n",
      "reply: retcode (250); Msg: b'smtp.gmail.com at your service, [52.38.54.186]\\nSIZE 35882577\\n8BITMIME\\nAUTH LOGIN PLAIN XOAUTH2 PLAIN-CLIENTTOKEN OAUTHBEARER XOAUTH\\nENHANCEDSTATUSCODES\\nPIPELINING\\nCHUNKING\\nSMTPUTF8'\n",
      "send: 'AUTH PLAIN AHNjaGVkdWxlYWxhcnRzcnhtZ0BnbWFpbC5jb20AeXlqZXRlanRqaGpoenh4aw==\\r\\n'\n",
      "reply: b'235 2.7.0 Accepted\\r\\n'\n",
      "reply: retcode (235); Msg: b'2.7.0 Accepted'\n",
      "send: 'mail FROM:<schedulealartsrxmg@gmail.com> size=2677\\r\\n'\n",
      "reply: b'250 2.1.0 OK e19-20020a170902ed9300b001c72c07c9d9sm164778plj.308 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.1.0 OK e19-20020a170902ed9300b001c72c07c9d9sm164778plj.308 - gsmtp'\n",
      "send: 'rcpt TO:<g.chao@rxmg.com>\\r\\n'\n",
      "reply: b'250 2.1.5 OK e19-20020a170902ed9300b001c72c07c9d9sm164778plj.308 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.1.5 OK e19-20020a170902ed9300b001c72c07c9d9sm164778plj.308 - gsmtp'\n",
      "send: 'data\\r\\n'\n",
      "reply: b'354  Go ahead e19-20020a170902ed9300b001c72c07c9d9sm164778plj.308 - gsmtp\\r\\n'\n",
      "reply: retcode (354); Msg: b'Go ahead e19-20020a170902ed9300b001c72c07c9d9sm164778plj.308 - gsmtp'\n",
      "data: (354, b'Go ahead e19-20020a170902ed9300b001c72c07c9d9sm164778plj.308 - gsmtp')\n",
      "send: b'Content-Type: multipart/mixed; boundary=\"===============6874464184363549485==\"\\r\\nMIME-Version: 1.0\\r\\nFrom: schedulealartsrxmg@gmail.com\\r\\nTo: g.chao@rxmg.com\\r\\nSubject: SMS Daily Update Report\\r\\n\\r\\n--===============6874464184363549485==\\r\\nContent-Type: text/plain; charset=\"us-ascii\"\\r\\nMIME-Version: 1.0\\r\\nContent-Transfer-Encoding: 7bit\\r\\n\\r\\nThe revenue discrepency after November between raw file and Tableau data is $361.13\\r\\nThe total revenue after November is $1753210.39\\r\\nThe revenue discrepency is 0.0% of the total revenue after November\\r\\nThe revenue we made after November is $1752849.26\\r\\n\\r\\nThe revenue by month is\\r\\nMonth\\r\\n1899-12         0.000000\\r\\n1999-12       652.800000\\r\\n2020-02       163.100000\\r\\n2020-03      1116.300000\\r\\n2020-04      5060.200000\\r\\n2020-05      2218.760000\\r\\n2020-06      3551.800000\\r\\n2020-07      6578.100000\\r\\n2020-08     41870.600000\\r\\n2020-09     52336.370000\\r\\n2020-10     50779.900000\\r\\n2020-11     37785.730000\\r\\n2020-12     32358.100000\\r\\n2021-01     23948.040000\\r\\n2021-02     20137.840000\\r\\n2021-03     26584.030000\\r\\n2021-04     50262.400000\\r\\n2021-05     74756.520000\\r\\n2021-06     91146.100000\\r\\n2021-07    144141.810000\\r\\n2021-08    158429.050000\\r\\n2021-09    154983.178800\\r\\n2021-10    201033.121100\\r\\n2021-11    225191.670000\\r\\n2021-12    266561.890000\\r\\n2022-01    276933.564528\\r\\n2022-02    268013.423635\\r\\n2022-03    249499.745543\\r\\n2022-04    296154.537417\\r\\n2022-05    277356.288745\\r\\n2022-06    404793.833177\\r\\n2022-07    247401.727505\\r\\n2022-08    155206.965008\\r\\n2022-09    161623.544255\\r\\n2022-10     39894.072484\\r\\n2022-11     87336.983168\\r\\n2022-12    112797.259432\\r\\n2023-01    126360.496178\\r\\n2023-02    106176.609126\\r\\n2023-03    145345.576130\\r\\n2023-04    141748.617699\\r\\n2023-05    140341.309997\\r\\n2023-06    133078.956674\\r\\n2023-07    159062.982617\\r\\n2023-08    176501.414455\\r\\n2023-09    172955.250641\\r\\n2023-10    190057.228332\\r\\n2023-11     53951.161001\\r\\nNaT          2254.170000\\r\\nName: Revenue, dtype: float64\\r\\n\\r\\nThe revenue by month for unclaimed revenue is month\\r\\n2022-11    159.13\\r\\n2022-12    684.18\\r\\n2023-01    859.75\\r\\n2023-02    881.73\\r\\n2023-03    292.22\\r\\n2023-04    869.75\\r\\n2023-05    860.70\\r\\n2023-06     11.45\\r\\n2023-07      2.55\\r\\n2023-08    142.34\\r\\n2023-09    102.80\\r\\n2023-10     14.24\\r\\n2023-11      0.40\\r\\nName: amount, dtype: float64\\r\\n\\r\\n\\r\\nThe revenue by Date is\\r\\nDate\\r\\n1899-12-31       0.000000\\r\\n1999-12-31     652.800000\\r\\n2020-02-11       4.500000\\r\\n2020-02-27      45.050000\\r\\n2020-02-28      38.500000\\r\\n                 ...     \\r\\n2023-11-06    6805.593469\\r\\n2023-11-07    7515.576476\\r\\n2023-11-08    4851.210000\\r\\n2023-11-09    3928.687000\\r\\n2023-11-10     275.570000\\r\\nName: Revenue, Length: 1354, dtype: float64\\r\\n\\r\\n--===============6874464184363549485==--\\r\\n.\\r\\n'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reply: b'250 2.0.0 OK  1699658074 e19-20020a170902ed9300b001c72c07c9d9sm164778plj.308 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.0.0 OK  1699658074 e19-20020a170902ed9300b001c72c07c9d9sm164778plj.308 - gsmtp'\n",
      "data: (250, b'2.0.0 OK  1699658074 e19-20020a170902ed9300b001c72c07c9d9sm164778plj.308 - gsmtp')\n",
      "send: 'quit\\r\\n'\n",
      "reply: b'221 2.0.0 closing connection e19-20020a170902ed9300b001c72c07c9d9sm164778plj.308 - gsmtp\\r\\n'\n",
      "reply: retcode (221); Msg: b'2.0.0 closing connection e19-20020a170902ed9300b001c72c07c9d9sm164778plj.308 - gsmtp'\n",
      "connect: to ('smtp.gmail.com', 587) None\n",
      "reply: b'220 smtp.gmail.com ESMTP p17-20020a17090adf9100b00281032f9f9csm2168981pjv.35 - gsmtp\\r\\n'\n",
      "reply: retcode (220); Msg: b'smtp.gmail.com ESMTP p17-20020a17090adf9100b00281032f9f9csm2168981pjv.35 - gsmtp'\n",
      "connect: b'smtp.gmail.com ESMTP p17-20020a17090adf9100b00281032f9f9csm2168981pjv.35 - gsmtp'\n",
      "send: 'ehlo WSAMZN-G2LI17MA.corp.rxmg.tech\\r\\n'\n",
      "reply: b'250-smtp.gmail.com at your service, [52.38.54.186]\\r\\n'\n",
      "reply: b'250-SIZE 35882577\\r\\n'\n",
      "reply: b'250-8BITMIME\\r\\n'\n",
      "reply: b'250-STARTTLS\\r\\n'\n",
      "reply: b'250-ENHANCEDSTATUSCODES\\r\\n'\n",
      "reply: b'250-PIPELINING\\r\\n'\n",
      "reply: b'250-CHUNKING\\r\\n'\n",
      "reply: b'250 SMTPUTF8\\r\\n'\n",
      "reply: retcode (250); Msg: b'smtp.gmail.com at your service, [52.38.54.186]\\nSIZE 35882577\\n8BITMIME\\nSTARTTLS\\nENHANCEDSTATUSCODES\\nPIPELINING\\nCHUNKING\\nSMTPUTF8'\n",
      "send: 'STARTTLS\\r\\n'\n",
      "reply: b'220 2.0.0 Ready to start TLS\\r\\n'\n",
      "reply: retcode (220); Msg: b'2.0.0 Ready to start TLS'\n",
      "send: 'ehlo WSAMZN-G2LI17MA.corp.rxmg.tech\\r\\n'\n",
      "reply: b'250-smtp.gmail.com at your service, [52.38.54.186]\\r\\n'\n",
      "reply: b'250-SIZE 35882577\\r\\n'\n",
      "reply: b'250-8BITMIME\\r\\n'\n",
      "reply: b'250-AUTH LOGIN PLAIN XOAUTH2 PLAIN-CLIENTTOKEN OAUTHBEARER XOAUTH\\r\\n'\n",
      "reply: b'250-ENHANCEDSTATUSCODES\\r\\n'\n",
      "reply: b'250-PIPELINING\\r\\n'\n",
      "reply: b'250-CHUNKING\\r\\n'\n",
      "reply: b'250 SMTPUTF8\\r\\n'\n",
      "reply: retcode (250); Msg: b'smtp.gmail.com at your service, [52.38.54.186]\\nSIZE 35882577\\n8BITMIME\\nAUTH LOGIN PLAIN XOAUTH2 PLAIN-CLIENTTOKEN OAUTHBEARER XOAUTH\\nENHANCEDSTATUSCODES\\nPIPELINING\\nCHUNKING\\nSMTPUTF8'\n",
      "send: 'AUTH PLAIN AHNjaGVkdWxlYWxhcnRzcnhtZ0BnbWFpbC5jb20AeXlqZXRlanRqaGpoenh4aw==\\r\\n'\n",
      "reply: b'235 2.7.0 Accepted\\r\\n'\n",
      "reply: retcode (235); Msg: b'2.7.0 Accepted'\n",
      "send: 'mail FROM:<schedulealartsrxmg@gmail.com> size=2677\\r\\n'\n",
      "reply: b'250 2.1.0 OK p17-20020a17090adf9100b00281032f9f9csm2168981pjv.35 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.1.0 OK p17-20020a17090adf9100b00281032f9f9csm2168981pjv.35 - gsmtp'\n",
      "send: 'rcpt TO:<nathan@rxmg.com>\\r\\n'\n",
      "reply: b'250 2.1.5 OK p17-20020a17090adf9100b00281032f9f9csm2168981pjv.35 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.1.5 OK p17-20020a17090adf9100b00281032f9f9csm2168981pjv.35 - gsmtp'\n",
      "send: 'data\\r\\n'\n",
      "reply: b'354  Go ahead p17-20020a17090adf9100b00281032f9f9csm2168981pjv.35 - gsmtp\\r\\n'\n",
      "reply: retcode (354); Msg: b'Go ahead p17-20020a17090adf9100b00281032f9f9csm2168981pjv.35 - gsmtp'\n",
      "data: (354, b'Go ahead p17-20020a17090adf9100b00281032f9f9csm2168981pjv.35 - gsmtp')\n",
      "send: b'Content-Type: multipart/mixed; boundary=\"===============3062872540740933026==\"\\r\\nMIME-Version: 1.0\\r\\nFrom: schedulealartsrxmg@gmail.com\\r\\nTo: nathan@rxmg.com\\r\\nSubject: SMS Daily Update Report\\r\\n\\r\\n--===============3062872540740933026==\\r\\nContent-Type: text/plain; charset=\"us-ascii\"\\r\\nMIME-Version: 1.0\\r\\nContent-Transfer-Encoding: 7bit\\r\\n\\r\\nThe revenue discrepency after November between raw file and Tableau data is $361.13\\r\\nThe total revenue after November is $1753210.39\\r\\nThe revenue discrepency is 0.0% of the total revenue after November\\r\\nThe revenue we made after November is $1752849.26\\r\\n\\r\\nThe revenue by month is\\r\\nMonth\\r\\n1899-12         0.000000\\r\\n1999-12       652.800000\\r\\n2020-02       163.100000\\r\\n2020-03      1116.300000\\r\\n2020-04      5060.200000\\r\\n2020-05      2218.760000\\r\\n2020-06      3551.800000\\r\\n2020-07      6578.100000\\r\\n2020-08     41870.600000\\r\\n2020-09     52336.370000\\r\\n2020-10     50779.900000\\r\\n2020-11     37785.730000\\r\\n2020-12     32358.100000\\r\\n2021-01     23948.040000\\r\\n2021-02     20137.840000\\r\\n2021-03     26584.030000\\r\\n2021-04     50262.400000\\r\\n2021-05     74756.520000\\r\\n2021-06     91146.100000\\r\\n2021-07    144141.810000\\r\\n2021-08    158429.050000\\r\\n2021-09    154983.178800\\r\\n2021-10    201033.121100\\r\\n2021-11    225191.670000\\r\\n2021-12    266561.890000\\r\\n2022-01    276933.564528\\r\\n2022-02    268013.423635\\r\\n2022-03    249499.745543\\r\\n2022-04    296154.537417\\r\\n2022-05    277356.288745\\r\\n2022-06    404793.833177\\r\\n2022-07    247401.727505\\r\\n2022-08    155206.965008\\r\\n2022-09    161623.544255\\r\\n2022-10     39894.072484\\r\\n2022-11     87336.983168\\r\\n2022-12    112797.259432\\r\\n2023-01    126360.496178\\r\\n2023-02    106176.609126\\r\\n2023-03    145345.576130\\r\\n2023-04    141748.617699\\r\\n2023-05    140341.309997\\r\\n2023-06    133078.956674\\r\\n2023-07    159062.982617\\r\\n2023-08    176501.414455\\r\\n2023-09    172955.250641\\r\\n2023-10    190057.228332\\r\\n2023-11     53951.161001\\r\\nNaT          2254.170000\\r\\nName: Revenue, dtype: float64\\r\\n\\r\\nThe revenue by month for unclaimed revenue is month\\r\\n2022-11    159.13\\r\\n2022-12    684.18\\r\\n2023-01    859.75\\r\\n2023-02    881.73\\r\\n2023-03    292.22\\r\\n2023-04    869.75\\r\\n2023-05    860.70\\r\\n2023-06     11.45\\r\\n2023-07      2.55\\r\\n2023-08    142.34\\r\\n2023-09    102.80\\r\\n2023-10     14.24\\r\\n2023-11      0.40\\r\\nName: amount, dtype: float64\\r\\n\\r\\n\\r\\nThe revenue by Date is\\r\\nDate\\r\\n1899-12-31       0.000000\\r\\n1999-12-31     652.800000\\r\\n2020-02-11       4.500000\\r\\n2020-02-27      45.050000\\r\\n2020-02-28      38.500000\\r\\n                 ...     \\r\\n2023-11-06    6805.593469\\r\\n2023-11-07    7515.576476\\r\\n2023-11-08    4851.210000\\r\\n2023-11-09    3928.687000\\r\\n2023-11-10     275.570000\\r\\nName: Revenue, Length: 1354, dtype: float64\\r\\n\\r\\n--===============3062872540740933026==--\\r\\n.\\r\\n'\n",
      "reply: b'250 2.0.0 OK  1699658075 p17-20020a17090adf9100b00281032f9f9csm2168981pjv.35 - gsmtp\\r\\n'\n",
      "reply: retcode (250); Msg: b'2.0.0 OK  1699658075 p17-20020a17090adf9100b00281032f9f9csm2168981pjv.35 - gsmtp'\n",
      "data: (250, b'2.0.0 OK  1699658075 p17-20020a17090adf9100b00281032f9f9csm2168981pjv.35 - gsmtp')\n",
      "send: 'quit\\r\\n'\n",
      "reply: b'221 2.0.0 closing connection p17-20020a17090adf9100b00281032f9f9csm2168981pjv.35 - gsmtp\\r\\n'\n",
      "reply: retcode (221); Msg: b'2.0.0 closing connection p17-20020a17090adf9100b00281032f9f9csm2168981pjv.35 - gsmtp'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "toaddr = ['lili@rxmg.com','g.chao@rxmg.com','nathan@rxmg.com']\n",
    "#toaddr = ['lili@rxmg.com']\n",
    "subject_line = 'SMS Daily Update Report' \n",
    "email_body \n",
    "for i in toaddr:\n",
    "    send_email.send_email('',subject_line,email_body,i)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
